---
# docs/index.md
---


# Valter

> The legal knowledge backend for the Diego Sens ecosystem — serving Brazilian STJ jurisprudence via REST API and Model Context Protocol (MCP).

## What is Valter?

Valter is a specialized backend that transforms raw legal data from Brazil's Superior Tribunal de Justica (STJ) into structured, verifiable legal knowledge. It combines four purpose-built data stores — PostgreSQL for documents and metadata, Qdrant for semantic vector search, Neo4j for a knowledge graph of legal relationships, and Redis for caching — to deliver hybrid search with knowledge-graph boosting, anti-hallucination verification, and graph analytics across tens of thousands of court decisions.

Unlike general-purpose legal search engines that treat decisions as isolated text blobs, Valter connects decisions through a knowledge graph based on the FRBR ontology. Criteria, legal devices, precedents, and ministers are linked as first-class graph entities, enabling queries like "which legal arguments have the highest success rate for this minister in this category" or "how has the application of this criterion evolved over the last five years." Every claim returned by the system is traceable to a specific graph node or verified against real tribunal data.

Valter is MCP-native: 28 tools are exposed via the Model Context Protocol, allowing LLMs like Claude and ChatGPT to query jurisprudence, verify references, analyze divergences, and compose arguments directly. The same capabilities are available through a REST API for traditional frontend consumption.

## Key Numbers

| Metric | Value |
|--------|-------|
| STJ decisions in knowledge graph | ~28,000 |
| Relations between legal entities | 207,000+ |
| STJ metadata records | 810,225 |
| Classified document features | 2,119 |
| Semantic vectors (768-dim) | ~3,673 |
| MCP tools for LLM integration | 28 |
| Graph analytics endpoints | 12 |
| Data stores | 4 (PostgreSQL, Qdrant, Neo4j, Redis) |
| REST API routers | 11 |
| Ingestion workflow endpoints | 17 |

## Quick Links

- **[Getting Started](/getting-started/quickstart)** — Docker setup, first API call, first MCP connection
- **[Architecture Overview](/architecture/overview)** — Modular monolith, dependency rule, 4 runtimes
- **[Technology Stack](/architecture/stack)** — Python, FastAPI, 4 databases, and why each was chosen
- **[API Reference](/api/)** — REST endpoints for search, verification, graph analytics, and ingestion
- **[MCP Tools](/api/mcp-tools)** — All 28 tools available to Claude, ChatGPT, and other MCP clients
- **[Features](/features/)** — Hybrid search, graph analytics, anti-hallucination, and more
- **[Architecture Decisions](/architecture/decisions)** — ADRs documenting key choices and their rationale
- **[Glossary](/reference/glossary)** — Legal and technical terms used throughout the project

## Part of sens.legal

Valter is one of three projects that form the sens.legal ecosystem:

| Project | Role | Stack | Status |
|---------|------|-------|--------|
| **Valter** | Legal knowledge backend — jurisprudence from STJ, served via REST API and MCP | Python, FastAPI, PostgreSQL, Qdrant, Neo4j, Redis | In production |
| **Leci** | Legislation backend — Brazilian statutes and norms as a complementary data source | Planned | Integration planned for v2.0 |
| **Juca** | Frontend for lawyers — the interface through which legal professionals interact with the ecosystem | Next.js | In development |

Valter acts as the knowledge engine: Juca calls Valter's REST API for search and analysis, while Claude and ChatGPT connect through MCP to use the same capabilities as tools in their reasoning process. When Leci is integrated, Valter will be able to cross-reference jurisprudence with the specific legislative provisions being applied, closing the loop between how courts interpret the law and what the law actually says.

---
# docs/getting-started/introduction.md
---


# Introduction

Valter is a legal knowledge backend that serves Brazilian STJ jurisprudence through a REST API and Model Context Protocol (MCP) server. It transforms raw court decisions into structured, verified, composable legal reasoning — enabling LLMs and lawyers to query precedents, analyze divergences between ministers, and compose arguments backed by a knowledge graph.

Valter is part of the [sens.legal](https://sens.legal) ecosystem, alongside Leci (legislation) and Juca (frontend for lawyers).

## Why Valter exists

LLMs are powerful legal research assistants, but they have a fundamental problem: **they hallucinate legal references**. An LLM will confidently cite a Súmula that doesn't exist, attribute a decision to the wrong minister, or invent a process number that was never filed.

Valter exists to solve this. Instead of letting LLMs generate legal knowledge from their training data, Valter provides a structured knowledge backend where every reference is traceable to real tribunal data. When an LLM uses Valter as a tool (via MCP), it can:

- **Search** jurisprudence with hybrid retrieval (lexical + semantic + knowledge graph)
- **Verify** that any legal reference actually exists before presenting it to a user
- **Compose** arguments from patterns of success found in the knowledge graph
- **Understand** who judges what, and how their positions diverge over time

## What Valter knows

Valter's knowledge base is built from public STJ (Superior Tribunal de Justiça) data:

| Store | Records | What it contains |
|-------|---------|-----------------|
| PostgreSQL | ~23,400 decisions | Full text, ementas, metadata, AI-extracted features |
| Neo4j | ~28,500 nodes, ~207,000 edges | Decisions connected by criteria, dispositivos, precedents |
| Qdrant | ~3,700 vectors (768-dim) | Semantic embeddings for similarity search |
| PostgreSQL | ~810,000 metadata records | Raw STJ tribunal metadata |

:::note
The vector coverage gap (3,700 out of 23,400 documents) is a known issue being addressed in milestone v1.0. See the [Roadmap](/roadmap/) for details.
:::

## Who uses Valter

Valter serves three types of consumers through two protocols:

### REST API consumers (port 8000)

**Juca** is a Next.js frontend for lawyers that consumes Valter's REST API directly. It provides a user-friendly interface for searching jurisprudence, analyzing cases, and reviewing AI-generated insights.

### MCP consumers

**Claude Desktop and Claude Code** connect via MCP stdio — a direct process-level connection that requires no network. Configuration is a single entry in `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "valter": {
      "command": "python",
      "args": ["-m", "valter.mcp"],
      "cwd": "/path/to/Valter"
    }
  }
}
```

**ChatGPT** connects via MCP remote — an HTTP/SSE server running on port 8001 with HMAC authentication. This is deployed on Railway alongside the main API.

All three consumers access the same 28 MCP tools and the same underlying data stores.

## What makes Valter different

Most legal tech platforms are search engines with a legal skin. Valter is a **knowledge graph with a reasoning layer**. Here's how that difference plays out:

| What others do | What Valter does |
|----------------|-----------------|
| Search by keywords | Hybrid search: BM25 lexical + semantic vectors + knowledge graph boost + cross-encoder reranking |
| Return a list of similar cases | Compose arguments from success patterns in the graph — which criteria lead to which outcomes, with what success rate |
| "The jurisprudence says..." | "REsp 1.234.567 says X, verified against tribunal data, cited 14 times in the graph, applied by Minister Y in 73% of category Z cases" |
| Treat all decisions as equal | Temporal intelligence — recent decisions weighted higher, trend detection (growing, declining, stable) |
| Ignore who judges | Minister profiles — per-minister success rates, active divergences, behavioral patterns |
| Hallucinate references | Anti-hallucination verification: every súmula, minister name, process number, and legislation reference checked against real data |
| Closed, proprietary systems | MCP-native — any MCP-compatible LLM can use Valter as a tool |
| Documents as text blobs | Knowledge graph — decisions connected by criteria, dispositivos, precedents, and legislation in a FRBR-based ontology |

## Core capabilities

Valter's capabilities are organized into six domains:

### Search and retrieval

The search pipeline combines five strategies into a single query flow. A search request hits BM25 lexical matching and Qdrant semantic similarity in parallel, merges results via weighted scoring or Reciprocal Rank Fusion (RRF), boosts scores using Neo4j graph connectivity, and reranks the top results with a cross-encoder. Query expansion via Groq LLM can generate up to 3 query variants for better recall.

**Endpoints:** `POST /v1/retrieve`, `POST /v1/similar_cases`, `POST /v1/search/features`, `POST /v1/factual/dual-search`

### Knowledge graph analytics

Twelve graph endpoints expose the Neo4j knowledge graph for legal reasoning. You can detect active divergences between ministers, find the optimal argument path for a legal category, track how criteria application evolves over time, and generate comprehensive minister profiles.

**Endpoints:** `POST /v1/graph/*` (12 endpoints)

### Verification and enrichment

The verification system checks legal references against real tribunal data — súmulas, minister names, process numbers, and legislation. The enrichment system adds IRAC analysis (Issue, Rule, Application, Conclusion) and knowledge graph context to any decision.

**Endpoints:** `POST /v1/verify`, `POST /v1/enrich`, `POST /v1/factual/extract`

### Document ingestion

A full case analysis workflow processes PDFs from upload to reviewed legal analysis. The PROJUDI pipeline extracts and segments the document, phase analysis identifies procedural stages, and jurisprudence matching finds relevant precedents for each phase. A state machine ensures audit-safe transitions, and human-in-the-loop review allows approval or rejection at each stage.

**Endpoints:** `POST /v1/ingest/*` (17 endpoints)

### MCP integration

Twenty-eight MCP tools expose all of Valter's capabilities to LLMs. Tools are organized into knowledge, graph, and workflow categories. Both stdio (Claude) and HTTP/SSE (ChatGPT) transports are supported, with API key and HMAC authentication respectively.

### Observability

Structured JSON logging via structlog with trace IDs on every request, 30+ Prometheus metrics, and OpenTelemetry tracing provide visibility into system behavior.

## Architecture at a glance

Valter is a **modular monolith with four runtimes** — all sharing the same Python codebase:

| Runtime | Command | Port | Purpose |
|---------|---------|------|---------|
| REST API | `make dev` | 8000 | HTTP endpoints for Juca and direct consumers |
| MCP stdio | `python -m valter.mcp` | — | Local connection for Claude Desktop/Code |
| MCP HTTP/SSE | `make mcp-remote` | 8001 | Remote connection for ChatGPT |
| ARQ Worker | `make worker-ingest` | — | Background job processing for PDF ingestion |

The codebase follows a strict layered architecture: `api/` → `core/` → `models/`, with `stores/` implementing protocol interfaces from `core/protocols.py`. Core business logic never imports stores directly — everything flows through dependency injection.

For a deeper dive into the architecture, see [Architecture Overview](/architecture/overview/).

## Next steps

- **[Quickstart](/getting-started/quickstart/)** — Get Valter running locally in under 5 minutes
- **[Installation](/getting-started/installation/)** — Full setup guide with all databases and options
- **[API Reference](/api/)** — Complete endpoint documentation
- **[MCP Tools](/api/mcp-tools/)** — All 28 MCP tools with parameters and examples

---
# docs/getting-started/quickstart.md
---


# Quickstart

This guide gets Valter running on your machine with the minimum viable setup. You'll start the database infrastructure, run migrations, and make your first API call. For a complete setup with all optional features, see the [Installation guide](/getting-started/installation/).

## Prerequisites

You need three things installed:

- **Python 3.12+** — Valter uses modern typing features (`X | None`, `type` statements)
- **Docker and Docker Compose** — for PostgreSQL, Qdrant, and Redis
- **make** — all commands go through the Makefile

## Step 1: Clone and install

```bash
git clone <repo-url> && cd Valter

# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate

# Install with dev dependencies (prefer uv for speed)
uv pip install -e ".[dev]"
# or: pip install -e ".[dev]"
```

## Step 2: Start databases

```bash
make docker-up
```

This starts three containers via Docker Compose:

| Service | Image | Port | Data |
|---------|-------|------|------|
| PostgreSQL 16 | `postgres:16-alpine` | 5432 | Documents, features, metadata, jobs |
| Qdrant | `qdrant/qdrant:latest` | 6333 | Semantic vector search |
| Redis 7 | `redis:7-alpine` | 6379 | Cache, rate limiting, job queue |

:::note
Neo4j is **not** included in Docker Compose by design. Without Neo4j, the search, verification, and enrichment endpoints work fine — only the 12 `/v1/graph/*` endpoints return 503. See the [Installation guide](/getting-started/installation/) for Neo4j setup.
:::

## Step 3: Configure environment

```bash
cp .env.example .env
```

The defaults work out of the box for local development. The `.env.example` file is pre-configured with local connection strings:

```bash
# These are the defaults — no changes needed for local dev
VALTER_DATABASE_URL=postgresql+asyncpg://valter:valter_dev@localhost:5432/valter
VALTER_QDRANT_URL=http://localhost:6333
VALTER_REDIS_URL=redis://localhost:6379/0
```

## Step 4: Run migrations

```bash
make migrate
```

This runs `alembic upgrade head`, applying all 8 PostgreSQL migrations that create tables for documents, features, metadata, jobs, workflows, API keys, audit logs, and session memory.

## Step 5: Start the server

```bash
make dev
```

The API starts on `http://localhost:8000` with hot reload enabled. You'll see structured JSON logs from structlog:

```
{"event": "valter.startup", "version": "0.1.0", ...}
```

## Step 6: Verify it works

### Health check

```bash
curl -s http://localhost:8000/health | python -m json.tool
```

```json
{
  "status": "healthy",
  "version": "0.1.0",
  "stores": [
    {"name": "qdrant", "status": "up", "latency_ms": 2.1},
    {"name": "neo4j", "status": "down", "latency_ms": null},
    {"name": "postgres", "status": "up", "latency_ms": 1.3},
    {"name": "redis", "status": "up", "latency_ms": 0.4},
    {"name": "artifact_storage", "status": "up", "latency_ms": 0.1},
    {"name": "worker_ingest", "status": "down", "latency_ms": null}
  ],
  "uptime_seconds": 3.42
}
```

A `"degraded"` status is normal at this point — Neo4j and the ingest worker aren't running yet.

### Interactive API docs

Open `http://localhost:8000/docs` in your browser. FastAPI generates interactive Swagger documentation for all endpoints.

### Your first search

```bash
curl -s -X POST http://localhost:8000/v1/retrieve \
  -H "Content-Type: application/json" \
  -d '{"query": "responsabilidade civil dano moral"}' \
  | python -m json.tool
```

:::tip
The first search request takes 30–60 seconds because it downloads and loads the embedding model (~500MB). Subsequent requests are fast. To avoid this delay, pre-download the model with `make download-model`.
:::

### Verify a legal reference

```bash
curl -s -X POST http://localhost:8000/v1/verify \
  -H "Content-Type: application/json" \
  -d '{"text": "Conforme Súmula 297 do STJ"}' \
  | python -m json.tool
```

## Step 7: Connect via MCP (optional)

To use Valter as a tool in Claude Desktop, add this to your `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "valter": {
      "command": "python",
      "args": ["-m", "valter.mcp"],
      "cwd": "/absolute/path/to/Valter",
      "env": {
        "VALTER_DATABASE_URL": "postgresql+asyncpg://valter:valter_dev@localhost:5432/valter",
        "VALTER_QDRANT_URL": "http://localhost:6333",
        "VALTER_REDIS_URL": "redis://localhost:6379/0"
      }
    }
  }
}
```

Restart Claude Desktop. You should see 28 tools available under "valter" in the MCP tools panel.

:::caution
The MCP stdio server runs as a separate process from the REST API. It needs its own environment variables because it doesn't read from the same `.env` file — pass them explicitly in the `env` block.
:::

## What's running

After completing this quickstart, you have:

```
┌─────────────────────────────────────────┐
│  Your machine                           │
│                                         │
│  ┌─────────────┐  ┌─────────────────┐   │
│  │ Valter API  │  │ Claude Desktop  │   │
│  │ :8000       │  │ (MCP stdio)     │   │
│  └──────┬──────┘  └────────┬────────┘   │
│         │                  │            │
│  ┌──────┴──────────────────┴──────────┐ │
│  │        Shared Data Stores          │ │
│  │  PostgreSQL :5432                  │ │
│  │  Qdrant     :6333                  │ │
│  │  Redis      :6379                  │ │
│  └────────────────────────────────────┘ │
└─────────────────────────────────────────┘
```

## What's not running (yet)

| Component | What it adds | How to enable |
|-----------|-------------|---------------|
| Neo4j | 12 graph analytics endpoints (divergences, optimal argument, temporal evolution) | [Installation guide](/getting-started/installation/#neo4j-knowledge-graph) |
| ARQ Worker | Background PDF ingestion and case analysis | `make worker-ingest` |
| MCP Remote | HTTP/SSE server for ChatGPT integration | `make mcp-remote` |
| Groq LLM | Factual extraction, query expansion, document classification | Set `VALTER_GROQ_API_KEY` and `VALTER_GROQ_ENABLED=true` |

## Next steps

- **[Installation](/getting-started/installation/)** — Full setup with Neo4j, Groq, R2, and production configuration
- **[API Reference](/api/)** — All endpoints with request/response schemas
- **[MCP Tools](/api/mcp-tools/)** — Complete reference for the 28 MCP tools
- **[Architecture](/architecture/overview/)** — How the layers and data stores fit together

---
# docs/getting-started/installation.md
---


# Installation

This guide covers the complete setup of every Valter component. If you just want to get running quickly, start with the [Quickstart](/getting-started/quickstart/) and come back here when you need Neo4j, Groq, or production configuration.

## System requirements

| Requirement | Minimum | Notes |
|-------------|---------|-------|
| Python | 3.12+ | Uses `X \| None` syntax, `type` statements |
| Docker | 20.10+ | For PostgreSQL, Qdrant, Redis containers |
| Docker Compose | v2+ | `docker compose` (not `docker-compose`) |
| make | any | Canonical command interface |
| RAM | ~4 GB | Embedding model (~1.5 GB) + databases (~2 GB) |
| Disk | ~3 GB | Model cache + container volumes |

## Python environment

Valter uses [uv](https://github.com/astral-sh/uv) as its preferred package manager, with pip as a fallback:

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate

# Install with dev dependencies
uv pip install -e ".[dev]"

# Or with pip:
pip install -e ".[dev]"
```

### Optional extras

```bash
# OCR support (pytesseract + pdf2image)
uv pip install -e ".[dev,ocr]"
```

:::caution
OCR requires the system package `tesseract-ocr` to be installed separately. On macOS: `brew install tesseract`. On Ubuntu: `apt install tesseract-ocr`. Without the system package, the Python packages install but OCR calls fail with an error at runtime.
:::

## Database setup

### PostgreSQL (relational data)

PostgreSQL stores documents, AI-extracted features, STJ metadata, ingestion jobs, workflows, API keys, and audit logs.

**Via Docker Compose (recommended):**

```bash
make docker-up  # Starts PostgreSQL 16 Alpine on port 5432
```

Default connection parameters (pre-configured in `.env.example`):

```
Host:     localhost:5432
Database: valter
User:     valter
Password: valter_dev
```

**Via existing PostgreSQL:**

Set your connection string in `.env`:

```bash
VALTER_DATABASE_URL=postgresql+asyncpg://user:password@host:5432/dbname
```

**Run migrations** after PostgreSQL is available:

```bash
make migrate  # alembic upgrade head
```

This creates all required tables across 8 migration files. Each migration supports `downgrade()` for rollback.

### Qdrant (vector search)

Qdrant stores semantic embeddings for similarity search. Valter uses 768-dimensional vectors with cosine distance.

**Via Docker Compose:**

```bash
make docker-up  # Also starts Qdrant on port 6333
```

The collection is created automatically on first startup. The `init_stores` function in `api/deps.py` calls `ensure_collection()` and validates that the configured dimension matches the existing collection.

```bash
VALTER_QDRANT_URL=http://localhost:6333
VALTER_QDRANT_COLLECTION=legal_chunks_v1  # default
```

### Redis (cache, rate limiting, job queue)

Redis serves three purposes: response caching with 180-second TTL, sliding-window rate limiting per API key, and ARQ job queue for background processing.

**Via Docker Compose:**

```bash
make docker-up  # Also starts Redis 7 Alpine on port 6379
```

```bash
VALTER_REDIS_URL=redis://localhost:6379/0  # Cache + rate limiting
VALTER_ARQ_REDIS_DB=1                       # Separate DB for job queue
```

:::danger
Redis is currently a single point of failure for the rate limiter. If Redis goes down, **all API requests are blocked** — even with valid API keys. This fail-closed behavior is being changed to fail-open in milestone v1.0 (issue #150).
:::

### Neo4j (knowledge graph)

Neo4j stores the knowledge graph: ~28,500 nodes (decisions, criteria, dispositivos, precedents) connected by ~207,000 edges. It powers the 12 `/v1/graph/*` endpoints, the KG boost in hybrid search, and structural similarity.

**Without Neo4j**, Valter still works — search, verification, enrichment, and ingestion all function. Only graph-specific features return 503.

#### Option A: Local Neo4j

Install [Neo4j Community Edition](https://neo4j.com/download/) 5.x, then configure:

```bash
VALTER_NEO4J_URI=bolt://localhost:7687
VALTER_NEO4J_USERNAME=neo4j
VALTER_NEO4J_PASSWORD=your_password
```

Run the Cypher schema migrations manually:

```bash
# Apply migrations in neo4j_migrations/ directory
cypher-shell -u neo4j -p your_password < neo4j_migrations/001_initial_schema.cypher
cypher-shell -u neo4j -p your_password < neo4j_migrations/002_indexes.cypher
```

#### Option B: Neo4j Aura (managed)

[Neo4j Aura](https://neo4j.com/cloud/aura/) is required for staging and production environments.

```bash
VALTER_NEO4J_URI=neo4j+s://abc123.databases.neo4j.io
VALTER_NEO4J_USERNAME=neo4j
VALTER_NEO4J_PASSWORD=your_aura_password
```

Validate the connection:

```bash
make validate-aura  # Runs scripts/validate_aura.py --max-latency-ms 15000
```

:::caution
Any PR that modifies graph-related code must pass `make validate-aura` before merge. This is enforced by CI and documented in the project's governance rules.
:::

## Embedding model

Valter uses [Legal-BERTimbau-sts-base](https://huggingface.co/rufimelo/Legal-BERTimbau-sts-base) — a Portuguese legal domain model producing 768-dimensional embeddings.

### Pre-download (recommended)

```bash
make download-model
```

This downloads ~500 MB to `~/.cache/huggingface/`. The model name is resolved with three-level priority:

1. Shell environment variable `VALTER_EMBEDDING_MODEL`
2. Value in `.env` file
3. Hardcoded fallback: `rufimelo/Legal-BERTimbau-sts-base`

### Automatic download

If you skip `make download-model`, the model downloads automatically on the first search request. This adds 30–60 seconds to the first request.

### Remote encoding

For production or resource-constrained environments, Valter supports remote encoding via a dedicated GPU service:

```bash
VALTER_EMBEDDING_SERVICE_URL=https://your-encoder-service.railway.app
```

When set, Valter uses `RailwayEncoder` instead of loading the model locally. The remote service must return 768-dimensional vectors.

Similarly for reranking:

```bash
VALTER_RERANKER_SERVICE_URL=https://your-reranker-service.railway.app
```

## Runtime modes

Valter runs in four modes from the same codebase, selected by `VALTER_RUNTIME`:

### API server (default)

```bash
make dev  # Development with hot reload
# or in production:
VALTER_RUNTIME=api uvicorn valter.main:app --host 0.0.0.0 --port 8000
```

The API server starts the full FastAPI application with middleware stack, 11 routers, and Prometheus metrics.

### ARQ worker

```bash
make worker-ingest
```

The worker processes background jobs: PDF ingestion, PROJUDI extraction, phase analysis, and jurisprudence matching. It connects to Redis (DB 1) as its job queue.

Configuration:

```bash
VALTER_INGEST_JOB_TIMEOUT_SECONDS=1800    # 30 min max per job
VALTER_INGEST_WORKER_CONCURRENCY=2         # Parallel job slots
```

### MCP stdio server (Claude Desktop/Code)

```bash
python -m valter.mcp
```

This starts the MCP server in stdio mode for local LLM connections. Configure it in Claude Desktop's `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "valter": {
      "command": "python",
      "args": ["-m", "valter.mcp"],
      "cwd": "/absolute/path/to/Valter",
      "env": {
        "VALTER_DATABASE_URL": "postgresql+asyncpg://valter:valter_dev@localhost:5432/valter",
        "VALTER_QDRANT_URL": "http://localhost:6333",
        "VALTER_REDIS_URL": "redis://localhost:6379/0",
        "VALTER_MCP_API_BASE_URL": "http://localhost:8000"
      }
    }
  }
}
```

:::tip
The MCP stdio server bridges to the REST API via httpx. Make sure `VALTER_MCP_API_BASE_URL` points to a running API server — the MCP tools call the REST endpoints internally.
:::

### MCP HTTP/SSE server (ChatGPT)

```bash
make mcp-remote  # Starts on port 8001
```

This starts a streamable-HTTP MCP server for remote consumers like ChatGPT. It requires authentication:

```bash
VALTER_MCP_SERVER_AUTH_MODE=api_key
VALTER_MCP_SERVER_API_KEYS=your_key_1,your_key_2  # comma-separated for rotation
VALTER_MCP_SERVER_PORT=8001
```

### Container runtime selection

In Docker/Railway, the `scripts/start-command.sh` script selects the runtime:

```bash
VALTER_RUNTIME=api           # → uvicorn valter.main:app
VALTER_RUNTIME=worker        # → python -m valter.workers
VALTER_RUNTIME=mcp-remote    # → python -m valter.mcp.remote_server
VALTER_RUNTIME=mcp-stdio     # → python -m valter.mcp.remote_server (stdio transport)
```

## Optional integrations

### Groq LLM

Groq powers three features: document classification (21 AI-extracted fields), factual extraction from decision text, and query expansion (up to 3 search variants).

```bash
VALTER_GROQ_API_KEY=gsk_your_key_here  # Get from https://console.groq.com/keys
VALTER_GROQ_ENABLED=true
VALTER_GROQ_MODEL=qwen/qwen3-32b       # default
```

Without Groq, all other features work normally — factual extraction endpoints return errors, and query expansion is silently skipped.

### Cloudflare R2

R2 provides S3-compatible object storage for workflow artifacts (PDFs, analysis JSONs). A deterministic canary rollout mechanism controls what percentage of artifacts go to R2 vs local storage.

```bash
VALTER_R2_ACCOUNT_ID=your_account_id
VALTER_R2_ACCESS_KEY_ID=your_access_key
VALTER_R2_SECRET_ACCESS_KEY=your_secret_key
VALTER_R2_BUCKET_NAME=valter-artifacts        # default
VALTER_R2_CANARY_PERCENT=0                     # 0 = all local, 100 = all R2
VALTER_R2_PRESIGN_TTL_SECONDS=600              # Signed URL validity
```

Without R2 credentials, artifacts are stored locally at `VALTER_UPLOAD_STORAGE_PATH` (default: `data/datasets/uploads/raw`).

## Production configuration

When `VALTER_ENV=production`, Valter enforces several safety checks at startup. The server **will not start** if any of these are violated:

| Requirement | Why |
|-------------|-----|
| `VALTER_AUTH_ENABLED=true` | API must require authentication |
| Explicit CORS origins (no `["*"]`) | Wildcard CORS is insecure |
| `VALTER_METRICS_IP_ALLOWLIST` set | `/metrics` must be IP-restricted |
| Remote Neo4j URI (`neo4j+s://`) | Local bolt connections are dev-only |
| Non-weak passwords | Rejects `neo4j_dev`, `password`, `changeme`, etc. |
| Non-default DB credentials | Rejects the default `valter:valter_dev` connection |
| Remote Redis | Rejects `localhost` Redis in production |

These checks are implemented in `config.py`'s production validator (lines 95–121).

## Verification

After setup, run the full verification suite:

```bash
# Run 660+ tests
make test

# Lint check (ruff + format verification)
make lint

# Full quality gate (lint + mypy + tests)
make quality

# Health check
curl -s http://localhost:8000/health | python -m json.tool
```

All stores should show `"status": "up"` except Neo4j (if not configured) and `worker_ingest` (if the worker isn't running).

## Complete make target reference

| Target | Command | Description |
|--------|---------|-------------|
| `make dev` | `uvicorn ... --reload` | Development server with hot reload (port 8000) |
| `make test` | `pytest tests/ -v` | Run all tests |
| `make test-cov` | `pytest ... --cov=valter` | Tests with coverage report |
| `make test-neo4j-live` | `pytest tests/integration/...` | Neo4j integration tests (requires Aura) |
| `make lint` | `ruff check + format --check` | Lint and format verification |
| `make fmt` | `ruff check --fix + format` | Auto-fix lint issues and format |
| `make quality` | `lint + mypy + test` | Full quality gate |
| `make migrate` | `alembic upgrade head` | Run database migrations |
| `make docker-up` | `docker compose up -d` | Start database containers |
| `make docker-down` | `docker compose down` | Stop database containers |
| `make download-model` | Python snippet | Download embedding model to cache |
| `make validate-aura` | `python scripts/validate_aura.py` | Validate Neo4j Aura connection |
| `make worker-ingest` | `python -m valter.workers` | Start background job worker |
| `make mcp-remote` | `python -m valter.mcp.remote_server` | Start MCP HTTP/SSE server (port 8001) |

## Next steps

- **[Architecture Overview](/architecture/overview/)** — How the layers and data stores fit together
- **[Environment Variables](/configuration/environment/)** — Complete reference for all 50+ env vars
- **[API Reference](/api/)** — All endpoints with schemas
- **[Coding Conventions](/development/conventions/)** — Standards enforced in the codebase

---
# docs/architecture/overview.md
---


# Architecture Overview

> Valter is a modular monolith with 4 runtimes (API, Worker, MCP stdio, MCP HTTP), all sharing the same Python codebase with strict layered separation.

## Architectural Pattern

Valter follows a **modular monolith** pattern — not microservices. All runtimes share the same Python package (`src/valter/`), the same domain models, and the same business logic. What changes between runtimes is the entry point and the transport layer, not the core logic.

This design was chosen for three reasons:

1. **Consistency** — a single codebase guarantees that the behavior of an MCP tool and its equivalent REST endpoint are always identical, because they call the same core function.
2. **Simplicity** — there is one deployment unit to build, test, and reason about. No inter-service communication, no API contracts between services, no distributed state.
3. **Testability** — all four runtimes can be validated by the same test suite, since the tests target core logic rather than transport-specific code.

The runtime is selected by the entry point used to start the process. In production on Railway, multiple instances of the same codebase run with different entry points (API on port 8000, MCP remote on port 8001, ARQ worker for background jobs).

## Dependency Rule

The codebase enforces a strict one-way dependency rule between layers:

```
api/ → core/ → models/
```

This means:

- **`api/`** can import from `core/` and `models/`, but never from `stores/`.
- **`core/`** can import from `models/`, but never from `stores/` or `api/`.
- **`models/`** has zero internal imports — it is the leaf layer.
- **`stores/`** implements protocols defined in `core/protocols.py` and is injected at runtime via FastAPI's `Depends()` mechanism, configured in `api/deps.py`.

This separation ensures that `core/` contains pure business logic with no coupling to any concrete database driver. A `PostgresDocStore` can be swapped for a mock in tests (or a different implementation entirely) without changing a single line in `core/`.

:::note
The `stores/` layer sits beside the dependency chain, not within it. Core modules receive store instances through dependency injection, never through direct imports. If you see `from valter.stores import ...` inside a `core/` module, that is a violation.
:::

## Layers

### API Layer (`src/valter/api/`)

The API layer is the outermost boundary of the application. It handles HTTP transport, request validation, authentication, and response serialization. Route handlers are intentionally thin — they validate input using Pydantic schemas, call a core function, and return the result.

Key components:

- **11 FastAPI routers** — `health`, `retrieve`, `verify`, `enrich`, `similar`, `graph`, `features`, `factual`, `ingest`, `memories`, `datasets`
- **Pydantic v2 schemas** — request and response models in `api/schemas/`, separate from domain models
- **DI container** — `api/deps.py` wires concrete stores into core functions using `Depends()`
- **Middleware stack** — requests pass through 5 middleware layers in order: CORS, Metrics IP Allowlist, Request Tracking (trace_id + Prometheus), Rate Limiter (Redis sliding window), Auth (API key + scopes)

:::caution
Route handlers must never contain business logic. If a handler does more than validate input, call core, and return a response, the logic belongs in `core/`.
:::

### Core Layer (`src/valter/core/`)

The core layer contains all business logic. It has approximately 25 modules organized by domain capability:

| Group | Modules | Purpose |
|-------|---------|---------|
| Search | `HybridRetriever`, `DualVectorRetriever`, `QueryExpander` | Hybrid search with BM25 + semantic + KG boost, dual-vector factual search, multi-query expansion |
| Analysis | `DocumentEnricher`, `LegalVerifier`, `SimilarityFinder`, `FactualExtractor` | IRAC analysis, anti-hallucination verification, case similarity, factual extraction via Groq |
| Workflow | `WorkflowOrchestrator`, `ProjudiOrchestrator`, `PhaseAnalysis` (5 modules) | Full ingestion pipeline from PDF upload to human-reviewed artifacts |
| Infrastructure | `Protocols` (runtime-checkable interfaces) | Contracts that stores must implement |

Every module in `core/` depends only on protocols and models — never on concrete store implementations.

### Store Layer (`src/valter/stores/`)

The store layer provides concrete implementations of the protocols defined in `core/protocols.py`. Each store is specialized for its data backend:

| Store | Backend | Responsibility |
|-------|---------|---------------|
| `PostgresDocStore` | PostgreSQL | Document CRUD, full-text search (BM25) |
| `PostgresFeaturesStore` | PostgreSQL | AI-extracted features (21 fields per decision) |
| `PostgresSTJStore` | PostgreSQL | STJ metadata (810K records) |
| `PostgresIngestStore` | PostgreSQL | Ingestion jobs, workflow state |
| `PostgresMemoryStore` | PostgreSQL | Session memory with TTL |
| `QdrantVectorStore` | Qdrant | Semantic search (768-dim vectors, cosine similarity) |
| `Neo4jGraphStore` | Neo4j | Knowledge graph queries (12+ analytical methods) |
| `RedisCacheStore` | Redis | Query cache (180s TTL), rate limiting counters |
| `GroqLLMClient` | Groq API | LLM calls for classification, extraction, query expansion |
| `ArtifactStorage` | Cloudflare R2 / local | PDF and JSON artifact storage with canary rollout |

:::tip
The `GroqLLMClient` lives in `stores/` even though Groq is not a traditional database. In Valter's architecture, `stores/` means "external data provider" — anything that lives outside the process boundary and requires I/O.
:::

### Model Layer (`src/valter/models/`)

The model layer defines domain entities as Pydantic v2 models. These models are shared across all layers and represent the canonical shape of data in the system:

| Module | Models |
|--------|--------|
| `document.py` | `Document`, `DocumentMetadata` |
| `chunk.py` | `Chunk`, `ChunkMetadata` |
| `irac.py` | IRAC analysis structure (Issue, Rule, Application, Conclusion) |
| `graph.py` | 30+ graph entity models (divergences, minister profiles, PageRank, communities, etc.) |
| `frbr.py` | FRBR ontology models (`Work`, `Expression`, `Manifestation`) |
| `phase.py` | Legal proceeding phase models |
| `features.py` | AI-extracted document features (21 fields) |
| `factual.py` | Factual digest and legal thesis |
| `stj_metadata.py` | STJ tribunal metadata |
| `memory.py` | Session memory key-value pairs |

All models use `model_config = {"strict": False}` to allow coercion from database results while maintaining type safety in application code.

## Entry Points

Valter exposes four runtime entry points, all from the same codebase:

| Entry Point | File | Command | Port | Consumers |
|-------------|------|---------|------|-----------|
| REST API | `src/valter/main.py` | `make dev` | 8000 | Juca frontend, direct API clients |
| MCP stdio | `src/valter/mcp/__main__.py` | `python -m valter.mcp` | -- | Claude Desktop, Claude Code |
| MCP HTTP/SSE | `src/valter/mcp/remote_server.py` | `make mcp-remote` | 8001 | ChatGPT Apps via HMAC auth |
| ARQ Worker | `src/valter/workers/__main__.py` | `make worker-ingest` | -- | Background ingestion jobs |

In production (Railway), the REST API and MCP HTTP/SSE run as separate services with distinct URLs, while the ARQ Worker runs as a separate process consuming the Redis job queue.

## Data Flow

At a high level, every request flows through the same pipeline regardless of entry point:

```
Consumer (Juca / ChatGPT / Claude)
    │
    ▼
Entry Point (REST API / MCP stdio / MCP HTTP)
    │
    ▼
Middleware Stack (CORS → Metrics → Tracking → RateLimit → Auth)
    │
    ▼
Route Handler (validates input, delegates to core)
    │
    ▼
Core Logic (retriever, enricher, verifier, etc.)
    │
    ▼
Stores (PostgreSQL, Qdrant, Neo4j, Redis, Groq, R2)
    │
    ▼
Response (serialized via Pydantic schema)
```

MCP tools follow the same path: each tool's implementation calls core functions, which in turn call stores. The MCP layer adds no business logic — it is a thin adapter that translates MCP tool calls into the same core function calls that REST route handlers make.

For detailed visual diagrams of component relationships and the search pipeline, see [Architecture Diagrams](/architecture/diagrams).

---
# docs/architecture/stack.md
---


# Technology Stack

> Every technology choice in Valter, with versions, purpose, and why it was chosen.

## Language

Valter is written in **Python >=3.12** (production uses the `3.12-slim` Docker image). Python was chosen for three reasons:

1. **ML ecosystem** — sentence-transformers, cross-encoders, and NLP libraries for legal text processing are Python-native. There is no viable alternative in other languages for Legal-BERTimbau embeddings.
2. **Async performance** — Python 3.12 with `asyncio`, `asyncpg`, and `uvicorn` delivers throughput sufficient for the expected load (single-digit concurrent users during early adoption).
3. **Rich typing** — type hints with `mypy --strict` and Pydantic v2 validation catch errors at development time that would otherwise surface as runtime bugs in legal data processing, where correctness is paramount.

All public functions require type hints. This is enforced by `mypy` in CI and by convention in `CLAUDE.md`.

## Web Framework

**FastAPI >=0.115.0** with **Uvicorn >=0.34.0** serves as the HTTP layer.

FastAPI was chosen because it aligns with every architectural constraint in the project:

- **Async-first** — every I/O operation in Valter (database queries, vector search, graph traversal, LLM calls) is `async/await`. FastAPI is built on Starlette's async foundation.
- **Pydantic v2 integration** — request and response validation uses the same Pydantic models that define domain entities, eliminating translation layers.
- **OpenAPI auto-generation** — the REST API produces a complete OpenAPI spec from type annotations, which is consumed by external integrators.
- **`Depends()` DI** — FastAPI's dependency injection system powers the protocol-based architecture. Stores are wired in `api/deps.py` and injected into route handlers without the core layer knowing about concrete implementations.

## Data Stores

Valter uses four specialized databases, each chosen for a specific workload:

| Store | Version | Role | Data Volume | Why This Store |
|-------|---------|------|-------------|----------------|
| PostgreSQL | 16 (Alpine) | Documents, features, STJ metadata, ingestion jobs, workflow state, session memory, API keys, audit logs | ~23,441 docs, 2,119 features, 810,225 metadata records | Relational integrity for legal documents, JSONB for flexible metadata, mature async support via `asyncpg`, Alembic migrations |
| Qdrant | latest | Semantic vector search | ~3,673 vectors (768-dim, cosine similarity) | Purpose-built vector database with payload filtering, native cosine similarity, and sub-second query latency for legal embeddings |
| Neo4j | 5.x / Aura | Knowledge graph (FRBR ontology) | ~28,482 nodes, ~207,163 edges | Native graph traversal for multi-hop queries (citation chains, divergence detection, community discovery), Cypher query language, managed Aura option for production |
| Redis | 7 (Alpine) | Cache, rate limiting, job queue | Ephemeral | Sub-millisecond latency for query cache (180s TTL), native TTL for rate limiting sliding windows, ARQ integration for background job queues |

:::note
The decision to use four databases instead of consolidating (for example, PostgreSQL with pgvector for vectors) is documented in [ADR-002: Four Data Stores](/architecture/decisions#adr-002-four-data-stores). The short version: each store is optimized for its workload, and the operational complexity is managed by Docker Compose locally and Railway in production.
:::

## Production Dependencies

All production dependencies with their purpose, grouped by function:

### Web and HTTP

| Package | Version | Purpose |
|---------|---------|---------|
| `fastapi` | >=0.115.0 | Web framework, routing, DI, OpenAPI generation |
| `uvicorn[standard]` | >=0.34.0 | ASGI server with HTTP/1.1 and WebSocket support |
| `httpx` | >=0.28.0 | Async HTTP client for bridge calls (MCP remote to API, Railway encoder) |

### Data Validation and Configuration

| Package | Version | Purpose |
|---------|---------|---------|
| `pydantic` | >=2.10.0 | Domain models, request/response schemas, runtime validation |
| `pydantic-settings` | >=2.7.0 | Environment variable parsing into typed settings objects |

### Database Drivers

| Package | Version | Purpose |
|---------|---------|---------|
| `sqlalchemy[asyncio]` | >=2.0.36 | Async ORM and query builder for PostgreSQL |
| `asyncpg` | >=0.30.0 | High-performance async PostgreSQL driver |
| `alembic` | >=1.14.0 | Database migration management |
| `qdrant-client` | >=1.12.0 | Qdrant vector database client |
| `neo4j` | >=5.27.0 | Neo4j graph database driver (async bolt protocol) |
| `redis` | >=5.2.0 | Redis client for cache, rate limiting, and ARQ |

### Search and ML

| Package | Version | Purpose |
|---------|---------|---------|
| `rank-bm25` | >=0.2.2 | BM25 scoring for keyword-based search |
| `sentence-transformers` | >=3.3.0 | Legal-BERTimbau embedding model (768-dim) and cross-encoder reranking |

### Observability

| Package | Version | Purpose |
|---------|---------|---------|
| `structlog` | >=24.4.0 | Structured JSON logging with `trace_id` per request |
| `prometheus-client` | >=0.21.0 | 30+ Prometheus metrics (latency, error rates, cache hits) |
| `opentelemetry-api` | >=1.29.0 | Distributed tracing API |
| `opentelemetry-sdk` | >=1.29.0 | OpenTelemetry SDK for trace export |
| `opentelemetry-instrumentation-fastapi` | >=0.50b0 | Auto-instrumentation of FastAPI routes |

### Background Jobs and Storage

| Package | Version | Purpose |
|---------|---------|---------|
| `arq` | >=0.26.1 | Async job queue backed by Redis (ingestion workflow) |
| `aioboto3` | >=13.3.0 | Async S3-compatible client for Cloudflare R2 artifact storage |

### Document Processing

| Package | Version | Purpose |
|---------|---------|---------|
| `pdfplumber` | >=0.11.0 | PDF text extraction for legal documents |
| `pypdf` | >=5.0.0 | PDF reading and metadata extraction |

### Protocol

| Package | Version | Purpose |
|---------|---------|---------|
| `mcp` | >=1.3.0 | Model Context Protocol SDK for stdio and HTTP/SSE transport |

## Development Dependencies

Development tools are installed via `pip install -e ".[dev]"`:

| Package | Version | Purpose |
|---------|---------|---------|
| `pytest` | >=8.0.0 | Test framework |
| `pytest-asyncio` | >=0.24.0 | Async test support (auto mode) |
| `pytest-cov` | >=6.0.0 | Coverage reporting |
| `ruff` | >=0.8.0 | Linter and formatter (replaces flake8 + isort + black) |
| `mypy` | >=1.13.0 | Static type checker |

Optional OCR dependencies (for scanned PDF processing):

| Package | Version | Purpose |
|---------|---------|---------|
| `pytesseract` | >=0.3.13 | OCR via Tesseract engine |
| `pdf2image` | >=1.17.0 | PDF to image conversion for OCR pipeline |

## External Services

Valter integrates with several external services, most of which are optional for local development:

| Service | Purpose | Required? | Notes |
|---------|---------|-----------|-------|
| Groq API | LLM for document classification, factual extraction, query expansion | Optional | Enables AI-powered features (21 fields per decision). Without it, these features are unavailable but the system still works. |
| Cloudflare R2 | Artifact storage (PDFs, JSON analysis files) | Optional | Canary rollout at 0%. Local filesystem is the default. |
| Railway | Production/staging deployment | Production only | Hosts API, MCP remote, and ARQ worker as separate services. |
| Neo4j Aura | Managed graph database | Staging/prod | Local Neo4j 5.x for development. PRs touching graph code must pass Aura validation in CI. |
| HuggingFace | Embedding model hosting | One-time download | `make download-model` fetches `rufimelo/Legal-BERTimbau-sts-base` (or the model specified by `VALTER_EMBEDDING_MODEL`). |

## Build and Deploy

| Tool | Purpose |
|------|---------|
| `make` | Canonical command interface. All common operations have `make` targets. Direct commands (`pytest`, `uvicorn`) only for debugging. |
| `uv` / `pip` | Dependency management. `uv` preferred for speed; `pip` as fallback. |
| Docker + docker-compose | Local development stack (PostgreSQL, Qdrant, Redis). Neo4j can run locally or via Aura. |
| Alembic | PostgreSQL schema migrations with upgrade/downgrade support. |
| GitHub Actions | CI/CD pipeline: `make lint` + `make test` + Aura validation for graph-related PRs. |
| Railway | Production deployment via `railway.json` and `Dockerfile`. Supports multi-service deploy (API, Worker, MCP remote). |
| Hatchling | Python build backend (configured in `pyproject.toml`). |

---
# docs/architecture/decisions.md
---


# Architecture Decision Records

> Significant architectural decisions, documented with context, options considered, and rationale.

## What Are ADRs?

Architecture Decision Records capture the "why" behind technical choices that are expensive to reverse. Each ADR documents the context that led to a decision, the alternatives considered, and the consequences — both positive and negative. When a future contributor asks "why does Valter use four databases instead of one?", the ADR provides the answer without requiring archeological digs through commit history.

ADRs in Valter follow a simple structure: context, decision, alternatives, consequences, and status. They are stored in `docs/adr/` as Markdown files.

## ADR Index

| # | Decision | Status | Date |
|---|----------|--------|------|
| 001 | MCP Remote Transport (streamable-HTTP + HMAC) | Accepted | 2026-02-21 |
| 002 | External Consumer Connectivity Baseline | Accepted | 2026-02-27 |
| 003 | Four Data Stores | Accepted | 2026-02 |
| 004 | Protocol-Based Dependency Injection | Accepted | 2026-02 |
| 005 | App Directory Deferral to v2.1 | Accepted | 2026-02 |

## ADR-001: MCP Remote Transport

**Full document:** [`docs/adr/0001-mcp-remoto-https.md`](/adr/0001-mcp-remoto-https)

### Context

Valter's MCP server originally operated only via `stdio` for local use with Claude Desktop and Claude Code. To enable ChatGPT Apps to consume Valter's 28 MCP tools, a remote transport over HTTPS was needed — without breaking the existing local mode.

Two operational modes already existed: tools that execute logic locally in the MCP process, and tools that bridge to the REST API via HTTP. The decision had to avoid regressing the `stdio` mode and avoid tight coupling between the MCP contract and the internal REST API contract.

### Decision

Adopt a **dedicated MCP remote service over HTTPS** with **Streamable HTTP** transport (SSE-compatible), running on a separate entry point (port 8001). The `stdio` mode remains the default for local development.

Key design choices:

- **Dual transport without regression** — `python -m valter.mcp` continues to work via `stdio`. The remote mode uses a separate entry point (`remote_server.py`).
- **Separation of concerns** — the MCP remote service exposes tool contracts to external clients. The REST API remains the internal domain backend. The MCP layer bridges to the API when needed, without exposing internal details.
- **Deny-by-default security** — the remote endpoint requires HMAC authentication. No credential means 401. Invalid or revoked credentials produce auditable log entries.
- **Observability** — structured logs with `trace_id` per request, latency and error metrics per tool, health endpoints for readiness and liveness.

### Alternatives Rejected

- **REST-only for external consumers** — would not satisfy the MCP protocol requirement and would force clients to integrate a custom REST contract.
- **Single process for API + MCP remote + stdio** — lower initial overhead but higher coupling, harder rollback, and risk of `stdio` regression.

### Consequences

The system now has a third runtime to operate, monitor, and secure. But the MCP contract is isolated from the REST API contract, `stdio` remains fully operational as a development and fallback mode, and external consumers get a governed integration path.


## ADR-002: External Consumer Connectivity

**Full document:** [`docs/adr/0002-conectividade-consumidores-externos.md`](/adr/0002-conectividade-consumidores-externos)

### Context

With the HTTPS endpoint live, API key authentication enabled, rate limiting in place, and basic observability operational, the next challenge was making external consumption predictable for integrators. Without a clear policy, the risks included: breaking contracts for already-onboarded consumers, silent regression in knowledge graph ranking quality, operational cost from avoidable incidents, and slow onboarding due to missing guides and examples.

### Decision

Adopt a contract-oriented external connectivity model with seven pillars:

1. **Stability classification** per endpoint (experimental, beta, stable)
2. **Formal versioning and deprecation** for payload changes
3. **API key authentication** with minimal scopes and controlled rotation
4. **Operational observability** with SLOs and error signals
5. **DX kit** for onboarding in less than one day
6. **Governance** with mandatory contract tests in CI
7. **Production-eval parity** for knowledge graph logic (same scoring in production and evaluation benchmarks)

### Consequences

External onboarding becomes faster and more predictable. The cost is increased discipline: changelog maintenance, contract test upkeep, and mandatory review for PRs that impact external-facing contracts.


## ADR-003: Four Data Stores

### Context

Legal data has four fundamentally different access patterns:

- **Relational queries** — documents with metadata, features, jobs, audit logs. Needs transactional integrity, flexible filtering, and schema migrations.
- **Semantic search** — finding decisions by meaning, not just keywords. Requires high-dimensional vector similarity with sub-second latency.
- **Graph traversal** — following relationships between decisions, criteria, legal devices, precedents, and ministers. Needs multi-hop queries, path finding, and community detection.
- **Ephemeral caching** — query results with short TTL, rate limiting counters, and background job queues. Needs sub-millisecond access.

### Decision

Use four specialized databases, each in its optimal role:

| Store | Workload |
|-------|----------|
| PostgreSQL 16 | Relational data: documents, features, metadata, jobs, memory, auth |
| Qdrant | Vector similarity: 768-dim embeddings with payload filtering |
| Neo4j 5.x / Aura | Graph: FRBR ontology with ~28K nodes and ~207K edges |
| Redis 7 | Cache: query results (180s TTL), rate limiting, ARQ job queue |

### Alternatives Considered

- **PostgreSQL + pgvector** — would consolidate two databases into one. Rejected because pgvector at the time lacked payload filtering and the operational overhead of a dedicated vector DB was minimal with Docker Compose.
- **Single PostgreSQL for everything** — graph queries would require expensive recursive CTEs instead of native graph traversal. The knowledge graph's multi-hop queries (citation chains of depth 5, community detection, PageRank) are not practical in SQL.
- **Managed-only solutions** — would increase cost and introduce vendor lock-in at an early stage.

### Consequences

Operational complexity is higher (four services to maintain). But each store delivers optimal performance for its workload, and Docker Compose abstracts the complexity for local development. In production, Railway manages the services with health checks.


## ADR-004: Protocol-Based Dependency Injection

### Context

Core business logic (retriever, enricher, verifier) needs to call data stores, but must remain testable and swappable. If core modules imported concrete store classes directly, tests would require live database connections and changing a store implementation would require modifying core logic.

### Decision

Define **runtime-checkable `Protocol` classes** in `core/protocols.py`. Each protocol specifies the interface a store must satisfy (method signatures with type hints). Concrete stores in `stores/` implement these protocols. FastAPI's `Depends()` mechanism wires concrete instances into route handlers via `api/deps.py`.

```python
# core/protocols.py
from typing import Protocol, runtime_checkable

@runtime_checkable
class DocStore(Protocol):
    async def get_document(self, doc_id: str) -> Document | None: ...
    async def search_documents(self, query: str, limit: int) -> list[Document]: ...

# stores/postgres_doc_store.py
class PostgresDocStore:  # No explicit inheritance needed
    async def get_document(self, doc_id: str) -> Document | None:
        ...  # Concrete implementation
```

### Alternatives Considered

- **Abstract base classes (ABCs)** — would work but require explicit inheritance, which adds coupling. Protocols use structural subtyping (duck typing with type safety).
- **Direct imports** — simplest approach but makes testing impossible without live databases and prevents swapping implementations.
- **DI frameworks (e.g., dependency-injector)** — adds a third-party dependency for a problem already solved by FastAPI's `Depends()`.

### Consequences

Core modules have zero coupling to concrete store implementations. Tests use mock stores that satisfy the same protocol. Swapping a store (for example, replacing `QdrantVectorStore` with a different vector database) requires only implementing the protocol and updating `deps.py` — no core code changes.


## ADR-005: App Directory Deferral

### Context

Submitting Valter to the ChatGPT App Directory was planned for v1.2. A premortem exercise revealed that with an estimated ~200 tool calls over 3 months from App Directory discovery, the cost of compliance (HTTPS enforcement, published privacy/terms policies, metadata packaging, security audit) would not be justified by the expected usage.

### Decision

Defer App Directory submission to **v2.1**. In the near term, prioritize serving direct users (1-2 law firms) and building the Reasoning Chain feature (v1.2), which provides more differentiation than marketplace visibility.

### Consequences

Reduced short-term visibility in the ChatGPT ecosystem. But resources are redirected to the Legal Reasoning Chain — the feature that transforms Valter from a search backend into a reasoning engine, which is the actual competitive differentiator.


## Pending Decisions

The following decisions are open and will be resolved as the project evolves:

| # | Decision | Context |
|---|----------|---------|
| 1 | R2 canary activation timeline | Currently at 0%. When to move to 5%, then 100%? Needs E2E validation. |
| 2 | Legacy route sunset | Routes with `Sunset: 2026-06-30` header. When to remove them entirely? |
| 3 | Privacy and terms authorship | Required for App Directory and external consumers. Who writes and hosts them? |
| 4 | Leci integration model | How will the legislation backend connect to Valter? Shared database, API calls, or graph federation? |
| 5 | Juca integration level | How tightly should the frontend couple to Valter's API? SDK, OpenAPI client, or direct HTTP? |
| 6 | Multi-tribunal: which court first | TRFs, TST, or STF? Each has different data formats, metadata, and verification rules. |
| 7 | Doutrina scope | Legal doctrine was deferred to a separate repository. What is its scope and how does it relate to Valter? |
| 8 | Embedding model: keep or migrate | Current model is Legal-BERTimbau (768-dim). Should Valter migrate to a larger model? Re-indexing ~23K documents is non-trivial. |
| 9 | Reasoning chain: sync vs async vs streaming | The Legal Reasoning Chain (v1.2) orchestrates 7 queries. Should it execute synchronously, as an async job, or stream partial results? |

---
# docs/architecture/diagrams.md
---


# Architecture Diagrams

> Visual representations of Valter's architecture using Mermaid diagrams.

## Component Diagram

This diagram shows all major components and their connections, from external consumers down to data stores. The arrows represent runtime dependencies — the direction data flows during a request.

```mermaid
graph TB
    subgraph Consumidores
        JUCA[Juca Frontend<br/>Next.js]
        CHATGPT[ChatGPT<br/>MCP Remoto]
        CLAUDE[Claude Desktop/Code<br/>MCP stdio]
    end

    subgraph "Valter — Entry Points"
        API[REST API<br/>FastAPI :8000]
        MCP_STDIO[MCP Server<br/>stdio]
        MCP_HTTP[MCP Server<br/>HTTP/SSE :8001]
        WORKER[ARQ Worker<br/>Background Jobs]
    end

    subgraph "Middleware Stack"
        CORS[CORS]
        METRICS_MW[Metrics IP Allowlist]
        TRACKING[Request Tracking<br/>trace_id + Prometheus]
        RATELIMIT[Rate Limiter<br/>Redis sliding window]
        AUTH[Auth Middleware<br/>API Key + Scopes]
    end

    subgraph "API Layer (api/)"
        ROUTES[11 Routers<br/>health, retrieve, verify,<br/>enrich, similar, graph,<br/>features, factual, ingest,<br/>memories, datasets]
        SCHEMAS[Pydantic v2 Schemas<br/>Request/Response]
        DEPS[Dependency Container<br/>DI via Depends()]
    end

    subgraph "Core Layer (core/)"
        RETRIEVER[HybridRetriever<br/>BM25 + Semantic + KG Boost]
        DVR[DualVectorRetriever<br/>Facts + Thesis]
        ENRICHER[DocumentEnricher<br/>IRAC + KG Context]
        VERIFIER[LegalVerifier<br/>Anti-hallucination]
        SIMILARITY[SimilarityFinder<br/>70% semantic + 30% structural]
        FACTUAL[FactualExtractor<br/>Groq LLM]
        QE[QueryExpander<br/>Multi-query RAG]
        WORKFLOW_ORCH[WorkflowOrchestrator<br/>Full Case Analysis]
        PROJUDI[ProjudiOrchestrator<br/>PDF Extraction]
        PHASE[PhaseAnalysis<br/>Interpreter + Rules]
        PROTOCOLS[Protocols<br/>Runtime-checkable interfaces]
    end

    subgraph "Store Layer (stores/)"
        PG_DOC[PostgresDocStore]
        PG_FEAT[PostgresFeaturesStore]
        PG_STJ[PostgresSTJStore]
        PG_INGEST[PostgresIngestStore]
        PG_MEM[PostgresMemoryStore]
        QDRANT_STORE[QdrantVectorStore]
        NEO4J_STORE[Neo4jGraphStore<br/>12+ query methods]
        REDIS_STORE[RedisCacheStore]
        GROQ_CLIENT[GroqLLMClient]
        ARTIFACT[ArtifactStorage<br/>R2 / Local]
    end

    subgraph "Embeddings"
        ENCODER[SentenceTransformerEncoder<br/>Legal-BERTimbau 768d]
        RERANKER[CrossEncoderReranker]
        RAIL_ENC[RailwayEncoder<br/>Remote HTTP]
        RAIL_RER[RailwayReranker<br/>Remote HTTP]
    end

    subgraph "Data Stores"
        PG[(PostgreSQL 16)]
        QD[(Qdrant)]
        N4J[(Neo4j / Aura)]
        RD[(Redis 7)]
        CF[(Cloudflare R2)]
        GROQ_API[(Groq API)]
    end

    subgraph "Observability"
        STRUCTLOG[structlog<br/>JSON + trace_id]
        PROM[Prometheus<br/>/metrics]
        OTEL[OpenTelemetry<br/>Tracing]
    end

    JUCA --> API
    CHATGPT --> MCP_HTTP
    CLAUDE --> MCP_STDIO
    MCP_STDIO --> API
    MCP_HTTP --> API

    API --> CORS --> METRICS_MW --> TRACKING --> RATELIMIT --> AUTH
    AUTH --> ROUTES
    ROUTES --> DEPS
    DEPS --> RETRIEVER & DVR & ENRICHER & VERIFIER & SIMILARITY & FACTUAL

    RETRIEVER --> QDRANT_STORE & NEO4J_STORE & PG_DOC & REDIS_STORE & ENCODER & RERANKER & QE
    WORKFLOW_ORCH --> PROJUDI & PHASE & PG_INGEST & ARTIFACT

    PG_DOC --> PG
    PG_FEAT --> PG
    PG_STJ --> PG
    PG_INGEST --> PG
    PG_MEM --> PG
    QDRANT_STORE --> QD
    NEO4J_STORE --> N4J
    REDIS_STORE --> RD
    GROQ_CLIENT --> GROQ_API
    ARTIFACT --> CF

    WORKER --> WORKFLOW_ORCH

    TRACKING --> STRUCTLOG & PROM & OTEL
```

### Reading the Diagram

The diagram is organized in layers from top to bottom, matching the dependency rule (`api/ -> core/ -> models/`):

- **Consumers** at the top connect to entry points. Juca uses the REST API directly. ChatGPT uses MCP over HTTP/SSE with HMAC authentication. Claude uses MCP over stdio locally.
- **Entry Points** represent the four runtimes. The REST API and both MCP servers ultimately call the same core logic. The ARQ Worker handles background ingestion jobs.
- **Middleware Stack** processes every HTTP request in sequence. Each middleware adds a capability: CORS headers, metrics access control, request tracing, rate limiting, and authentication.
- **API Layer** contains 11 routers (one per domain), Pydantic schemas for request/response validation, and the dependency container that wires stores into core functions.
- **Core Layer** holds all business logic. No module here imports from `stores/` directly — they receive store instances through dependency injection via the `Protocols` module.
- **Store Layer** provides concrete implementations for each data backend. The `GroqLLMClient` is here because it is an external data provider, not because it is a traditional database.
- **Embeddings** are handled by either local models (SentenceTransformer, CrossEncoder) or remote Railway-hosted instances, selected by configuration.
- **Data Stores** are the actual database processes (PostgreSQL, Qdrant, Neo4j, Redis) plus external APIs (Groq, Cloudflare R2).
- **Observability** collects structured logs, Prometheus metrics, and OpenTelemetry traces from the middleware tracking layer.

## Search Pipeline

The hybrid search pipeline is Valter's most complex data flow. It combines three search strategies (BM25 keyword search, semantic vector search, and knowledge graph boosting) into a single ranked result set.

```mermaid
flowchart LR
    Q[Query] --> CACHE{Cache?}
    CACHE -->|hit| RESULT
    CACHE -->|miss| QE[Query Expansion<br/>Groq LLM]
    QE --> ENCODE[Encode<br/>Legal-BERTimbau]
    ENCODE --> PAR{Parallel}
    PAR --> BM25[BM25<br/>rank-bm25]
    PAR --> SEM[Semantic<br/>Qdrant cosine]
    BM25 --> MERGE[Merge<br/>Weighted / RRF]
    SEM --> MERGE
    MERGE --> FETCH[Fetch Docs<br/>PostgreSQL]
    FETCH --> FILTER[Filters<br/>ministro, data, tipo]
    FILTER --> KG{KG Boost?}
    KG -->|yes| BOOST[Neo4j Batch<br/>KG Boost]
    KG -->|no| RERANK
    BOOST --> RERANK[Reranking<br/>Cross-Encoder]
    RERANK --> STORE_CACHE[Store Cache<br/>Redis TTL 180s]
    STORE_CACHE --> RESULT[Response]
```

### Pipeline Stages

1. **Cache check** — Redis is queried first with the exact query hash. Cache TTL is 180 seconds. On a hit, the cached result is returned immediately.
2. **Query expansion** — if Groq is enabled, the query is expanded into up to 3 semantic variants using multi-query RAG. This improves recall for ambiguous legal queries.
3. **Encoding** — the query (and its expansions) are encoded into 768-dimensional vectors using Legal-BERTimbau (`rufimelo/Legal-BERTimbau-sts-base`).
4. **Parallel search** — BM25 keyword search against PostgreSQL and semantic cosine similarity search against Qdrant run concurrently.
5. **Merge** — results from both strategies are combined using either weighted fusion or Reciprocal Rank Fusion (RRF), configurable per request.
6. **Document fetch** — full document data is loaded from PostgreSQL for the merged candidate set.
7. **Filters** — post-retrieval filters apply (minister, date range, decision type, result).
8. **KG Boost** — if enabled, Neo4j is queried in batch to score each candidate based on shared criteria, facts, evidence, and legal devices in the knowledge graph. The KG boost score is blended with the search score.
9. **Reranking** — a cross-encoder model re-scores the top candidates based on query-document relevance, producing the final ranking.
10. **Cache store** — the final result is stored in Redis with a 180-second TTL.

## Ingestion Workflow

The ingestion workflow handles the complete lifecycle of a legal case analysis, from PDF upload through AI-powered extraction to human-reviewed artifacts. The workflow is managed by a state machine in `core/workflow_state_machine.py`.

```mermaid
stateDiagram-v2
    [*] --> pending: PDF uploaded

    pending --> extracting: Start extraction
    extracting --> segmented: PROJUDI segments extracted
    extracting --> failed: Extraction error

    segmented --> classifying: Start classification
    classifying --> classified: Groq classification complete
    classifying --> failed: Classification error

    classified --> analyzing: Start phase analysis
    analyzing --> analyzed: Phases interpreted
    analyzing --> failed: Analysis error

    analyzed --> matching: Start jurisprudence matching
    matching --> matched: Cases matched + suggestions generated
    matching --> failed: Matching error

    matched --> reviewing: Submit for human review
    reviewing --> approved: Human approves
    reviewing --> rejected: Human rejects
    rejected --> analyzing: Reprocess from analysis

    approved --> generating: Generate artifacts
    generating --> completed: Artifacts stored (R2 / local)
    generating --> failed: Generation error

    failed --> pending: Retry
    completed --> [*]
```

### Workflow Stages

- **pending** — a PDF has been uploaded but no processing has started.
- **extracting** — the PROJUDI pipeline (`core/projudi_pipeline.py`) segments the document, classifying each section with confidence scores and applying sibling inheritance for ambiguous segments.
- **classifying** — Groq LLM extracts 21 structured fields from the document (facts, legal thesis, outcome, minister, category, etc.).
- **analyzing** — the phase analysis pipeline (`core/phase_interpreter.py` + rules) produces deterministic interpretation of legal proceeding phases with jurisprudence recommendations per phase.
- **matching** — the hybrid retriever finds similar cases, and the system generates suggestions for the lawyer.
- **reviewing** — a human (lawyer) reviews the analysis and suggestions, approving or rejecting each phase. Rejections trigger reprocessing.
- **generating** — approved analyses are serialized as JSON and PDF artifacts, stored in Cloudflare R2 (or local filesystem).
- **completed** — the workflow is finished. All state transitions are recorded as auditable events in a JSONL manifest.

:::note
The state machine enforces valid transitions. For example, a workflow cannot jump from `pending` to `reviewing` — it must pass through all intermediate states. Invalid transitions raise errors that are logged for audit.
:::

## Data Model (FRBR Ontology)

The Neo4j knowledge graph is structured around the FRBR (Functional Requirements for Bibliographic Records) ontology, adapted for Brazilian legal documents. This ontology distinguishes between the abstract intellectual content of a law or decision (Work), a specific version of that content (Expression), and its physical form (Manifestation).

### Node Types

| Node Type | Description | Example |
|-----------|-------------|---------|
| `Decision` | An STJ court decision (the primary entity) | REsp 1.234.567/SP |
| `Criterion` | A legal criterion or test applied in decisions | "boa-fe objetiva", "nexo causal" |
| `LegalDevice` | A specific article, paragraph, or clause of legislation | Art. 14 do CDC, Art. 927 do CC |
| `Precedent` | A precedent cited by decisions | Sumula 297/STJ |
| `Minister` | An STJ minister (judge) | Min. Nancy Andrighi |
| `Category` | A legal subject area | Direito do Consumidor, Responsabilidade Civil |
| `Fact` | A factual element relevant to the case | "defeito do produto", "dano moral" |
| `Evidence` | A type of evidence referenced | "prova pericial", "documento" |

### Relationship Types

| Relationship | From | To | Meaning |
|-------------|------|-----|---------|
| `APLICA` | Decision | LegalDevice | The decision applies this legal provision |
| `USA_CRITERIO` | Decision | Criterion | The decision uses this legal criterion |
| `CITA` | Decision | Decision | One decision cites another |
| `CITA_PRECEDENTE` | Decision | Precedent | The decision cites this precedent |
| `RELATOR_DE` | Minister | Decision | This minister was the reporting judge |
| `PERTENCE_A` | Decision | Category | The decision belongs to this subject area |
| `TEM_FATO` | Decision | Fact | The decision involves this fact |
| `TEM_PROVA` | Decision | Evidence | The decision references this evidence |
| `DIVERGE_DE` | Decision | Decision | The decisions have divergent outcomes on the same criterion |

### Scale

The current graph contains approximately **28,482 nodes** and **207,163 edges**, representing the relational structure of STJ jurisprudence. The 12 graph analytics endpoints in the API expose different traversal and aggregation patterns over this graph.

## Deployment Architecture

In production, Valter runs on Railway with the following service layout:

```
                    Internet
                       │
          ┌────────────┼────────────┐
          │            │            │
          ▼            ▼            ▼
    ┌──────────┐ ┌──────────┐ ┌──────────┐
    │ REST API │ │MCP Remote│ │   Juca   │
    │ :8000    │ │ :8001    │ │ (Next.js)│
    │ FastAPI  │ │ HMAC Auth│ │          │
    └────┬─────┘ └────┬─────┘ └────┬─────┘
         │            │            │
         │            ▼            │
         │      ┌──────────┐       │
         │      │ REST API │◄──────┘
         │      │ (bridge)  │
         │      └──────────┘
         │
    ┌────┴─────────────────────────────┐
    │         Internal Network         │
    │                                  │
    │  ┌──────┐ ┌──────┐ ┌──────────┐ │
    │  │  PG  │ │Qdrant│ │  Redis   │ │
    │  │  16  │ │      │ │    7     │ │
    │  └──────┘ └──────┘ └──────────┘ │
    │                                  │
    │  ┌──────────┐                    │
    │  │ARQ Worker│                    │
    │  │(ingest)  │                    │
    │  └──────────┘                    │
    └──────────────────────────────────┘

    External Services:
    ┌──────────┐ ┌──────────┐ ┌──────┐
    │Neo4j Aura│ │Groq API  │ │  R2  │
    │(managed) │ │(LLM)     │ │(S3)  │
    └──────────┘ └──────────┘ └──────┘
```

### Key Points

- **REST API** and **MCP Remote** are separate Railway services running the same Docker image with different start commands. This provides independent scaling and rollback.
- **PostgreSQL, Qdrant, and Redis** run as Railway plugins on the internal network, not exposed to the internet.
- **Neo4j Aura** is a managed external service (not self-hosted). PRs touching graph code must pass the `Aura Validation` CI workflow before merge.
- **Groq API** and **Cloudflare R2** are external services accessed over HTTPS. Both are optional — the system degrades gracefully without them (no AI features, local filesystem for artifacts).
- **ARQ Worker** runs as a separate process consuming the Redis job queue for background ingestion workflows.
- **Juca** (the Next.js frontend) calls the REST API directly. It is a separate deployment, not part of the Valter codebase.

---
# docs/features/index.md
---


# Features

Valter ships with 33 implemented features spanning search, graph analytics, LLM integration, document processing, verification, and infrastructure. This page provides the full matrix and links to detailed documentation per domain.

## Feature Matrix

| Feature | Status | Endpoint / Module | Docs |
|---------|--------|-------------------|------|
| Hybrid Search | Implemented | `POST /v1/retrieve` | [Hybrid Search](hybrid-search/) |
| Graph Analytics (12 endpoints) | Implemented | `POST /v1/graph/*` | [Graph Analytics](graph-analytics/) |
| MCP Server (28 tools) | Implemented | stdio + HTTP/SSE | [MCP Server](mcp-server/) |
| Ingestion Workflow (17 endpoints) | Implemented | `POST /v1/ingest/*` | [Ingestion Workflow](ingestion-workflow/) |
| Verification & Enrichment | Implemented | `POST /v1/verify`, `/v1/enrich` | [Verification & Enrichment](verification-enrichment/) |
| Observability | Implemented | `/metrics`, structlog, OTel | [Observability](observability/) |
| Legal Reasoning Chain | Planned (v1.2) | `POST /v1/reasoning-chain` | [Reasoning Chain](reasoning-chain/) |

## By Domain

### Search & Retrieval

Search is Valter's core query interface. It combines multiple strategies into a single pipeline that outperforms keyword-only or vector-only approaches.

- **Hybrid Search** -- BM25 lexical + semantic vectors (Qdrant) + KG boost (Neo4j), with weighted and RRF merge strategies. Endpoint: `POST /v1/retrieve`.
- **Dual-Vector Search** -- Encodes facts and thesis separately, producing a divergence report. Endpoint: `POST /v1/factual/dual-search`.
- **Query Expansion** -- Multi-query RAG via Groq LLM generates up to 3 legal term variants per query. Integrated into the retriever.
- **Cross-Encoder Reranking** -- Reorders top results using a cross-encoder model (local or Railway-hosted). Integrated into the retriever.
- **Feature Search** -- 9 combinable filters over 21 AI-extracted fields (categorias, resultado, tipo_decisao, argumento_vencedor, etc.). Endpoint: `POST /v1/search/features`.
- **Cursor-Based Pagination** -- Opaque cursor pagination across listing endpoints.

See [Hybrid Search](hybrid-search/) for full pipeline details.

### Knowledge Graph

The Neo4j knowledge graph contains decisions, criteria, legal statutes, precedents, ministers, and categories connected by relationship types like CITA, APLICA, DIVERGE_DE, and RELATOR_DE.

- **12 graph analytics endpoints** under `POST /v1/graph/*`
- **Divergence detection** between ministers on specific legal criteria
- **Optimal argument composition** with success rates per category and outcome
- **Temporal evolution tracking** for criteria over time with trend analysis
- **Minister profiling** with decision patterns, divergences, and top precedents
- **PageRank, communities, citations** for structural graph analysis
- **Structural similarity, shortest-path, graph embeddings** for advanced case comparison

See [Graph Analytics](graph-analytics/) for the full endpoint reference.

### LLM Integration (MCP)

Valter exposes all capabilities as 28 Model Context Protocol tools, allowing any MCP-compatible LLM to act as a legal research assistant.

- **28 MCP tools** organized across knowledge, graph, and workflow domains
- **stdio server** for Claude Desktop and Claude Code (`python -m valter.mcp`)
- **HTTP/SSE remote server** for ChatGPT and other remote clients (`make mcp-remote`, port 8001)
- **API key + HMAC authentication** for remote transport
- **Rate limiting** per API key with configurable per-minute limits

See [MCP Server](mcp-server/) for tool categories and setup instructions.

### Document Processing

The ingestion workflow transforms a raw case PDF into a structured legal analysis with phase identification, jurisprudence matching, and human-reviewed suggestions.

- **Full case workflow** with 17 endpoints under `/v1/ingest/*`
- **PROJUDI pipeline** for first-instance process extraction (segmentation, classification, confidence scoring)
- **Phase analysis** via 5 core modules (interpreter, matcher, rules, recommender, jurisprudence)
- **State machine** with validated transitions and auditable events in JSONL format
- **Human-in-the-loop review** at phase and final levels, with immutable reprocessing
- **Artifact storage** via local filesystem or Cloudflare R2 with deterministic canary rollout

See [Ingestion Workflow](ingestion-workflow/) for the full pipeline description.

### Verification & Intelligence

These features ensure that legal references used by LLMs and frontends are accurate and contextually enriched.

- **Anti-hallucination verification** validates sumulas (STJ/STF), minister names, process numbers (CNJ format), and legislation references against known datasets
- **IRAC analysis** classifies decision text into Issue, Rule, Application, and Conclusion using heuristic regex patterns
- **KG enrichment** adds criteria, legal statutes, precedents, and legislation from the Neo4j graph
- **Factual extraction** via Groq LLM produces structured factual digests and legal thesis for dual-vector search
- **Temporal validity** checks whether referenced legal norms remain in effect

See [Verification & Enrichment](verification-enrichment/) for implementation details.

### Infrastructure

- **API key auth + scopes + audit** with hashed keys, path-scoped permissions, and persistent audit logging
- **Rate limiting** via Redis sliding window per API key (both REST API and MCP)
- **Observability** with structlog JSON logging, 30+ Prometheus metrics, and OpenTelemetry tracing
- **CI/CD** via GitHub Actions (lint, test, Aura validation)
- **Railway deployment** for API, worker, and MCP remote server
- **Session memory** with key-value TTL storage (60s to 30 days) in PostgreSQL

See [Observability](observability/) for monitoring details.

---
# docs/features/hybrid-search.md
---


# Hybrid Search

Valter's search pipeline combines five strategies into a single query flow: BM25 lexical match, semantic vector similarity, knowledge graph boost, LLM-powered query expansion, and cross-encoder reranking. The endpoint is `POST /v1/retrieve`.

The core formula is:

```
score_final(d, q) = w_bm25 * norm(BM25) + w_sem * norm(cosine) + w_kg * kg_boost
```

Default weights: BM25 = 0.5, semantic = 0.4, KG = 0.1.

## Pipeline Stages

Every search request flows through 8 stages. Each stage is independently measurable and most can be toggled via request parameters.

### 1. Cache Check

The retriever computes a cache key from the hash of query text, filters, strategy, and feature flags. If a cached response exists in Redis (TTL 180s), it returns immediately without executing the pipeline.

A cache hit sets `cache_hit: true` in the response and records the latency as the cache lookup time only. This is the fastest path through the system.

### 2. Query Expansion

When `expand_query` is enabled, the `LLMQueryExpander` generates up to 3 query variants using a Groq LLM. The expansion prompt is domain-specific for Brazilian legal search:

```python
# From core/query_expander.py
# Rules for expansion:
# 1. Legal synonyms (e.g., "dano moral" -> "dano extrapatrimonial")
# 2. Procedural equivalents (e.g., "recurso especial" -> "REsp")
# 3. Thesis/statute reformulations (e.g., "prazo prescricional" -> "prescricao quinquenal art. 206 CC")
```

The expander has a configurable timeout (default 3.0s). On timeout or failure, expansion returns an empty list and the pipeline continues with the original query only. The expanded variants are included in the response as `expansion_queries`.

### 3. Encoding

The original query and any expansion variants are encoded into dense vectors using the embedding model defined by `VALTER_EMBEDDING_MODEL` (default: `rufimelo/Legal-BERTimbau-sts-base`, 768 dimensions).

Two encoder backends are available:

- **Local**: `SentenceTransformerEncoder` -- loads the model in-process
- **Remote**: `RailwayEncoder` -- sends HTTP requests to a dedicated GPU service on Railway

### 4. Parallel Retrieval

BM25 and semantic search run concurrently using `asyncio.gather` for latency optimization.

- **BM25**: Uses the `rank_bm25` library (`BM25Okapi`) over tokenized PostgreSQL documents. The index is built at startup from all documents' ementa, tese, and razoes_decidir fields.
- **Semantic**: Performs cosine similarity search against Qdrant using the encoded query vector. When query expansion is active, all variant vectors are searched and results are merged.

Both retrieval paths fetch up to `top_k * 3` candidates (capped at 100) to ensure sufficient diversity before merging.

### 5. Merge

Candidates from BM25 and semantic search are combined using one of two strategies, selectable via the `strategy` request parameter:

- **weighted** (default): Normalizes BM25 and semantic scores independently, then combines them using configurable weights. The `SearchWeights` dataclass defaults to `bm25=0.5, semantic=0.4, kg=0.1`.
- **rrf** (Reciprocal Rank Fusion): Position-based merging using the formula `1 / (k + rank)` with `k=60`. This approach is less sensitive to score distribution differences between retrievers.

If one retriever returns empty results, the strategy automatically falls back to the other retriever's results only.

### 6. Filtering

Post-retrieval filters are applied to the merged candidate list. Available filters in `SearchFilters`:

| Filter | Type | Behavior |
|--------|------|----------|
| `ministro` | string | Normalized to uppercase for case-insensitive matching |
| `data_inicio` | string (YYYYMMDD) | Minimum decision date |
| `data_fim` | string (YYYYMMDD) | Maximum decision date |
| `tipos_recurso` | list[string] | Extracted from processo number format |
| `resultado` | string | Decision outcome filter |

:::note
Filters are applied after retrieval, so the returned count may be less than `top_k` when filters exclude candidates.
:::

### 7. Knowledge Graph Boost

When `include_kg` is enabled and the Neo4j graph store is available, each result receives a relevance boost based on its graph connections.

The KG boost computation (`compute_kg_boost_from_entities` in `stores/graph.py`) evaluates three entity types for each decision:

- **Criterios** -- legal criteria connected to the decision, weighted by a qualitative `peso` multiplier
- **Fatos** -- factual elements linked in the graph
- **Provas** -- evidence entities

Boost queries run concurrently with configurable concurrency (`VALTER_KG_BOOST_MAX_CONCURRENCY`, default 20). The boost score is combined with the search score using the KG weight (default 0.1).

:::tip
If Neo4j is unavailable, results still return without KG boost. The system logs a warning but does not fail the request.
:::

### 8. Reranking

When `rerank` is enabled and a reranker is configured, the top results are reordered by a cross-encoder model that scores query-document pairs for relevance.

Two reranker backends are available:

- **Local**: `CrossEncoderReranker` -- runs the cross-encoder model in-process
- **Remote**: `RailwayReranker` -- sends HTTP requests to a dedicated GPU service

Reranking typically adds 200-500ms of latency but improves precision for the top results.

## Dual-Vector Search

The dual-vector retriever (`DualVectorRetriever` in `core/dual_vector_retriever.py`) takes a different approach: instead of a single query vector, it encodes facts and legal thesis separately and searches the corpus with each.

**Endpoint**: `POST /v1/factual/dual-search`

The divergence report classifies results into three categories:

| Category | Meaning |
|----------|---------|
| `fact_only` | Factually similar but legally different |
| `thesis_only` | Legally similar but factually different |
| `overlap` | Similar in both factual and legal dimensions |

This separation is valuable for identifying cases where the same facts led to different legal reasoning, or where the same legal thesis was applied to different factual scenarios.

## Feature Search

Feature search provides structured filtering over 21 AI-extracted fields (generated by Groq LLM classification).

**Endpoint**: `POST /v1/search/features`

Nine combinable filters with AND semantics (except `categorias` which uses OR/ANY):

| Filter | Match Type |
|--------|-----------|
| `categorias` | OR/ANY semantics across listed categories |
| `dispositivo_norma` | Exact match (e.g., "CDC", "CC/2002") |
| `resultado` | Exact, case-sensitive |
| `unanimidade` | Boolean |
| `tipo_decisao` | Exact, case-sensitive |
| `tipo_recurso` | Exact, case-sensitive |
| `ministro_relator` | Exact, case-sensitive |
| `argumento_vencedor` | Partial ILIKE, case-insensitive |
| `argumento_perdedor` | Partial ILIKE, case-insensitive |

## Configuration

Key environment variables controlling the search pipeline:

| Variable | Default | Purpose |
|----------|---------|---------|
| `VALTER_EMBEDDING_MODEL` | `rufimelo/Legal-BERTimbau-sts-base` | Embedding model for encoding |
| `VALTER_EMBEDDING_DIMENSION` | `768` | Vector dimension |
| `VALTER_KG_BOOST_BATCH_ENABLED` | `true` | Enable batch KG boost |
| `VALTER_KG_BOOST_MAX_CONCURRENCY` | `20` | Max concurrent Neo4j queries for KG boost |
| `VALTER_QUERY_EXPANSION_MAX_VARIANTS` | `3` | Max query expansion variants |

---
# docs/features/graph-analytics.md
---


# Graph Analytics

Valter exposes 12 endpoints under `POST /v1/graph/*` that query the Neo4j knowledge graph for legal reasoning tasks: divergence detection, optimal argument composition, temporal trend analysis, minister profiling, and structural similarity.

## Overview

The knowledge graph contains decisions, criteria, legal statutes (dispositivos), precedents, ministers, and categories connected by typed relationships such as CITA, APLICA, DIVERGE_DE, and RELATOR_DE. The ontology is adapted from FRBR (Functional Requirements for Bibliographic Records) for Brazilian law.

All graph endpoints share common patterns:

- **15-second timeout** per Neo4j query, enforced via `asyncio.wait_for`
- **Structured error handling**: Neo4j connection/driver errors return `SERVICE_UNAVAILABLE`; programming bugs (`KeyError`, `TypeError`) propagate as 500 errors for visibility
- **Trace ID** correlation in every response via `MetaResponse(trace_id, latency_ms)`
- **Caching** via Redis for frequently accessed queries

## Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/v1/graph/divergencias` | POST | Detect split outcomes on legal criteria |
| `/v1/graph/divergencias/turma` | POST | Analyze split outcomes by minister/topic |
| `/v1/graph/optimal-argument` | POST | Find highest success rate arguments |
| `/v1/graph/optimal-argument-by-ministro` | POST | Compare minister vs category averages |
| `/v1/graph/ministro-profile` | POST | Comprehensive minister behavior profile |
| `/v1/graph/temporal-evolution` | POST | Track criteria trends over time |
| `/v1/graph/citation-chain` | POST | Trace outbound citation edges |
| `/v1/graph/pagerank` | POST | Rank influential decisions |
| `/v1/graph/communities` | POST | Find high-overlap decision pairs |
| `/v1/graph/structural-similarity` | POST | Compare two decisions across 5 dimensions |
| `/v1/graph/shortest-path` | POST | Find connection chains between decisions |
| `/v1/graph/graph-embeddings` | POST | Compute 7D structural vectors |

### Divergence Detection

**`POST /v1/graph/divergencias`**

Finds legal criteria where decisions have split outcomes (provido vs improvido). The divergence score is computed as `minority_count / total_count` -- a balanced split (50/50) produces the highest score.

**Parameters**: `categoria_id` (optional exact filter), `limit` (1-50, default 10).

**Use case**: Identify where law is unsettled, find active disagreements between ministers, and locate counter-arguments for a given legal position.

**`POST /v1/graph/divergencias/turma`**

Analyzes split outcomes for criteria matching a topic substring and aggregates counts by minister. Despite the name, the current graph query uses minister-level aggregation rather than explicit turma (court division) metadata.

**Parameters**: `tema` (required, case-insensitive substring match on criterion names).

### Optimal Argument

**`POST /v1/graph/optimal-argument`**

Given a category and desired outcome, finds the argument paths with the highest success rates. The graph store runs multiple Cypher queries internally to compute success rates across three argument types:

| Type | Internal Limit | Min Decisions |
|------|---------------|---------------|
| Criterio | 5 | 2 |
| Dispositivo (statute) | 3 | 2 |
| Precedente | 3 | 2 |

Each argument chain step includes: fact, criterion, success_rate, dispositivo, and supporting decisions.

**Parameters**: `categoria_id` (required), `resultado_desejado` (provido/improvido/parcialmente provido), `tipo_argumento` (all/criterio/dispositivo/precedente), `min_decisions`, `top_k` (1-50).

### Optimal Argument by Minister

**`POST /v1/graph/optimal-argument-by-ministro`**

Compares a specific minister's success rates against the category average for each argument. Returns a delta per argument:

- **Positive delta**: argument works better with this minister than the court average
- **Negative delta (< -0.1)**: argument to avoid with this minister

Internal graph limits: up to 10 criteria, 5 statutes, 5 precedents. Minister names are auto-normalized to uppercase.

**Parameters**: `categoria_id` (required), `ministro` (required), `resultado_desejado`, `tipo_argumento`, `min_decisions`, `min_category_decisions`, `top_k`.

### Temporal Evolution

**`POST /v1/graph/temporal-evolution`**

Tracks how a legal criterion's application changes over time. Aggregates decision counts by year or month, with provido/improvido split per period, and computes a heuristic trend label (growing, declining, or stable).

**Parameters**: `criterio` (required), `granularity` (year/month), `periodo_inicio`, `periodo_fim`.

:::note
Period format must match granularity: use `YYYY` for year granularity and `YYYY-MM` for month. Mismatched formats return a validation error.
:::

### Minister Profile

**`POST /v1/graph/ministro-profile`**

Returns a comprehensive profile of a minister's judicial behavior, built from 5 internal Cypher queries:

- **Summary**: total decisions, date range
- **Top criteria** used in decisions (capped at 10 internally)
- **Outcome distribution**: provido, improvido, parcialmente provido counts
- **Peer divergences**: ministers who disagree most frequently (capped at 20)
- **Most cited decisions**: the minister's most influential rulings (capped at 5)

**Parameters**: `ministro` (required), `include_divergencias` (default true), `include_precedentes` (default true), `limit_criterios` (1-50, cannot exceed internal cap of 10).

### PageRank

**`POST /v1/graph/pagerank`**

Ranks the most influential decisions using a simplified scoring formula:

```
score = in_citations * 10 + second_order_citations * 3
```

This is based on citation patterns in the graph rather than the full PageRank algorithm. The `min_citations` filter is applied in post-processing.

**Parameters**: `limit` (1-100, default 20), `min_citations` (default 0).

### Communities

**`POST /v1/graph/communities`**

Returns high-overlap decision pairs based on shared legal criteria. This is pairwise co-occurrence detection (size=2 per item), not full graph-theory community detection.

**Parameters**: `min_shared` (minimum shared criteria, default 3), `limit` (1-100, default 20).

### Citations

**`POST /v1/graph/citation-chain`**

Traces outbound citation edges from a root decision through `CITA_PRECEDENTE` relationships up to a configurable depth. Returns citation nodes, edges, and a `max_depth_reached` flag. Does not include inbound citations (decisions that cite the root).

**Parameters**: `decisao_id` (required), `max_depth` (1-5, default 3).

### Structural Similarity

**`POST /v1/graph/structural-similarity`**

Compares two decisions across five graph dimensions using weighted Jaccard scoring:

| Dimension | What It Measures |
|-----------|-----------------|
| Criteria | Shared legal criteria between decisions |
| Facts | Shared factual elements |
| Evidence | Shared evidence types |
| Statutes | Shared legal statutes cited |
| Precedents | Shared precedent citations |

Returns per-dimension stats and a `weighted_score` in [0, 1].

**Parameters**: `source_id` (required), `target_id` (required).

### Shortest Path

**`POST /v1/graph/shortest-path`**

Finds a bidirectional shortest path between two decisions using all relationship types, up to a configurable depth. Returns the path nodes, edges with real relationship types, and `found=false` when no path exists within the depth limit.

**Parameters**: `source_id` (required), `target_id` (required), `max_depth` (1-20, default 10).

### Graph Embeddings

**`POST /v1/graph/graph-embeddings`**

Computes a 7-dimensional structural vector per decision, capturing graph topology rather than text semantics:

| Dimension | Description |
|-----------|-------------|
| 1 | Criteria count |
| 2 | Facts count |
| 3 | Evidence count |
| 4 | Statutes count |
| 5 | Inbound citation count |
| 6 | Outbound citation count |
| 7 | Encoded outcome (provido/improvido/parcial) |

Supports two modes: **batch** (explicit `decisao_ids` list) and **sample** (random decisions limited by `limit`, default 100, max 500).

---
# docs/features/mcp-server.md
---


# MCP Server

Valter exposes 28 MCP tools via two transport modes -- stdio for Claude Desktop/Code and HTTP/SSE for ChatGPT -- enabling any MCP-compatible LLM to act as a legal research assistant backed by real jurisprudence data.

## What is MCP?

Model Context Protocol (MCP) is an open standard for structured communication between LLMs and external tools. Instead of building custom API integrations for each LLM provider, MCP provides a single interface that any compatible client can consume.

For Valter, this means that Claude Desktop, Claude Code, ChatGPT, and any future MCP client can all use the same 28 tools without separate integration work.

## Transport Modes

### stdio (Claude Desktop / Claude Code)

The stdio transport runs as a subprocess that communicates via JSON-RPC over standard input/output. No network configuration is needed.

```bash
# Start the MCP server in stdio mode
python -m valter.mcp
```

Logs are directed to stderr so they do not interfere with the JSON-RPC protocol on stdout.

### HTTP/SSE (ChatGPT and Remote Clients)

The remote transport runs a Starlette ASGI application that accepts Streamable HTTP requests with HMAC authentication, deployed alongside the REST API on Railway.

```bash
# Start the remote MCP server on port 8001
make mcp-remote
```

Architecture: MCP remote server (port 8001) bridges to the REST API (port 8000) via httpx. The remote server handles authentication, rate limiting, and MCP protocol framing, while the API handles actual data operations.

The runtime configuration is defined in `MCPRemoteRuntimeConfig`:

```python
# From mcp/remote_server.py
@dataclass(frozen=True)
class MCPRemoteRuntimeConfig:
    transport: str          # "stdio" or "streamable-http"
    host: str
    port: int
    path: str
    auth_mode: str          # "none" or "api_key"
    api_keys: tuple[str, ...]
    rate_limit_per_minute: int  # default 60
    # ...
```

## Tool Categories

### Knowledge Tools (7 tools)

These tools cover search, verification, enrichment, and document retrieval.

| Tool | Description |
|------|-------------|
| `search_jurisprudence` | Hybrid BM25 + semantic search with optional KG boost, reranking, query expansion, and cursor pagination. Start here when looking for candidate cases. |
| `verify_legal_claims` | Validates sumulas, minister names, process numbers, and legislation references against reference data. Returns hallucination risk metrics. |
| `get_irac_analysis` | Heuristic IRAC analysis (regex-based) and knowledge graph context for one document. Requires a known document_id. |
| `find_similar_cases` | Finds similar cases using 70% semantic + 30% structural KG overlap. Falls back to semantic-only on timeout. |
| `get_document_integra` | Retrieves the full text (inteiro teor) of a specific STJ decision. Check `has_integra` in search results before calling. |
| `remember` | Stores a session-scoped key-value memory in PostgreSQL with configurable TTL (60s to 30 days). |
| `recall` | Retrieves a previously stored session memory by key. Returns `found=false` when missing or expired. |

### Graph Tools (10 tools)

These tools query the Neo4j knowledge graph for analytical insights.

| Tool | Description |
|------|-------------|
| `get_divergencias` | Finds criteria with split outcomes (provido vs improvido), ranked by divergence score. |
| `get_turma_divergences` | Analyzes split outcomes by minister for a topic substring. |
| `get_optimal_argument` | Computes argument success rates (criteria, statutes, precedents) for a category and desired outcome. |
| `get_optimal_argument_by_ministro` | Compares minister-specific success rates vs category averages, with delta and recommendations. |
| `get_ministro_profile` | Comprehensive minister profile: decisions, criteria, outcome distribution, peer divergences. |
| `get_temporal_evolution` | Aggregates jurisprudence counts over time with provido/improvido split and trend label. |
| `search_features` | Structured search over 21 AI-extracted document features with 9 combinable filters. |
| `get_citation_chain` | Traces outbound citations from a root decision up to 5 hops. |
| `get_pagerank` | Ranks the most influential decisions by citation-based scoring. |
| `get_communities` | Returns high-overlap decision pairs based on shared legal criteria. |

### Structural Analysis Tools (3 tools)

| Tool | Description |
|------|-------------|
| `get_structural_similarity` | Compares two decisions across 5 graph dimensions (criteria, facts, evidence, statutes, precedents) with weighted Jaccard scoring. |
| `get_shortest_path` | Finds bidirectional shortest path between two decisions using all relationship types. |
| `get_graph_embeddings` | Computes 7D structural vectors per decision (criteria/facts/evidence/statutes counts, citations, encoded outcome). |

### Workflow Tools (8 tools)

These tools manage the full case analysis workflow from PDF submission through human review.

| Tool | Description |
|------|-------------|
| `submit_case_pdf_analysis` | Starts an async PDF analysis workflow. Accepts local_path (multipart) or pdf_base64 (JSON). |
| `get_case_pdf_analysis_status` | Polls workflow status for a previously submitted analysis. |
| `get_case_pdf_analysis_result` | Fetches the consolidated result of a completed workflow. |
| `review_case_phase` | Submits human approval/rejection for a specific workflow phase. |
| `review_case_final` | Submits final human approval/rejection for the workflow outcome. |
| `reprocess_case_analysis` | Creates a new immutable execution for an existing workflow. Prior executions are preserved. |
| `get_case_workflow_artifacts` | Lists versioned workflow artifacts (PDF, JSON, markdown, logs). |
| `get_case_artifact_signed_url` | Generates a temporary signed download URL for one artifact. |

## Authentication

### stdio Mode

No authentication is required for the stdio transport. The server runs as a local subprocess with the same permissions as the calling process.

### Remote Mode (HTTP/SSE)

The remote server supports API key authentication. Keys are configured via `VALTER_MCP_SERVER_API_KEYS` (comma-separated list). Authentication failures are tracked by the `valter_mcp_auth_failures_total` Prometheus counter.

The runtime config also supports a maximum auth failure rate (`auth_max_failures_per_minute`, default 10) to defend against brute-force attempts.

## Rate Limiting

MCP requests are rate-limited independently from the REST API. The limit is configurable via `VALTER_MCP_RATE_LIMIT_PER_MINUTE` (default: 60 requests per minute per API key).

Rate limit state is stored in Redis using a sliding window. Blocked requests are counted by the `valter_mcp_rate_limit_blocks_total` Prometheus counter. The `/metrics` endpoint for the MCP remote server is IP-restricted via `VALTER_METRICS_IP_ALLOWLIST`.

## Setup Guide

### Claude Desktop

Add the following to your `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "valter": {
      "command": "python",
      "args": ["-m", "valter.mcp"],
      "env": {
        "VALTER_DATABASE_URL": "postgresql://...",
        "VALTER_QDRANT_URL": "http://localhost:6333",
        "VALTER_NEO4J_URI": "bolt://localhost:7687"
      }
    }
  }
}
```

### Claude Code

Add the following to your `.mcp.json` in the project root:

```json
{
  "mcpServers": {
    "valter": {
      "command": "python",
      "args": ["-m", "valter.mcp"]
    }
  }
}
```

### ChatGPT (Remote)

Configure the remote server URL in the ChatGPT custom GPT settings, pointing to the deployed MCP remote endpoint (e.g., `https://valter.legal:8001/mcp`). The HMAC authentication key must match the configured `VALTER_MCP_SERVER_API_KEYS`.

---
# docs/features/ingestion-workflow.md
---


# Ingestion Workflow

Valter's ingestion workflow transforms a raw case PDF into a structured legal analysis with phase identification, jurisprudence matching, and human-reviewed suggestions. The workflow is exposed through 17 endpoints under `/v1/ingest/*` and is also accessible via 8 MCP tools.

## Overview

The workflow operates as an async pipeline backed by an ARQ worker (Redis job queue) with a state machine that enforces valid transitions and generates auditable events. Each stage produces intermediate artifacts stored locally or in Cloudflare R2.

The pipeline follows this high-level flow:

```
PDF Upload -> PROJUDI Extraction -> Phase Analysis -> Jurisprudence Matching
    -> Suggestions -> Human Review -> Artifacts
```

Human review is required at two checkpoints: after individual phase analysis and as a final approval of the complete analysis. Rejected phases can trigger immutable reprocessing -- new versions are created while prior executions are preserved.

## Pipeline Stages

### 1. PDF Upload & Extraction

The workflow starts with a PDF upload via `POST /v1/ingest/workflow`. The PROJUDI pipeline (`core/projudi_pipeline.py`) handles first-instance process extraction:

- **Text extraction** using pdfplumber/pypdf, with optional OCR fallback via pytesseract
- **Movement detection** via regex patterns that identify dated process movements (e.g., `Ref. mov. 1.2`, `Data: 15/02/2024 | Movimentacao: Petição Inicial`)
- **Section segmentation** into labeled sections: fatos, fundamentos_processuais, merito, pedidos, decisao, anexo

Each extracted document is represented as a `ProcessDocument` with:

```python
# From core/projudi_pipeline.py
@dataclass(slots=True)
class ProcessDocument:
    document_id: str
    tipo_documento: str
    tipo_subdocumento: str | None
    movement_number: str | None
    movement_datetime: str | None
    page_start: int
    page_end: int
    text_excerpt: str
    key_sections: list[KeySection]
    confidence: float
    confidence_signals: dict[str, float]
    confidence_explanation: str
    needs_review: bool | None
```

Maximum upload size is controlled by `VALTER_MAX_UPLOAD_MB` (default: 100).

### 2. Classification & Confidence Scoring

After extraction, each document segment is automatically classified:

- **Document type** identification (peticao inicial, contestacao, decisao, sentenca, etc.)
- **Confidence scoring** per extracted field, with individual signal breakdowns
- **Sibling inheritance** shares metadata across related documents in the same case
- **Type rules** evaluated by `ProjudiTypeRuleEvaluator` for deterministic classification

Documents with confidence below the threshold are flagged with `needs_review: true`.

### 3. Phase Analysis

The `DeterministicPhaseInterpreter` (`core/phase_interpreter.py`) maps extracted documents to procedural phases using a rules-based state machine. Seven phases are tracked in order:

| Phase | Label |
|-------|-------|
| 1 | Postulacao Inicial (initial pleading) |
| 2 | Resposta do Reu (defendant's response) |
| 3 | Saneamento e Organizacao (procedural cleanup) |
| 4 | Instrucao Probatoria (evidence gathering) |
| 5 | Sentenca de Primeiro Grau (first-instance judgment) |
| 6 | Cumprimento/Execucao (enforcement) |
| 7 | Transicao Recursal (appeal transition) |

Five core modules handle phase analysis:

| Module | Purpose |
|--------|---------|
| `phase_interpreter.py` | Deterministic state machine for phase identification |
| `phase_matcher.py` | Matches documents to phases based on content signals |
| `phase_rules.py` | Rule definitions and evaluator (`PhaseRuleEvaluator`) |
| `phase_recommender.py` | Generates recommendations per phase |
| `phase_jurisprudence.py` | Finds relevant precedents per phase |

The rules version is configurable via `VALTER_PHASE_RULES_VERSION`.

### 4. Jurisprudence Matching

For each identified phase, the system finds relevant STJ precedents using the same hybrid search pipeline as `POST /v1/retrieve`. Matching is filtered by phase-relevant criteria.

| Parameter | Default | Purpose |
|-----------|---------|---------|
| `VALTER_PHASE_MIN_PRECEDENT_SCORE` | 55.0 | Minimum score threshold for a precedent match |
| `VALTER_PHASE_MAX_MATCHES_PER_PHASE` | 5 | Maximum precedents returned per phase |

### 5. Human Review

The workflow pauses at two review checkpoints:

**Phase review** (`POST /v1/ingest/workflow/{id}/review`): The operator approves or rejects the analysis for each individual phase. Each review records:

- `approved` (boolean)
- `reviewer` identity for audit trail
- `notes` for context

**Final review**: After all phases are reviewed, the operator approves or rejects the complete analysis.

Rejections do not delete prior work. If reprocessing is needed, `reprocess_case_analysis` creates a new immutable execution while preserving all prior versions and their review history.

### 6. Artifact Generation

Completed workflows produce versioned artifacts:

- **JSONL manifest** of analysis results with all phases, precedents, and review decisions
- **PDF reports** (when configured)
- **Logs** from each processing stage

Artifacts are stored using a dual-backend strategy:

| Backend | Status | Purpose |
|---------|--------|---------|
| Local filesystem | Active | Default storage |
| Cloudflare R2 | Ready, canary at 0% | Cloud storage with deterministic canary rollout |

Signed download URLs are generated via `POST /v1/ingest/workflow/{id}/artifacts/{aid}/signed-url`.

## State Machine

The `WorkflowStateMachine` (`core/workflow_state_machine.py`) enforces valid transitions through the workflow. Invalid transitions raise `InvalidWorkflowTransitionError`.

### States

| State | Description |
|-------|-------------|
| `queued_extraction` | Initial state, waiting for extraction job |
| `processing_extraction` | PDF extraction in progress |
| `queued_phase_analysis` | Extraction complete, waiting for phase analysis |
| `processing_phase_analysis` | Phase analysis in progress |
| `awaiting_phase_reviews` | Waiting for human review of individual phases |
| `awaiting_final_review` | Phase reviews complete, waiting for final approval |
| `needs_user_action` | Terminal state: requires operator intervention |
| `completed` | Terminal state: workflow finished successfully |
| `failed` | Terminal state: workflow encountered an unrecoverable error |

### Transitions

```
queued_extraction --[enqueue_extraction_job]--> processing_extraction
processing_extraction --[extraction_completed]--> queued_phase_analysis
queued_phase_analysis --[enqueue_phase_analysis_job]--> processing_phase_analysis
processing_phase_analysis --[phase_analysis_completed]--> awaiting_phase_reviews
                                                      --> awaiting_final_review
                                                      --> completed
```

:::note
Terminal failure states (`failed`, `needs_user_action`) are not reachable through the state machine's `next_state()` method. They are applied exclusively via `FullCaseWorkflowOrchestrator.mark_failed()`, which handles persistence and event emission for error scenarios. This is an intentional architectural separation: the machine covers the happy path, while `mark_failed()` covers all error/termination paths.
:::

Each transition generates an audit event persisted in JSONL format.

## Configuration

| Variable | Default | Purpose |
|----------|---------|---------|
| `VALTER_INGEST_JOB_TIMEOUT_SECONDS` | 1800 | Timeout for individual ingest jobs |
| `VALTER_INGEST_WORKER_CONCURRENCY` | 2 | ARQ worker concurrency |
| `VALTER_WORKFLOW_TIMEOUT_SECONDS` | 2400 | Overall workflow timeout |
| `VALTER_WORKFLOW_MAX_RETRIES` | 3 | Maximum retry attempts |
| `VALTER_WORKFLOW_STRICT_INFRA_REQUIRED` | true | Fail when required infra dependencies are unavailable |
| `VALTER_PHASE_RULES_VERSION` | `phase-rules-v1` | Active ruleset version |
| `VALTER_MAX_UPLOAD_MB` | 100 | Maximum PDF upload size |

---
# docs/features/verification-enrichment.md
---


# Verification & Enrichment

Valter provides two complementary features for ensuring accuracy of legal content. Verification catches hallucinated legal references before they reach users. Enrichment adds structured legal analysis (IRAC) and knowledge graph context to decisions.

## Anti-Hallucination Verification

**Endpoint**: `POST /v1/verify`

LLMs frequently hallucinate legal citations -- inventing sumula numbers, misspelling minister names, or fabricating process numbers. The `LegalVerifier` (`core/verifier.py`) validates references found in text against known datasets and computes a hallucination risk score.

### What Gets Verified

The verifier checks four categories of legal references, each toggleable via request parameters:

#### Sumulas

Validates sumula numbers against local STJ and STF reference data using `SumulaValidator`. The validation confirms:

- The sumula number exists
- The correct court is attributed (STJ vs STF)
- Current status (vigente or not)
- The associated legal area

#### Ministers

Validates minister names against a known list using `MinistroValidator`. Returns:

- `valid`: whether the name matches a known minister
- `confidence`: `exact`, `partial`, or `none`
- `is_aposentado`: whether the minister is retired
- `suggestion`: corrected name when a partial match is found

#### Process Numbers

Validates the CNJ process number format using a regex pattern:

```python
# From core/verifier.py
# CNJ format: NNNNNNN-NN.NNNN.N.NN.NNNN
PROCESSO_REGEX = re.compile(r"\b(\d{7}-\d{2}\.\d{4}\.\d\.\d{2}\.\d{4})\b")
```

This validates format only -- it does not confirm the process exists in external systems.

#### Legislation

Extracts and classifies legislation mentions using regex patterns. Recognizes both explicit references (e.g., "Lei 8.078/1990") and common aliases:

| Alias | Resolves To |
|-------|-------------|
| CDC | Lei 8.078/1990 |
| CC | Lei 10.406/2002 |
| CPC | Lei 13.105/2015 |
| CLT | Decreto-Lei 5.452/1943 |
| CP | Decreto-Lei 2.848/1940 |
| CTN | Lei 5.172/1966 |
| CF | Constituicao Federal 1988 |
| ECA | Lei 8.069/1990 |

Article references (e.g., "Art. 186") are also extracted and linked to their parent law.

:::note
Legislation references are extracted and classified but not verified for legal existence or current validity.
:::

### Hallucination Risk Metrics

The verifier computes an overall `HallucinationMetrics` object:

```python
# From core/verifier.py
@dataclass
class HallucinationMetrics:
    risk_level: str      # "low", "medium", "high"
    risk_score: float    # 0-100
    total_citations: int
    valid_count: int
    invalid_count: int
    unverified_count: int
    details: dict
```

The risk score aggregates validation results: a text with many invalid references produces a higher score, signaling that the content may contain hallucinated citations.

### Reference Data

Verification relies on golden datasets stored in the `data/reference/` directory. The project also has 810,225 STJ metadata records for broader process validation.

## IRAC Analysis & Enrichment

**Endpoint**: `POST /v1/enrich`

The `DocumentEnricher` (`core/enricher.py`) performs two operations on a legal document: heuristic IRAC classification and knowledge graph context loading.

### IRAC Components

IRAC is a standard framework for analyzing legal decisions:

| Component | What It Identifies | Example Patterns |
|-----------|-------------------|-----------------|
| **Issue** | The legal question being decided | "questao", "controversia", "discute-se", "cinge-se" |
| **Rule** | The legal norm or principle applied | "artigo", "lei", "sumula", "nos termos de" |
| **Application** | How the rule was applied to the facts | "no caso", "in casu", "verifica-se", "configurado" |
| **Conclusion** | The court's decision | "portanto", "ante o exposto", "da provimento", "nega" |

The classification is heuristic and regex-based -- it does not depend on an LLM. Each IRAC section is identified by scanning the document text (ementa, tese, razoes_decidir) against compiled regex patterns:

```python
# From core/enricher.py
IRAC_PATTERNS = {
    IRACSection.ISSUE: [
        r"\b(?:questao|problema|controversia|debate|discussao|tese|cerne)\b",
        r"\b(?:discute-se|indaga-se|pergunta-se|cinge-se)\b",
    ],
    IRACSection.RULE: [
        r"\b(?:art\.?|artigo|lei|sumula|codigo|dispositivo|norma)\b",
        r"\b(?:preve|estabelece|dispoe|prescreve|determina)\b",
    ],
    # ...
}
```

### Knowledge Graph Context

After IRAC classification, the enricher loads graph context by running 5 parallel queries against Neo4j:

| Entity Type | Method | What It Returns |
|-------------|--------|-----------------|
| Criterios | `get_criterios()` | Legal criteria connected to the decision |
| Dispositivos | `get_dispositivos()` | Legal statutes cited |
| Precedentes | `get_precedentes()` | Precedent decisions cited |
| Legislacao | `get_legislacao()` | Legislation edges with relationship metadata |
| Related Decisions | `get_related_decisions()` | Decisions connected via shared criteria |

The enrichment result includes a `kg_available` flag indicating whether the Neo4j graph contained data for this decision.

```python
# From core/enricher.py
@dataclass
class EnrichmentResult:
    document_id: str
    irac: IRACAnalysis | None = None
    features: DocumentFeatures | None = None
    criterios: list[Criterio] = field(default_factory=list)
    dispositivos: list[DispositivoLegal] = field(default_factory=list)
    precedentes: list[Precedente] = field(default_factory=list)
    legislacao: list[DecisaoLegislacaoEdge] = field(default_factory=list)
    related_decisions: list[RelatedDecision] = field(default_factory=list)
    kg_available: bool = False
```

If a `FeaturesStore` is configured, the enricher also loads the 21 AI-extracted features for the document.

## Factual Extraction

**Endpoint**: `POST /v1/factual/extract`

The `FactualExtractor` (`core/factual_extractor.py`) uses a Groq LLM to extract two independent structured representations from legal text:

### Factual Digest

A set of 10-15 factual bullets plus a condensed narrative (2-3 sentences), optimized for semantic search:

- Each bullet cites the source excerpt from the original text when identifiable
- Uncertain or contested facts are marked with `uncertainty: true`
- The `digest_text` is designed to be dense and comparable across cases

### Legal Thesis

The core legal argument extracted from the document:

- Central thesis description (1-3 sentences)
- Legal basis: statutes cited (e.g., "CDC art. 14", "CC/2002 art. 186")
- Precedents cited in the text (e.g., "REsp 1.234.567/SP", "Sumula 297/STJ")

These dense representations produce more discriminative vectors than embedding entire decisions, which suffer from topic averaging across long documents. The input is capped at 4,000 characters to align with LLM context limits.

:::note
Factual extraction requires `VALTER_GROQ_API_KEY` and `VALTER_GROQ_ENABLED=true`.
:::

## Temporal Validity

Integrated into the verifier pipeline, temporal validity checks whether referenced legal norms are still in effect. This catches references to revoked or superseded legislation, which is a common source of inaccuracy in AI-generated legal content.

---
# docs/features/reasoning-chain.md
---


# Legal Reasoning Chain

:::caution
This is a planned feature for v1.2. The design is finalized and all prerequisite capabilities are implemented, but the orchestrator itself has not been built yet. Prerequisites: v1.1 (circuit breaker, connection pools) must be completed first, since the reasoning chain executes multiple concurrent graph queries.
:::

The Legal Reasoning Chain is the feature that transforms Valter from a search backend into a legal reasoning engine. Instead of returning a list of similar cases, it composes a verified legal argument with provenance -- every claim traceable to a specific node in the knowledge graph.

## Vision

No competitor in the Brazilian legal tech space does argument composition from a knowledge graph. Existing tools either return keyword-matched case lists or generate unverified text. The reasoning chain closes that gap by orchestrating 7 existing Valter capabilities into a single response that an attorney can cite with confidence.

The orchestrator will live at `core/reasoning_chain.py` and requires no new graph queries -- it chains existing endpoints that are already production-tested.

## Input / Output

### Input

```json
{
  "fatos": "Passageiro teve voo cancelado sem aviso previo, ficou 18h no aeroporto sem assistencia",
  "pergunta": "Cabe indenizacao por dano moral por cancelamento de voo?",
  "ministro": "NANCY ANDRIGHI",
  "turma": "3a Turma"
}
```

| Field | Required | Description |
|-------|----------|-------------|
| `fatos` | Yes | Case facts in natural language |
| `pergunta` | Yes | Legal question to answer |
| `ministro` | No | Filter argument composition for a specific minister |
| `turma` | No | Filter by court division |

### Output

```json
{
  "argument_steps": [
    {
      "fact": "cancelamento sem aviso previo",
      "criterion": "dano moral em transporte aereo",
      "success_rate": 0.87,
      "dispositivo": "CDC art. 14",
      "supporting_decisions": ["REsp 1.584.465/SP", "REsp 1.734.946/RJ"]
    }
  ],
  "counter_arguments": [
    {
      "criterion": "excludente de responsabilidade por caso fortuito",
      "dissenting_ministers": ["MINISTRO X"],
      "divergence_score": 0.35
    }
  ],
  "temporal_strength": [
    {
      "criterion": "dano moral em transporte aereo",
      "trend": "growing",
      "recent_decisions_count": 42
    }
  ],
  "anchor_decision": {
    "processo": "REsp 1.584.465/SP",
    "ministro": "NANCY ANDRIGHI",
    "ementa": "...",
    "kg_score": 0.92,
    "citation_count": 15,
    "verified": true
  },
  "overall_strength": 0.83,
  "provenance": [
    {
      "claim_index": 0,
      "graph_node_id": "criterio_dano_moral_transporte",
      "graph_node_type": "Criterio",
      "source_decision": "decisao_REsp_1584465"
    }
  ]
}
```

The `provenance` array is the critical differentiator: every claim in the argument can be traced back to a specific node in the knowledge graph, making the output auditable.

## Orchestrated Capabilities

The reasoning chain does not introduce new queries. It orchestrates existing, production-tested endpoints:

| Reasoning Component | Existing Endpoint | What It Provides |
|---------------------|-------------------|-----------------|
| Fact to Criterion with success rate | `get_optimal_argument` | Maps facts to criteria and computes success rates |
| Criterion to Legal statute | `get_optimal_argument` | Chain includes dispositivos (statutes) |
| Counter-argument detection | `get_divergencias` + `get_ministro_profile` | Finds active disagreements on the criteria used |
| Minister delta vs category | `get_optimal_argument_by_ministro` | Shows how a specific minister diverges from average |
| Temporal trend | `get_temporal_evolution` | Whether the criterion is gaining or losing traction |
| Verified anchor decision | `search_jurisprudence` + `verify_legal_claims` | The strongest decision to cite, verified against reference data |
| Structural similarity | `find_similar_cases` | Cases with similar graph topology |

## Planned Endpoints

| Verb | Route | Description |
|------|-------|-------------|
| POST | `/v1/reasoning-chain` | REST API endpoint for argument composition |
| MCP | `compose_legal_argument` | MCP tool for Claude Desktop/Code and ChatGPT |

## Completion Criteria

The v1.2 milestone defines clear acceptance criteria:

- `POST /v1/reasoning-chain` returns an argument with at least 3 verified steps
- Every claim in the output has a `provenance` entry linking to a graph node
- MCP tool `compose_legal_argument` is functional in Claude Desktop
- Latency p95 for the complete chain is under 5 seconds
- A spike of 50 TRF decisions is executed and documented to assess multi-tribunal complexity

## Open Decision: Sync vs Async

The execution model for the reasoning chain is not yet decided:

| Option | Pros | Cons |
|--------|------|------|
| **Synchronous** | Simple implementation, good for target latency < 5s | Blocks the request thread; fails if any sub-query is slow |
| **Async with polling** | Tolerates > 10s execution; client polls for result | Adds complexity (job queue, status endpoint, client-side polling) |
| **Streaming via SSE** | Real-time partial results; best UX | Most complex; requires SSE support in all consumers |

The decision depends on measured latency of the orchestrated queries once circuit breakers and connection pools (v1.1) are in place.

---
# docs/features/observability.md
---


# Observability

Valter ships with structured JSON logging, 30+ Prometheus metrics, and OpenTelemetry tracing. The instrumentation is implemented across three modules in `src/valter/observability/`: `logging.py`, `metrics.py`, and `tracing.py`.

:::caution
The metrics are fully instrumented in code, but production wiring (dashboards, alert dispatch to Slack) is a v1.0 priority item. Currently, metrics are exposed via `/metrics` and logs go to stdout for Railway to capture, but no alerting system is connected.
:::

## Logging (structlog)

Valter uses structlog for structured JSON logging. Every log entry is machine-parseable and includes contextual fields for correlation.

### Configuration

```python
# From observability/logging.py
structlog.configure(
    processors=[
        structlog.contextvars.merge_contextvars,
        structlog.processors.add_log_level,
        structlog.processors.StackInfoRenderer(),
        structlog.dev.set_exc_info,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.JSONRenderer(),
    ],
    logger_factory=structlog.PrintLoggerFactory(file=sys.stderr),
)
```

Key design decisions:

- **JSON format** to stdout/stderr -- Railway captures logs automatically
- **Logs go to stderr** so they do not interfere with MCP stdio transport (which uses stdout for JSON-RPC)
- **Context variables** via `structlog.contextvars.merge_contextvars` inject `trace_id` and other request-scoped data into every log entry
- **Log level** is configurable via `VALTER_LOG_LEVEL` (default: INFO)

### Trace ID Correlation

Every incoming request generates a `trace_id` that is injected into context variables and propagated through all log entries for that request. This allows correlating logs across the full request lifecycle -- from API handler through store queries to response serialization.

## Metrics (Prometheus)

Valter defines 30+ Prometheus metrics using the `prometheus_client` library. Metrics are exposed via `GET /metrics`, with access restricted by `VALTER_METRICS_IP_ALLOWLIST`.

### Request Metrics

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `valter_request_duration_seconds` | Histogram | endpoint, method, status | Request duration with buckets from 10ms to 10s |
| `valter_requests_total` | Counter | endpoint, method, status | Total request count |

### MCP Metrics

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `valter_mcp_rpc_requests_total` | Counter | rpc_method, tool, status_class | Total MCP JSON-RPC requests |
| `valter_mcp_rpc_duration_seconds` | Histogram | rpc_method, tool, status_class | MCP request duration (buckets to 60s) |
| `valter_mcp_tool_calls_total` | Counter | tool, outcome | Tool call count by outcome |
| `valter_mcp_tool_call_duration_seconds` | Histogram | tool, outcome | Tool call duration |
| `valter_mcp_auth_failures_total` | Counter | reason | Authentication failures |
| `valter_mcp_rate_limit_blocks_total` | Counter | -- | Rate-limited MCP requests |

### Rate Limiting Metrics

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `valter_api_rate_limit_blocks_total` | Counter | reason | API requests blocked by rate limit |
| `valter_api_rate_limit_failsafe_blocks_total` | Counter | -- | API requests blocked when rate limit backend is unavailable |
| `valter_mcp_rate_limit_failsafe_blocks_total` | Counter | -- | MCP requests blocked when rate limit backend is unavailable |
| `valter_rate_limit_redis_errors_total` | Counter | surface | Redis errors in rate limiting middleware |

### Cache Metrics

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `valter_cache_hits_total` | Counter | store | Cache hit count |
| `valter_cache_misses_total` | Counter | store | Cache miss count |

### Store Health

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `valter_store_health` | Gauge | store | Per-store health (1=up, 0=down) |
| `valter_queue_depth` | Gauge | queue | Pending jobs in the ARQ queue |

### Ingestion Metrics

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `valter_ingest_stage_duration_seconds` | Histogram | stage, status | Duration per ingestion stage |
| `valter_artifact_put_latency_ms` | Histogram | backend | Artifact write latency |
| `valter_artifact_get_latency_ms` | Histogram | backend | Artifact read latency |
| `valter_artifact_put_fail_total` | Counter | backend, content_type | Failed artifact writes |
| `valter_presign_issued_total` | Counter | backend | Signed URL generation count |

### Knowledge Graph Metrics

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `valter_kg_boost_errors_total` | Counter | source | KG boost errors |
| `valter_kg_enrichment_total` | Counter | outcome | KG enrichment attempts |
| `valter_kg_boost_candidates_total` | Counter | strategy | Search results eligible for KG boost |
| `valter_kg_boost_enriched_total` | Counter | strategy | Search results actually enriched by KG boost |
| `valter_kg_boost_score` | Histogram | -- | Distribution of raw KG boost scores |

### Operational Failures

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `valter_operation_failures_total` | Counter | surface, stage, error_class | Operational failures with low-cardinality taxonomy |

:::note
The `error_class` label uses a curated taxonomy (not raw exception class names) to keep Prometheus cardinality bounded. The raw exception type is available in structured log events.
:::

## Tracing (OpenTelemetry)

OpenTelemetry tracing is configured in `observability/tracing.py` with FastAPI auto-instrumentation.

### Current Setup

```python
# From observability/tracing.py
def setup_tracing(service_name: str = "valter") -> None:
    resource = Resource.create({"service.name": service_name})
    provider = TracerProvider(resource=resource)
    provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))
    trace.set_tracer_provider(provider)
```

The current configuration exports spans to the console via `ConsoleSpanExporter`. This is useful for development but not for production monitoring.

### FastAPI Instrumentation

```python
# From observability/tracing.py
def instrument_fastapi_app(app: FastAPI) -> None:
    from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
    FastAPIInstrumentor.instrument_app(app)
```

This auto-instruments all FastAPI endpoints with trace context propagation. The instrumentation is idempotent -- calling it multiple times on the same app has no effect.

:::tip
The production plan is to replace `ConsoleSpanExporter` with a proper backend (Jaeger, Grafana Tempo) and connect trace IDs to the structured log entries for full request lifecycle visibility.
:::

## Health Check

**Endpoint**: `GET /v1/health`

The health endpoint checks connectivity to all backend stores with a 5-second timeout per store:

| Store | What It Checks |
|-------|---------------|
| `qdrant` | Vector store connectivity |
| `neo4j` | Graph database connectivity |
| `postgres` | Document store connectivity |
| `redis` | Cache store connectivity |
| `artifact_storage` | Artifact backend (local or R2) |

Each store returns `up` or `down` with measured latency in milliseconds. The overall status is `healthy` when all stores are up, or `degraded` when any store is down. The response also includes the Valter version number and uptime.

Store health is tracked by the `valter_store_health` Prometheus gauge, enabling monitoring systems to detect degradation over time.

## Alert Rules

Six alert rules are defined in the project documentation, targeting:

- Latency p95 exceeding threshold
- Error rate exceeding threshold
- Store health degradation
- Cache hit rate drops
- Queue depth increases
- KG boost error spikes

:::caution
The alert rules exist as definitions, but no alert dispatcher is currently connected. The v1.0 plan is to wire Railway metrics to a Slack webhook for critical alerts.
:::

---
# docs/configuration/environment.md
---


# Environment Variables

All Valter configuration is driven by environment variables prefixed with `VALTER_`. Variables are loaded in this order of precedence: shell environment, `.env` file, built-in defaults. The `Settings` class in `src/valter/config.py` uses `pydantic-settings` with `env_prefix="VALTER_"`.

## Core

| Variable | Purpose | Default | Required? |
|---|---|---|---|
| `VALTER_ENV` | Environment name. Set to `production` or `prod` to activate production guardrails. | `development` | No |
| `VALTER_RUNTIME` | Execution mode that determines which process starts. See [Runtime Modes](./settings.md#runtime-modes). | `api` | No |

:::caution
Setting `VALTER_ENV=production` activates strict validation: authentication must be enabled, CORS wildcard is blocked, Neo4j must be remote, and database credentials cannot be defaults. See [Production Guardrails](#production-guardrails) below.
:::

## Databases

| Variable | Purpose | Default | Required? |
|---|---|---|---|
| `VALTER_DATABASE_URL` | PostgreSQL connection string using the `asyncpg` driver. | `postgresql+asyncpg://valter:valter_dev@localhost:5432/valter` | No |
| `VALTER_QDRANT_URL` | Qdrant vector database URL. | `http://localhost:6333` | No |
| `VALTER_QDRANT_COLLECTION` | Name of the Qdrant collection for legal document chunks. | `legal_chunks_v1` | No |
| `VALTER_NEO4J_URI` | Neo4j connection URI. Use `bolt://` for local, `neo4j+s://` for Aura. | `bolt://localhost:7687` | No |
| `VALTER_NEO4J_USERNAME` | Neo4j authentication username. | `neo4j` | No |
| `VALTER_NEO4J_PASSWORD` | Neo4j authentication password. | `neo4j_dev` | No |
| `VALTER_REDIS_URL` | Redis connection URL with database number. | `redis://localhost:6379/0` | No |
| `VALTER_ARQ_REDIS_DB` | Redis database number for the ARQ background worker. Kept separate from the main Redis DB to avoid key collisions. | `1` | No |

:::tip
In production, `VALTER_DATABASE_URL` must not contain `valter_dev`, `VALTER_NEO4J_URI` must point to a remote host (not localhost), and `VALTER_NEO4J_PASSWORD` must not be a weak default like `neo4j_dev` or `password`. The application will refuse to start otherwise.
:::

## Embeddings & LLM

| Variable | Purpose | Default | Required? |
|---|---|---|---|
| `VALTER_EMBEDDING_MODEL` | HuggingFace model identifier for semantic embeddings. Downloaded via `make download-model`. | `rufimelo/Legal-BERTimbau-sts-base` | No |
| `VALTER_EMBEDDING_DIMENSION` | Vector dimension produced by the embedding model. Must match the model output. | `768` | No |
| `VALTER_EMBEDDING_SERVICE_URL` | URL of a remote embedding service (hosted on Railway). When set, the local model is bypassed. | None | No |
| `VALTER_RERANKER_SERVICE_URL` | URL of a remote reranking service (hosted on Railway). When set, the local cross-encoder is bypassed. | None | No |
| `VALTER_GROQ_API_KEY` | API key for Groq LLM. Enables factual extraction and query expansion features. | None | No |
| `VALTER_GROQ_MODEL` | Model identifier used with the Groq API. | `qwen/qwen3-32b` | No |
| `VALTER_GROQ_ENABLED` | Feature flag to enable Groq-powered features. Requires `VALTER_GROQ_API_KEY` to be set. | `false` | No |

:::note
Without `VALTER_GROQ_API_KEY` and `VALTER_GROQ_ENABLED=true`, the system works normally but factual extraction endpoints and query expansion in hybrid search are disabled. Setting `VALTER_EMBEDDING_SERVICE_URL` avoids the need to download the ~500 MB embedding model locally.
:::

## API Server

| Variable | Purpose | Default | Required? |
|---|---|---|---|
| `VALTER_API_HOST` | Host address the API server binds to. | `0.0.0.0` | No |
| `VALTER_API_PORT` | Port the API server listens on. | `8000` | No |
| `VALTER_AUTH_ENABLED` | Enable API key authentication on REST endpoints. | `false` | **Yes (prod)** |
| `VALTER_RATE_LIMIT_READ` | Maximum read requests per API key per minute. | `100` | No |
| `VALTER_RATE_LIMIT_WRITE` | Maximum write requests per API key per minute. | `10` | No |
| `VALTER_CORS_ORIGINS` | JSON array of allowed CORS origins. | `["*"]` | No |
| `VALTER_LOG_LEVEL` | Application log level (`DEBUG`, `INFO`, `WARNING`, `ERROR`). | `INFO` | No |
| `VALTER_METRICS_IP_ALLOWLIST` | Comma-separated list of IP addresses allowed to access `/metrics`. | Empty | **Yes (prod)** |

:::danger
In production, `VALTER_AUTH_ENABLED` must be `true`, `VALTER_CORS_ORIGINS` must not contain `"*"`, and `VALTER_METRICS_IP_ALLOWLIST` must be set. The application raises a `ValueError` at startup if any of these constraints are violated.
:::

## Upload & Ingest

| Variable | Purpose | Default | Required? |
|---|---|---|---|
| `VALTER_UPLOAD_STORAGE_PATH` | Local filesystem path where uploaded files are stored before processing. | `data/datasets/uploads/raw` | No |
| `VALTER_MAX_UPLOAD_MB` | Maximum upload file size in megabytes. Converted to bytes internally via the `max_upload_bytes` property. | `100` | No |
| `VALTER_INGEST_JOB_TIMEOUT_SECONDS` | Maximum duration (seconds) an ARQ ingest job can run before being killed. | `1800` | No |
| `VALTER_INGEST_WORKER_CONCURRENCY` | Number of concurrent jobs the ARQ worker processes. | `2` | No |

## Cloudflare R2

| Variable | Purpose | Default | Required? |
|---|---|---|---|
| `VALTER_R2_ACCOUNT_ID` | Cloudflare account ID. Used to construct the endpoint URL if `VALTER_R2_ENDPOINT_URL` is not set. | None | No |
| `VALTER_R2_ACCESS_KEY_ID` | S3-compatible access key for R2. | None | No |
| `VALTER_R2_SECRET_ACCESS_KEY` | S3-compatible secret key for R2. | None | No |
| `VALTER_R2_BUCKET_NAME` | R2 bucket name for storing workflow artifacts. | `valter-artifacts` | No |
| `VALTER_R2_ENDPOINT_URL` | Override the auto-constructed R2 endpoint URL. | None | No |
| `VALTER_R2_PRESIGN_TTL_SECONDS` | Time-to-live (seconds) for pre-signed download URLs. | `600` | No |
| `VALTER_R2_CANARY_PERCENT` | Percentage (0-100) of artifact uploads routed to R2 instead of local storage. Use for gradual rollout. | `0` | No |

:::note
All three credentials (`VALTER_R2_ACCOUNT_ID`, `VALTER_R2_ACCESS_KEY_ID`, `VALTER_R2_SECRET_ACCESS_KEY`) must be set together for R2 to function. When `VALTER_R2_CANARY_PERCENT` is `0` (default), all artifacts are stored locally. Increase gradually during migration to R2.
:::

## Workflow Engine

| Variable | Purpose | Default | Required? |
|---|---|---|---|
| `VALTER_WORKFLOW_TIMEOUT_SECONDS` | Global timeout for an entire workflow execution. | `2400` | No |
| `VALTER_WORKFLOW_MAX_RETRIES` | Maximum number of retry attempts per workflow step before marking it as failed. | `3` | No |
| `VALTER_WORKFLOW_POLL_RECOMMENDED_SECONDS` | Recommended interval (seconds) for clients polling workflow status. Returned in API responses. | `3` | No |
| `VALTER_WORKFLOW_STRICT_INFRA_REQUIRED` | When `true`, workflows fail immediately if required infrastructure (Qdrant, Redis) is unavailable. When `false`, they degrade gracefully. | `true` | No |

## MCP Server

| Variable | Purpose | Default | Required? |
|---|---|---|---|
| `VALTER_MCP_SERVER_TRANSPORT` | Transport protocol for the MCP server (`streamable-http` or `stdio`). | Derived from `VALTER_RUNTIME` | No |
| `VALTER_MCP_SERVER_HOST` | Host address the MCP remote server binds to. | `0.0.0.0` | No |
| `VALTER_MCP_SERVER_PORT` | Port the MCP remote server listens on. Falls back to `$PORT` if set (for Railway compatibility). | `8001` | No |
| `VALTER_MCP_SERVER_PATH` | URL path for the MCP HTTP endpoint. | `/mcp` | No |
| `VALTER_MCP_SERVER_AUTH_MODE` | Authentication mode for incoming MCP requests. | `api_key` | No |
| `VALTER_MCP_SERVER_API_KEYS` | Comma-separated list of valid API keys for MCP authentication. | None | No |
| `VALTER_MCP_API_BASE_URL` | Base URL of the REST API that the MCP server delegates to internally. | `http://localhost:8000` | No |
| `VALTER_MCP_API_KEY` | API key the MCP server uses when calling the REST API. | None | No |
| `VALTER_MCP_RATE_LIMIT_PER_MINUTE` | Maximum MCP requests per minute per client. | `60` | No |

:::tip
The MCP server acts as a bridge: it receives tool calls from LLM clients (ChatGPT, Claude) and translates them into REST API calls against `VALTER_MCP_API_BASE_URL`. If the REST API has authentication enabled, set `VALTER_MCP_API_KEY` accordingly.
:::

## Phase Analysis

| Variable | Purpose | Default | Required? |
|---|---|---|---|
| `VALTER_PHASE_RULES_VERSION` | Version of the phase classification rules used for procedural analysis. | `phase-rules-v1.1` | No |
| `VALTER_PHASE_MIN_PRECEDENT_SCORE` | Minimum similarity score (0-100) for a precedent to be considered relevant in phase matching. | `55.0` | No |
| `VALTER_PHASE_MAX_MATCHES_PER_PHASE` | Maximum number of precedent matches returned per procedural phase. | `5` | No |

## Knowledge Graph

| Variable | Purpose | Default | Required? |
|---|---|---|---|
| `VALTER_KG_BOOST_BATCH_ENABLED` | Enable batch knowledge graph boosting in the hybrid retriever. Enriches search results with graph context in parallel. | `true` | No |
| `VALTER_KG_BOOST_MAX_CONCURRENCY` | Maximum number of concurrent Neo4j queries during KG boost. Tune based on Neo4j capacity. | `12` | No |
| `VALTER_QUERY_EXPANSION_MAX_VARIANTS` | Maximum number of query variants generated during search expansion (requires Groq). | `3` | No |

## Production Guardrails

The `Settings` class enforces the following constraints when `VALTER_ENV` is set to `production` or `prod`. These validations run at application startup and cause an immediate failure with a descriptive `ValueError` if violated.

**API runtime guardrails** (skipped for MCP-only runtimes which have their own auth):

- `VALTER_AUTH_ENABLED` must be `true`
- `VALTER_CORS_ORIGINS` must not contain `"*"`
- `VALTER_METRICS_IP_ALLOWLIST` must be non-empty

**Infrastructure guardrails** (always enforced in production):

- `VALTER_NEO4J_URI` must point to a remote host (not `localhost` or `127.0.0.1`)
- `VALTER_NEO4J_PASSWORD` must not be a weak default (`neo4j_dev`, `password`, `changeme`, etc.)
- `VALTER_DATABASE_URL` must not contain `valter_dev`
- `VALTER_REDIS_URL` must not point to `localhost` or `127.0.0.1`

---
# docs/configuration/settings.md
---


# Settings & Configuration Files

Valter centralizes all configuration in a single Pydantic Settings class that loads values from environment variables and `.env` files. This page explains the configuration architecture, runtime modes, feature flags, and the project's configuration files.

## Configuration System

The `Settings` class in `src/valter/config.py` extends `pydantic_settings.BaseSettings` and serves as the single source of truth for all application configuration. It uses the `VALTER_` prefix for environment variable resolution.

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    ENV: str = "development"
    DATABASE_URL: str = "postgresql+asyncpg://valter:valter_dev@localhost:5432/valter"
    # ... 50+ fields with sensible defaults

    model_config = {
        "env_prefix": "VALTER_",
        "env_file": ".env",
    }
```

### Loading Order

Values are resolved in this order (first match wins):

1. **Shell environment variables** -- highest priority, always override
2. **`.env` file** -- convenient for local development
3. **Field defaults** -- defined in the `Settings` class itself

A singleton instance is created at module level:

```python
settings = Settings()
```

This instance is imported wherever configuration is needed and injected via FastAPI's `Depends()` where appropriate.

### Production Validation

The `Settings` class includes a `@model_validator` that runs after all fields are loaded. When `VALTER_ENV` is `production` or `prod`, it validates security-critical constraints and raises a `ValueError` at startup if any are violated. See [Environment Variables > Production Guardrails](./environment.md#production-guardrails) for the full list.

### Computed Properties

Several configuration values are derived from raw settings via `@property` methods:

| Property | Computation |
|---|---|
| `max_upload_bytes` | `MAX_UPLOAD_MB * 1024 * 1024` |
| `is_production` | `ENV.strip().lower() in {"production", "prod"}` |
| `r2_endpoint_url` | Uses `R2_ENDPOINT_URL` if set, otherwise constructs from `R2_ACCOUNT_ID` |
| `r2_canary_percent` | Clamps `R2_CANARY_PERCENT` to 0-100 range |
| `arq_redis_url` | Replaces the DB number in `REDIS_URL` with `ARQ_REDIS_DB` |
| `metrics_ip_allowlist` | Parses comma-separated string into a `list[str]` |

## Runtime Modes

The `VALTER_RUNTIME` variable determines which process starts. The routing logic lives in `scripts/start-command.sh`, which is the container entrypoint.

| Mode | Value | Process | Port |
|---|---|---|---|
| REST API | `api` | `uvicorn valter.main:app` | 8000 |
| Background worker | `worker` | `python -m valter.workers` | N/A |
| MCP remote server | `mcp-remote` | `python -m valter.mcp.remote_server` | 8001 |
| MCP stdio server | `mcp-stdio` | `python -m valter.mcp.remote_server` (stdio transport) | N/A |

Each mode initializes different components. The API mode starts FastAPI with middleware, routes, and database connections. The worker mode starts the ARQ job processor for background ingest tasks. The MCP modes start a Model Context Protocol server that bridges LLM tool calls to the REST API.

:::note
The `mcp-remote` and `mcp-stdio` modes both use the same Python module (`valter.mcp.remote_server`), differing only in transport. The start script sets `VALTER_MCP_SERVER_TRANSPORT` accordingly.
:::

For local development, you typically start each mode separately:

```bash
make dev            # Starts API mode on port 8000
make worker-ingest  # Starts ARQ worker
make mcp-remote     # Starts MCP HTTP server on port 8001
```

In Railway deployments, each mode runs as a separate service with `VALTER_RUNTIME` set in the service environment.

## Feature Flags

Several boolean or numeric variables act as feature flags, enabling functionality that is off by default:

| Flag | Default | What it enables |
|---|---|---|
| `VALTER_GROQ_ENABLED` | `false` | Groq LLM features: factual extraction (`POST /v1/factual/extract`), query expansion in hybrid search. Requires `VALTER_GROQ_API_KEY`. |
| `VALTER_AUTH_ENABLED` | `false` | API key authentication on REST endpoints. Must be `true` in production. |
| `VALTER_KG_BOOST_BATCH_ENABLED` | `true` | Batch knowledge graph boosting in the hybrid retriever. Enriches vector search results with graph context from Neo4j. |
| `VALTER_R2_CANARY_PERCENT` | `0` | Percentage of artifact uploads routed to Cloudflare R2. At `0`, all artifacts are stored locally. Increase for gradual R2 migration. |
| `VALTER_WORKFLOW_STRICT_INFRA_REQUIRED` | `true` | When `true`, workflows fail immediately if required infrastructure is unavailable. When `false`, they degrade gracefully. |

## Configuration Files

Beyond environment variables, Valter uses several configuration files:

| File | Purpose |
|---|---|
| `pyproject.toml` | Project metadata, dependencies, and tool configuration (ruff, pytest, mypy, hatch) |
| `docker-compose.yml` | Local development database stack (PostgreSQL 16, Qdrant, Redis 7) |
| `Dockerfile` | Production container image |
| `railway.json` | Railway.app deployment configuration |
| `Makefile` | Canonical command interface for all development and operational tasks |
| `.env` | Local environment variable overrides (gitignored) |
| `migrations/alembic.ini` | Alembic database migration configuration |

## .env File

For local development, create a `.env` file at the project root. Most variables have sensible defaults that work with the `docker-compose.yml` stack, so only a minimal `.env` is needed:

```bash
# .env (local development)
# These defaults work with docker-compose.yml — only override what you need.

# Optional: enable Groq LLM features
# VALTER_GROQ_API_KEY=gsk_your_key_here
# VALTER_GROQ_ENABLED=true

# Optional: Neo4j Aura for graph features
# VALTER_NEO4J_URI=neo4j+s://xxxxx.databases.neo4j.io
# VALTER_NEO4J_USERNAME=neo4j
# VALTER_NEO4J_PASSWORD=your_aura_password

# Optional: remote embedding service (skips local model download)
# VALTER_EMBEDDING_SERVICE_URL=https://your-railway-service.up.railway.app
```

:::caution
Never commit the `.env` file to version control. It is listed in `.gitignore`. For the full list of available variables, see the [Environment Variables](./environment.md) reference.
:::

---
# docs/configuration/integrations.md
---


# External Integrations

Valter integrates with several external services. Each integration is optional for local development but may be required in staging or production. This page provides step-by-step setup for each service.

## Groq API

Groq provides fast LLM inference used for document classification, factual extraction, and query expansion in hybrid search.

### What it enables

- `POST /v1/factual/extract` -- extracts structured factual data from legal documents
- Query expansion in hybrid search -- generates semantic variants of user queries for broader recall

### Setup

1. Create an account at [console.groq.com](https://console.groq.com) and generate an API key.

2. Set the following environment variables:

```bash
VALTER_GROQ_API_KEY=gsk_your_key_here
VALTER_GROQ_ENABLED=true
# Optional: override the default model
# VALTER_GROQ_MODEL=qwen/qwen3-32b
```

3. Verify by starting the API and checking that factual endpoints respond without errors.

### Without Groq

When Groq is not configured, the system operates normally with these differences:

- Factual extraction endpoints return an error indicating the feature is disabled
- Hybrid search works without query expansion (single query only)
- No LLM costs are incurred

## Cloudflare R2

Cloudflare R2 provides S3-compatible object storage for workflow artifacts (generated PDFs, JSON reports). It replaces local filesystem storage for production deployments.

### Setup

1. In the Cloudflare dashboard, create an R2 bucket and generate S3-compatible API credentials.

2. Set the following environment variables:

```bash
VALTER_R2_ACCOUNT_ID=your_cloudflare_account_id
VALTER_R2_ACCESS_KEY_ID=your_access_key
VALTER_R2_SECRET_ACCESS_KEY=your_secret_key
# Optional: override defaults
# VALTER_R2_BUCKET_NAME=valter-artifacts
# VALTER_R2_PRESIGN_TTL_SECONDS=600
```

3. The R2 endpoint URL is auto-constructed from the account ID (`https://{account_id}.r2.cloudflarestorage.com`). Override with `VALTER_R2_ENDPOINT_URL` if needed.

### Canary rollout

Use `VALTER_R2_CANARY_PERCENT` to gradually migrate artifact storage from local disk to R2:

```bash
# Start with 10% of uploads going to R2
VALTER_R2_CANARY_PERCENT=10

# After validation, increase to 50%
VALTER_R2_CANARY_PERCENT=50

# Full migration
VALTER_R2_CANARY_PERCENT=100
```

The value is clamped to the 0-100 range internally.

### Without R2

When R2 credentials are not set, all artifacts are stored locally at `VALTER_UPLOAD_STORAGE_PATH` (default: `data/datasets/uploads/raw`). This is the default behavior and is suitable for development.

## Neo4j Aura

Neo4j Aura is a managed graph database service. It is the required graph backend for staging and production environments. Local development can use either Neo4j Community Edition or Aura.

### Local development (Community Edition)

1. Run Neo4j locally via Docker (not included in `docker-compose.yml` by default):

```bash
docker run -d \
  --name neo4j-dev \
  -p 7474:7474 -p 7687:7687 \
  -e NEO4J_AUTH=neo4j/neo4j_dev \
  neo4j:5-community
```

2. Use the default connection settings (no changes to `.env` needed):

```bash
VALTER_NEO4J_URI=bolt://localhost:7687
VALTER_NEO4J_USERNAME=neo4j
VALTER_NEO4J_PASSWORD=neo4j_dev
```

### Aura setup (staging/production)

1. Create an Aura instance at [console.neo4j.io](https://console.neo4j.io). Save the connection URI, username, and password from the instance creation screen.

2. Set the environment variables:

```bash
VALTER_NEO4J_URI=neo4j+s://xxxxxxxx.databases.neo4j.io
VALTER_NEO4J_USERNAME=neo4j
VALTER_NEO4J_PASSWORD=your_aura_password
```

3. Validate the connection:

```bash
make validate-aura
```

:::caution
Any PR that modifies graph-related code (files in `stores/graph.py`, Cypher queries, graph endpoints) must pass `make validate-aura` before merging. This runs `scripts/validate_aura.py` with a 15-second latency threshold to ensure compatibility with the managed service.
:::

### Production constraints

In production (`VALTER_ENV=production`), the application enforces:

- `VALTER_NEO4J_URI` must not point to `localhost` or `127.0.0.1`
- `VALTER_NEO4J_PASSWORD` must not be a weak default (`neo4j_dev`, `password`, `changeme`, etc.)

## Railway.app

Railway hosts Valter's production and staging environments. Each runtime mode runs as a separate Railway service sharing the same codebase.

### Service architecture

| Railway Service | `VALTER_RUNTIME` | Purpose |
|---|---|---|
| API | `api` | REST API server |
| Worker | `worker` | Background ingest job processor |
| MCP Remote | `mcp-remote` | MCP HTTP server for ChatGPT/Claude |

### Deployment configuration

The deployment is configured by two files:

- `railway.json` -- Railway-specific build and deploy settings
- `Dockerfile` -- Container image definition

The container entrypoint is `scripts/start-command.sh`, which reads `VALTER_RUNTIME` and starts the appropriate process. Railway's `$PORT` variable is respected automatically.

### Setting up a new Railway service

1. Connect the GitHub repository to Railway.
2. Create three services from the same repo, each with a different `VALTER_RUNTIME`.
3. Set all required `VALTER_*` environment variables in the Railway dashboard for each service. See [Environment Variables](./environment.md) for the full reference.
4. Ensure `VALTER_ENV=production` for production services.

:::note
The MCP remote service uses `$PORT` from Railway as a fallback for `VALTER_MCP_SERVER_PORT`. The start script handles this automatically.
:::

### Remote services

For production deployments, you can offload embedding and reranking to dedicated Railway services:

```bash
VALTER_EMBEDDING_SERVICE_URL=https://your-embedding-service.up.railway.app
VALTER_RERANKER_SERVICE_URL=https://your-reranker-service.up.railway.app
```

This avoids bundling the ~500 MB embedding model in the main container.

## HuggingFace

Valter uses a HuggingFace model for generating semantic embeddings of legal documents. The default model is `rufimelo/Legal-BERTimbau-sts-base`, a Portuguese legal domain model producing 768-dimensional vectors.

### Downloading the model

```bash
make download-model
```

This downloads the model from HuggingFace Hub and caches it at `~/.cache/huggingface/`. The download is approximately 500 MB and only needs to happen once.

### Overriding the model

To use a different embedding model, set the environment variable before downloading:

```bash
VALTER_EMBEDDING_MODEL=my-org/my-model make download-model
```

:::caution
Changing the embedding model changes the vector dimensions and invalidates all existing vectors in Qdrant. You must re-ingest all documents after switching models. Update `VALTER_EMBEDDING_DIMENSION` to match the new model's output dimension.
:::

### Remote encoding alternative

If you prefer not to run the model locally (e.g., in CI or lightweight containers), set `VALTER_EMBEDDING_SERVICE_URL` to point to a remote encoding service:

```bash
VALTER_EMBEDDING_SERVICE_URL=https://your-embedding-service.up.railway.app
```

When this variable is set, the local model is not loaded and `make download-model` is not required. The same applies to the reranker via `VALTER_RERANKER_SERVICE_URL`.

---
# docs/development/setup.md
---


# Development Setup

This guide walks through setting up a complete local development environment for Valter, from installing prerequisites to verifying that all services are running.

## Prerequisites

Before starting, ensure you have the following installed:

| Tool | Version | Purpose |
|---|---|---|
| Python | >= 3.12 | Runtime language |
| Docker + Docker Compose | Recent stable | Database services (PostgreSQL, Qdrant, Redis) |
| make | Any | Canonical command interface |
| Git | Any | Version control |
| uv (recommended) | Recent | Fast Python package manager. Falls back to `pip` if unavailable. |

**Resource requirements:** approximately 4 GB RAM and 3 GB disk space (including the embedding model).

## Step 1: Clone and Install Dependencies

Clone the repository and create a Python virtual environment:

```bash
git clone <repo-url>
cd Valter
python -m venv .venv
source .venv/bin/activate
```

Install all dependencies including dev tools:

```bash
# Preferred: using uv (faster)
uv pip install -e ".[dev]"

# Alternative: using pip
pip install -e ".[dev]"
```

This installs FastAPI, SQLAlchemy, Qdrant client, Neo4j driver, sentence-transformers, and all development tools (pytest, ruff, mypy).

## Step 2: Start Database Services

The `docker-compose.yml` file defines three database services for local development:

```bash
make docker-up
```

This starts the following containers:

| Service | Image | Port | Purpose |
|---|---|---|---|
| PostgreSQL | `postgres:16-alpine` | 5432 | Document metadata, migrations |
| Qdrant | `qdrant/qdrant:latest` | 6333 | Vector similarity search |
| Redis | `redis:7-alpine` | 6379 | Caching, rate limiting, ARQ job queue |

Verify the services are running:

```bash
docker compose ps
```

All three containers should show `running` status.

:::note
Neo4j is not included in `docker-compose.yml`. For graph features, either run Neo4j locally via Docker (see [Neo4j setup](../configuration/integrations.md#local-development-community-edition)) or connect to a Neo4j Aura instance. Graph features are optional for basic development.
:::

## Step 3: Configure Environment

Most environment variables have defaults that work with the `docker-compose.yml` stack. For basic development, no `.env` file is required.

To enable optional features, create a `.env` file at the project root:

```bash
# .env (optional — only for features you need)

# Groq LLM (enables factual extraction, query expansion)
# VALTER_GROQ_API_KEY=gsk_your_key_here
# VALTER_GROQ_ENABLED=true

# Neo4j (enables graph features)
# VALTER_NEO4J_URI=bolt://localhost:7687

# Remote embedding (skips local model download)
# VALTER_EMBEDDING_SERVICE_URL=https://your-service.up.railway.app
```

See the [Environment Variables](../configuration/environment.md) reference for all available options.

## Step 4: Run Database Migrations

Apply Alembic migrations to set up the PostgreSQL schema:

```bash
make migrate
```

This runs `alembic upgrade head` using the configuration in `migrations/alembic.ini`. The migration connects to the database defined by `VALTER_DATABASE_URL` (defaults to the local PostgreSQL from docker-compose).

:::caution
If migrations fail with a connection error, verify that the PostgreSQL container is running with `docker compose ps` and that port 5432 is accessible.
:::

## Step 5: Download the Embedding Model

The semantic search feature requires a pre-trained embedding model:

```bash
make download-model
```

This downloads `rufimelo/Legal-BERTimbau-sts-base` (~500 MB) from HuggingFace Hub and caches it at `~/.cache/huggingface/`. The download only needs to happen once.

To use a different model:

```bash
VALTER_EMBEDDING_MODEL=my-org/my-model make download-model
```

:::tip
If you set `VALTER_EMBEDDING_SERVICE_URL` in your `.env` to point to a remote encoding service, you can skip this step entirely. The remote service handles embedding generation.
:::

## Step 6: Verify the Setup

### Start the development server

```bash
make dev
```

The API starts at `http://localhost:8000`. Verify with:

```bash
curl http://localhost:8000/v1/health
```

A successful response confirms that the API is running and database connections are healthy.

### Run the test suite

```bash
make test
```

This runs 660+ tests across unit, regression, and MCP test suites. All tests should pass without external service dependencies (stores are mocked in unit tests).

### Run linting

```bash
make lint
```

This runs `ruff check` and `ruff format --check` across `src/` and `tests/`.

## Make Targets

The `Makefile` is the canonical command interface. Use `make <target>` for all routine operations.

| Target | Description |
|---|---|
| `make dev` | Start development server with hot reload (port 8000) |
| `make test` | Run the full pytest suite |
| `make test-cov` | Run tests with coverage report |
| `make test-neo4j-live` | Run Neo4j integration tests (requires Aura credentials) |
| `make lint` | Check code style with ruff (check + format verification) |
| `make fmt` | Auto-fix code style issues and format code |
| `make quality` | Run lint, mypy (scoped), and tests together |
| `make migrate` | Apply Alembic database migrations |
| `make worker-ingest` | Start the ARQ background worker for ingest jobs |
| `make mcp-remote` | Start the MCP HTTP server (port 8001) |
| `make docker-up` | Start database containers (PostgreSQL, Qdrant, Redis) |
| `make docker-down` | Stop and remove database containers |
| `make download-model` | Download the embedding model from HuggingFace |
| `make validate-aura` | Validate graph queries against a live Neo4j Aura instance |

:::tip
Use `make quality` before pushing code. It runs linting, type checking, and the full test suite in sequence, matching the CI validation pipeline.
:::

## Troubleshooting

### Database connection errors

If `make migrate` or `make dev` fails to connect to PostgreSQL:

```bash
# Check that containers are running
docker compose ps

# Check PostgreSQL logs
docker compose logs postgres

# Restart the stack
make docker-down && make docker-up
```

### Port conflicts

If port 5432, 6333, or 6379 is already in use, either stop the conflicting service or modify the port mappings in `docker-compose.yml`.

### Model download failures

If `make download-model` fails due to network issues, retry or set `VALTER_EMBEDDING_SERVICE_URL` to use a remote encoding service instead.

---
# docs/development/conventions.md
---


# Coding Conventions

These conventions are enforced across the Valter codebase. They are derived from the project's governance rules and established through practice. All contributors -- human and AI agents -- must follow them.

## Language and Typing

Valter is a Python 3.12+ project. All code must include type hints on public functions and use modern Python features.

**Core rules:**

- **Type hints** are mandatory on all public function signatures (parameters and return types)
- **Pydantic v2** is the only model framework for domain objects. Do not use `dataclasses` or plain dicts for data that crosses API boundaries.
- **`async/await`** is required for all I/O operations (database queries, HTTP calls, file reads). No synchronous I/O in the hot path.

```python
# Correct: typed, async, Pydantic model
async def get_decisao(decisao_id: str) -> DecisaoDetail:
    ...

# Wrong: missing types, sync I/O
def get_decisao(id):
    result = db.query(...)  # blocking call
```

## Architecture Rules

The dependency rule is strictly enforced: `api/` depends on `core/`, which depends on `models/`. No reverse imports.

```
api/ ---> core/ ---> models/
            ^
            |
stores/ implements protocols from core/protocols.py
```

**Key constraints:**

- **`core/` never imports `stores/` directly.** Store implementations are injected via FastAPI's `Depends()` dependency injection. This keeps business logic decoupled from infrastructure.
- **`stores/` implements protocols** defined in `core/protocols.py`. Each store (PostgreSQL, Neo4j, Qdrant, Redis) conforms to an abstract interface.
- **Before creating an endpoint, read the entire store method** -- not just its signature. Identify hardcoded limits, case-sensitivity behavior, data formats, and possible errors. Document any limitations in the endpoint's OpenAPI schema.

```python
# In api/routes/graph.py -- store injected via Depends()
@router.post("/divergences")
async def get_divergences(
    request: DivergenceRequest,
    graph_store: Neo4jGraphStore = Depends(get_graph_store),
):
    return await graph_store.find_divergences(request.ministro, request.tema)
```

## Logging

Valter uses `structlog` for structured JSON logging. Every HTTP request receives a `trace_id` for correlation.

**Guidelines:**

- Use structured key-value fields, not string interpolation
- Log at the appropriate level: `debug` for development details, `info` for normal operations, `warning` for recoverable issues, `error` for failures
- The log level is configured via `VALTER_LOG_LEVEL` (default: `INFO`)

```python
import structlog

logger = structlog.get_logger()

# Correct: structured fields
logger.info("document_indexed", doc_id=doc.id, chunks=len(chunks))

# Wrong: string interpolation
logger.info(f"Indexed document {doc.id} with {len(chunks)} chunks")
```

## Error Handling

Error handling must be precise. The goal is to catch expected failures and let programming bugs surface as 500 errors so they are visible and fixable.

**Rules:**

- **No catch-all `except Exception`.** Catch only specific, expected errors (connection timeouts, network failures, known validation errors).
- **Programming bugs must surface as 500s.** `KeyError`, `TypeError`, `AttributeError`, and similar bugs should never be silently caught. They indicate a code defect that needs fixing.
- **Verify that catch blocks do not swallow internal errors.** A broad `except` on an external call can accidentally hide bugs in the code that prepares or processes the call.

```python
# Correct: catch only expected errors
try:
    result = await neo4j_driver.execute_query(cypher, params)
except neo4j.exceptions.ServiceUnavailable:
    logger.warning("neo4j_unavailable", uri=settings.NEO4J_URI)
    raise HTTPException(status_code=503, detail="Graph database unavailable")

# Wrong: catch-all hides bugs
try:
    result = await neo4j_driver.execute_query(cypher, params)
except Exception:
    return {"results": []}  # Silently returns empty on ANY error
```

## Naming Conventions

| Context | Convention | Example |
|---|---|---|
| REST endpoints | kebab-case | `/v1/graph/optimal-argument` |
| Python functions/variables | snake_case | `find_divergences` |
| Python classes | PascalCase | `Neo4jGraphStore` |
| Files | snake_case | `graph_store.py` |
| Environment variables | UPPER_SNAKE_CASE with `VALTER_` prefix | `VALTER_NEO4J_URI` |

:::note
The endpoint `/similar_cases` uses underscores instead of kebab-case. This is a legacy naming that would be a breaking change to rename. New endpoints must use kebab-case.
:::

## Git and PR Workflow

### Commit messages

All commits must follow the [Conventional Commits](https://www.conventionalcommits.org/) specification:

```
feat: add divergence detection endpoint
fix: handle empty result set in phase analysis
chore: update Neo4j driver to 5.28
docs: add MCP configuration guide
refactor: extract query builder from retriever
test: add edge case tests for graph routes
```

### Branch naming

```
feat/[issue-id]-[description]
fix/[issue-id]-[description]
chore/[issue-id]-[description]
docs/[issue-id]-[description]
refactor/[issue-id]-[description]
test/[issue-id]-[description]
codex/[issue-id]-[description]
```

Example: `feat/SEN-267-mcp-auth-claude`

Claude Code branches use the `-claude` suffix. Codex branches use the `codex/` prefix. See the [Contributing Guide](./contributing.md#multi-agent-coordination) for multi-agent rules.

## Pre-Commit Review Checklist

Before committing, verify every change against these five checks. The goal is to commit code that **works**, not code that merely compiles.

### 1. Parameter honesty

Every parameter exposed in the API must do exactly what its `description` says. If there are internal hardcoded limits (`LIMIT` clauses, `WHERE >= N` filters), they must be documented in the OpenAPI schema. If a parameter cannot fulfill its promise, either fix the implementation or document the limitation.

### 2. Error handling precision

Verify that:
- No `except Exception` catch-all exists
- Only expected errors are caught (connection failures, timeouts)
- Programming bugs (`KeyError`, `TypeError`) are allowed to surface as 500s
- Catch blocks do not accidentally swallow internal system errors

### 3. Data edge cases

Test the behavior for:
- Input that does not exist in the backend (nonexistent ministro, invalid category)
- Empty result sets -- can the client distinguish "not found" from "does not exist"?
- Inputs with accents, mixed case, and special characters
- Incompatible filter combinations

### 4. Tests test behavior

Tests must verify actual behavior, not just schema structure:
- Test post-processing logic, not just that the response has the right fields
- Test edge cases: empty results, filters that exclude everything, boundary values
- Use mocked stores (do not depend on live services for unit tests)

### 5. Read the store first

Before writing an endpoint, read the entire store method it calls:
- Identify hardcoded limits and their effects on results
- Understand case-sensitivity behavior in queries
- Note data formats and any transformations applied
- Document any limitation the client needs to know about

## Priority Order

When conventions or principles conflict, resolve them in this order:

1. **Correctness** -- especially for legal data, billing, and data integrity
2. **Simplicity and clarity** -- code that another agent or developer can understand without additional context
3. **Maintainability** -- easy to modify without breaking unrelated features
4. **Reversibility** -- prefer decisions that can be undone
5. **Performance** -- optimize only when there is evidence of a problem

## Non-Goals

The following actions are prohibited without explicit authorization:

- Introducing abstractions without a clear, immediate need
- Adding dependencies for problems already solved in the codebase
- Refactoring working code without a specific issue driving the change
- Optimizing code without evidence of a performance problem
- Expanding scope beyond the issue being worked on

---
# docs/development/testing.md
---


# Testing Guide

Valter uses pytest with pytest-asyncio for async test support. The test suite spans 64 files with approximately 12,200 lines of test code, covering unit, integration, regression, and MCP tests.

## Running Tests

All test commands use Make targets:

```bash
make test             # Run the full test suite (660+ tests)
make test-cov         # Run with coverage report (term-missing format)
make test-neo4j-live  # Run Neo4j integration tests (requires Aura credentials)
make lint             # Check code style (ruff check + format verification)
make quality          # Run lint + mypy (scoped) + tests in sequence
```

To run a specific test file or test function:

```bash
pytest tests/unit/test_graph_routes.py -v
pytest tests/unit/test_features_search.py::test_empty_results -v
```

:::tip
Use `make quality` before pushing. It runs the same checks as CI: linting, type checking (on scoped files), and the full test suite.
:::

## Test Configuration

Test configuration lives in `pyproject.toml`:

```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
asyncio_mode = "auto"
```

Key settings:

- **`testpaths`**: pytest only discovers tests under `tests/`
- **`asyncio_mode = "auto"`**: all `async def test_*` functions run as async tests automatically, without needing the `@pytest.mark.asyncio` decorator
- **Global fixtures**: defined in `tests/conftest.py`, including `settings` (test-safe `Settings` instance) and `live_graph_store` (Neo4j Aura connection, skipped if credentials are missing)

## Test Categories

### Unit Tests (57 files, ~10,000 lines)

Unit tests cover core logic, route handlers, stores (mocked), authentication, and models. They run without any external service dependencies.

Key test files:

| File | Tests | Coverage |
|---|---|---|
| `test_graph_routes.py` | ~162 | Graph API endpoints (divergences, optimal argument, etc.) |
| `test_features_search.py` | ~40 | Hybrid search, filtering, ranking |
| `test_retriever.py` | ~27 | Retriever logic, KG boost, query expansion |
| `test_mcp_tools.py` | ~28 | MCP tool registration and validation |

**Pattern:** all external dependencies (stores, Redis, Neo4j, Qdrant) are mocked using `unittest.mock.AsyncMock` or `MagicMock`. No live service connections in unit tests.

### Integration Tests (3 files, ~350 lines)

Integration tests run against a live Neo4j Aura instance and verify end-to-end graph query behavior.

| File | Purpose |
|---|---|
| `test_kg_live_graph.py` | Knowledge graph query correctness |
| `test_retriever_kg_live.py` | Retriever with real KG boost |
| `test_graph_store_live.py` | Graph store methods against real data |

These tests **skip automatically** if the Neo4j environment variables (`VALTER_NEO4J_URI`, `VALTER_NEO4J_USERNAME`, `VALTER_NEO4J_PASSWORD`) are not set. To run them:

```bash
export VALTER_NEO4J_URI=neo4j+s://xxxxx.databases.neo4j.io
export VALTER_NEO4J_USERNAME=neo4j
export VALTER_NEO4J_PASSWORD=your_password
make test-neo4j-live
```

### Regression Tests (3 files, ~130 lines)

Regression tests guard against quality regressions in search and graph results:

- **Golden questions**: known-good search queries with expected result characteristics
- **KG quality CI**: knowledge graph consistency checks (node/relationship counts, expected patterns)
- **Parity tests**: verify compatibility between Valter's API responses and the Juca frontend's expectations

### MCP Tests (1 file, ~1,700 lines)

MCP tests cover all 28 MCP tools, verifying:

- Tool registration and metadata
- Parameter validation (required fields, types, constraints)
- Response format compliance with the MCP protocol
- Error handling for invalid inputs

## Writing Tests

### Test structure

Follow the Arrange-Act-Assert pattern:

```python
async def test_divergences_returns_empty_for_unknown_ministro():
    # Arrange
    mock_store = AsyncMock()
    mock_store.find_divergences.return_value = []

    # Act
    result = await get_divergences(
        request=DivergenceRequest(ministro="Unknown", tema="civil"),
        graph_store=mock_store,
    )

    # Assert
    assert result == []
    mock_store.find_divergences.assert_called_once_with("Unknown", "civil")
```

### What to test

Tests must verify **behavior**, not just schema structure:

- Test the actual logic and post-processing, not only that the response has the correct fields
- Test edge cases: empty results, filters that exclude everything, boundary values
- Test error paths: what happens with invalid inputs, missing data, service failures
- Test inputs with accents, mixed case, and special characters (relevant for Brazilian legal data)

### Mocking patterns

**AsyncMock for async store methods:**

```python
from unittest.mock import AsyncMock, MagicMock

# Mock an entire store
mock_doc_store = AsyncMock()
mock_doc_store.get_by_id.return_value = sample_document

# Mock a specific method with side effects
mock_graph_store = AsyncMock()
mock_graph_store.find_divergences.side_effect = ConnectionError("Neo4j unavailable")
```

**Dependency injection override in route tests:**

```python
from fastapi.testclient import TestClient
from valter.api.deps import get_graph_store

app.dependency_overrides[get_graph_store] = lambda: mock_graph_store
client = TestClient(app)
response = client.post("/v1/graph/divergences", json={"ministro": "Test"})
```

**Settings fixture from conftest.py:**

```python
@pytest.fixture
def settings():
    return Settings(
        _env_file=None,
        ENV="test",
        DATABASE_URL="postgresql+asyncpg://test:test@localhost:5432/valter_test",
        NEO4J_URI="bolt://localhost:7687",
        NEO4J_USERNAME="neo4j",
        NEO4J_PASSWORD="test",
        REDIS_URL="redis://localhost:6379/1",
    )
```

The `_env_file=None` parameter prevents the test `Settings` from loading `.env` values, ensuring test isolation.

### File naming

- Test files: `test_<module_name>.py`
- Test functions: `test_<behavior_description>`
- Place unit tests in `tests/unit/`, integration tests in `tests/integration/`, regression tests in `tests/regression/`

## Coverage

Current test coverage by category:

| Category | Files | Lines | Scope |
|---|---|---|---|
| Unit | 57 | ~10,000 | Core logic, routes, stores (mocked), auth, models |
| Integration | 3 | ~350 | Neo4j Aura live queries |
| Regression | 3 | ~130 | Golden set, KG quality, Juca parity |
| MCP | 1 | ~1,700 | 28 tools + handlers + validation |
| Load | 0 | 0 | Placeholder (Locust available in dev deps) |
| **Total** | **64** | **~12,200** | |

Generate a coverage report:

```bash
make test-cov
```

This produces a terminal report with `--cov-report=term-missing`, showing which lines lack test coverage.

## Coverage Gaps

The following modules have limited or no direct test coverage:

| Module | Status | Notes |
|---|---|---|
| `stores/document.py` (PostgresDocStore) | No direct test | Tested indirectly via route tests |
| `stores/vector.py` (QdrantVectorStore) | No direct test | Tested indirectly via retriever tests |
| `stores/cache.py` (RedisCacheStore) | No direct test | Tested indirectly via route tests |
| `stores/artifact_storage.py` | No test | R2/local artifact storage |
| `api/middleware.py` | No direct test | Auth, rate limiting, CORS middleware |
| `tests/load/` | Empty | Placeholder for Locust load tests |

These gaps are known. Store modules are exercised indirectly through route and retriever tests that mock them, but dedicated unit tests for each store would improve confidence in edge case handling.

---
# docs/development/contributing.md
---


# Contributing Guide

This guide covers the contribution workflow for Valter, including branch naming, commit conventions, the PR process, CI validation, and the multi-agent coordination model used in this project.

## Git Workflow

### Branch naming

All branches follow a consistent naming pattern:

```
<type>/[issue-id]-[description]
```

Supported types:

| Prefix | Use |
|---|---|
| `feat/` | New features |
| `fix/` | Bug fixes |
| `chore/` | Maintenance tasks (dependency updates, cleanup) |
| `docs/` | Documentation changes |
| `refactor/` | Code restructuring without behavior change |
| `test/` | Test additions or modifications |
| `codex/` | Changes made by the Codex agent |

Examples:

```
feat/SEN-267-mcp-auth-claude
fix/SEN-301-empty-divergence-response
chore/update-neo4j-driver
docs/configuration-reference
```

### Commit messages

All commits must follow the [Conventional Commits](https://www.conventionalcommits.org/) specification. The format is:

```
<type>: <description>
```

Valid types: `feat`, `fix`, `chore`, `docs`, `refactor`, `test`.

Examples:

```
feat: add divergence detection endpoint
fix: handle empty result set in phase analysis
chore: update Neo4j driver to 5.28
docs: add MCP configuration guide
refactor: extract query builder from retriever
test: add edge case tests for graph routes
```

### Starting work on a new task

Before starting any work, check that no other agent is working on conflicting branches:

```bash
git branch -a
git branch -a | grep codex   # Check for active Codex branches
```

Create your branch from `main`:

```bash
git checkout main
git pull origin main
git checkout -b feat/SEN-XXX-description-claude
```

## Pull Request Process

### 1. Implement and validate locally

Make your changes, then run the full validation suite:

```bash
make quality   # Runs lint + mypy (scoped) + tests
```

If your changes touch graph-related code:

```bash
make validate-aura   # Required for Neo4j/graph changes
```

### 2. Open a pull request

Push your branch and open a PR against `main`. The PR must use the template from `.github/pull_request_template.md`, which includes:

- **Summary**: description of the problem, change, and expected impact
- **Change type**: checkboxes for API/Core/Stores, Graph, Migrations, Scripts, Docs
- **Validation matrix**: applicable checks completed
- **Governance**: branch naming and commit convention compliance

### 3. Validation matrix

The PR template includes a validation matrix. Mark only the items applicable to your change:

| Change Type | Required Validation |
|---|---|
| API/Core/Stores | `make lint` + `make test` + relevant regression tests |
| Graph (Neo4j/Aura) | All above + `make validate-aura` + graph-specific unit tests |
| Migrations (Alembic) | `alembic upgrade head` in a safe environment. Test `downgrade()` if the migration is reversible. If irreversible, declare it in the PR and include a contingency plan. |
| Scripts (`scripts/`) | Local or staging execution allowed. Production execution requires explicit approval, a prior `--dry-run`, and a documented rollback plan. |

### 4. External consumer impact

If your change affects the API contract (request/response format, status codes, endpoint semantics):

- Evaluate impact on external consumers (Juca frontend, MCP clients)
- Avoid breaking changes in `/v1` endpoints, or provide a versioning/deprecation plan
- Update integration documentation when contracts change
- Add or update contract tests for critical endpoints

## CI/CD Pipeline

The CI pipeline runs on every pull request:

| Stage | What it checks |
|---|---|
| Lint | `ruff check` and `ruff format --check` on `src/` and `tests/` |
| Type check | `mypy` on scoped files (deps, ingest routes, MCP tools) |
| Test | `pytest` with verbose output and short tracebacks |
| Aura validation | For PRs touching graph code: `make validate-aura` with a 15-second latency threshold |

Deployment to Railway happens automatically when changes are merged to `main`. Each Railway service (API, Worker, MCP Remote) rebuilds from the same codebase, differentiated by the `VALTER_RUNTIME` environment variable.

## Multi-Agent Coordination

Valter is developed by two AI coding agents working in parallel:

| Agent | Environment | Branch convention |
|---|---|---|
| Claude Code | Local execution | Suffix: `-claude` (e.g., `feat/SEN-267-mcp-auth-claude`) |
| Codex (OpenAI) | Cloud execution | Prefix: `codex/` (e.g., `codex/sen-217-missing-integras`) |

### Fundamental rule

**Never work on the same branch as the other agent.** Before starting any task:

```bash
# Check for active Codex branches
git branch -a | grep codex

# If a codex/ branch exists touching the same files, coordinate before proceeding
```

### Avoiding conflicts

- Each agent works on separate issues and separate files when possible
- If both agents need to modify the same file, coordinate via separate PRs merged sequentially
- Always pull the latest `main` before creating a new branch

## Authorship

Valter is the exclusive property of Diego Sens (@sensdiego). All conception, architecture, product decisions, and intellectual property belong to the author.

When AI agents contribute to implementation, commits use this format:

```
Co-Authored-By (execucao): Claude Opus 4.6 <noreply@anthropic.com>
```

The term **(execucao)** indicates that the AI agent assists with code implementation. It does not imply authorship of the design or architecture.

## Priority Order

When making decisions about implementation, resolve conflicts in this order:

1. **Correctness** -- especially for legal data, billing, and data integrity
2. **Simplicity and clarity** -- code another agent or developer understands without additional context
3. **Maintainability** -- easy to modify without breaking unrelated features
4. **Reversibility** -- prefer decisions that can be undone
5. **Performance** -- optimize only with evidence of a problem

---
# docs/api/index.md
---


# API Reference

Valter exposes a REST API on port 8000 with API key authentication, Redis-backed rate limiting, cursor-based pagination, and structured JSON error responses.

## Base URL

| Environment | URL |
|---|---|
| Development | `http://localhost:8000` |
| Production | Railway deployment URL |

All versioned endpoints use the `/v1/` prefix. Non-versioned endpoints (`/health`, `/metrics`) live at the root.

```bash
# Development
curl http://localhost:8000/v1/retrieve -X POST -H "Content-Type: application/json" ...

# Production
curl https://<railway-url>/v1/retrieve -X POST -H "Content-Type: application/json" ...
```

## Authentication

API key authentication is controlled by the `VALTER_AUTH_ENABLED` environment variable (required `true` in production).

Pass the key as a Bearer token:

```http
POST /v1/retrieve HTTP/1.1
Authorization: Bearer vlt_k1_abc123...
Content-Type: application/json
```

### Scopes

Each API key carries one or more scopes that restrict which endpoints it can access:

| Scope | Access |
|---|---|
| `read` | Search, graph queries, verify, enrich, health |
| `write` | Ingest workflows, memory creation |
| `admin` | Metrics, dataset management, all operations |

### Security

- Keys are hashed with **SHA-256** before storage (not bcrypt).
- All API key usage is logged to a persistent **audit log** with timestamp, endpoint, and key fingerprint.
- Invalid or missing keys return `401 UNAUTHORIZED`.
- Insufficient scope returns `403 FORBIDDEN`.

## Rate Limiting

Rate limiting uses a **Redis sliding window** with per-key counters (INCR).

| Operation type | Default limit | Env var |
|---|---|---|
| Read endpoints | 100 requests/min | `VALTER_RATE_LIMIT_READ` |
| Write endpoints | 10 requests/min | `VALTER_RATE_LIMIT_WRITE` |

### Response headers

Every response includes rate limit headers:

```http
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 87
X-RateLimit-Reset: 1709136000
```

When the limit is exceeded, the API returns `429 RATE_LIMITED`.

:::caution
Rate limiting currently operates in **fail-closed** mode: if Redis is unreachable, all requests are blocked. Fail-open mode is planned for a future release.
:::

## Error Format

All errors return a consistent JSON envelope with an `error` object:

```json
{
  "error": {
    "code": "VALIDATION_ERROR",
    "message": "Request validation failed",
    "trace_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "details": null
  }
}
```

### Error codes

| Code | HTTP Status | Description |
|---|---|---|
| `INVALID_REQUEST` | 400 | Malformed request body or parameters |
| `UNAUTHORIZED` | 401 | Missing or invalid API key |
| `FORBIDDEN` | 403 | Valid key but insufficient scope |
| `NOT_FOUND` | 404 | Resource does not exist |
| `TIMEOUT` | 408 | Backend query exceeded time limit |
| `VALIDATION_ERROR` | 422 | Pydantic validation failure |
| `RATE_LIMITED` | 429 | Rate limit exceeded |
| `INTERNAL_ERROR` | 500 | Unhandled server error |
| `SERVICE_UNAVAILABLE` | 503 | Backend service (Neo4j, Qdrant, etc.) unreachable |

The `trace_id` field correlates with structured logs (structlog JSON format) for debugging.

## Pagination

List endpoints support **cursor-based pagination** using opaque base64-encoded cursors.

### Request parameters

| Parameter | Type | Description |
|---|---|---|
| `cursor` | `string` | Opaque cursor from a previous response. Omit on the first request. |
| `page_size` | `integer` | Number of results per page (must be <= `top_k`). |

### Response fields

| Field | Type | Description |
|---|---|---|
| `pagination.cursor` | `string \| null` | Cursor for the next page, or `null` if no more results. |
| `pagination.has_more` | `boolean` | Whether additional pages exist. |
| `pagination.total_estimate` | `integer \| null` | Estimated total result count. |

Cursors encode the last score, last document ID, page number, and a query hash for validation. A cursor from one query cannot be reused with a different query.

```json
{
  "data": [...],
  "meta": { "trace_id": "...", "latency_ms": 42.5 },
  "pagination": {
    "cursor": "eyJsYXN0X3Njb3JlIjogMC44NSwgInBhZ2UiOiAxfQ==",
    "has_more": true,
    "total_estimate": 156
  }
}
```

## Middleware Stack

Requests pass through the middleware stack in this order:

| Order | Middleware | Purpose |
|---|---|---|
| 1 | **CORS** | Configurable allowed origins for cross-origin requests |
| 2 | **MetricsIPAllowlist** | Restricts `/metrics` access to IPs in `VALTER_METRICS_IP_ALLOWLIST` (CIDR supported). Reads `cf-connecting-ip` and `x-forwarded-for` for proxy-aware IP resolution. |
| 3 | **RequestTracking** | Assigns a `trace_id` (UUID) to each request, records Prometheus request count and duration histograms |
| 4 | **RateLimit** | Redis sliding window rate limiting per API key |
| 5 | **Authentication** | API key validation + scope checking. Skips auth for `/health`, `/metrics`, `/docs`, `/openapi.json`, `/redoc`. |

Audit logging runs within the authentication middleware for all endpoints except health, metrics, and documentation paths.

## Endpoint Groups

| Group | Key endpoints | Docs |
|---|---|---|
| Search | `POST /v1/retrieve`, `POST /v1/similar_cases`, `POST /v1/search/features`, `POST /v1/factual/dual-search` | [Search Endpoints](./search/) |
| Graph | `POST /v1/graph/*` (13 endpoints) | [Graph Endpoints](./graph/) |
| Verify & Enrich | `POST /v1/verify`, `POST /v1/context/enrich`, `POST /v1/factual/extract` | [Verify & Enrich](./verify-enrich/) |
| Ingest | `POST /v1/ingest/*` (20+ endpoints) | [Ingest Endpoints](./ingest/) |
| Admin | `GET /v1/health`, `GET /metrics`, `GET/POST /v1/memories`, `GET /v1/datasets/*` | [Admin & Utility](./admin/) |
| MCP Tools | 28 tools via stdio/HTTP | [MCP Tools Reference](./mcp-tools/) |

---
# docs/api/search.md
---


# Search Endpoints

Four endpoints for searching and retrieving STJ legal decisions using hybrid strategies, AI-extracted features, and dual-vector analysis.

## POST /v1/retrieve

Hybrid search over the jurisprudence corpus combining BM25 lexical matching with semantic vector similarity, optional knowledge graph boost, and optional cross-encoder reranking.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `query` | `string` | **required** | Natural-language legal query (1-1000 chars) |
| `top_k` | `integer` | `20` | Number of results to retrieve (1-100) |
| `strategy` | `string` | `"weighted"` | Scoring strategy: `weighted`, `rrf`, `bm25`, or `semantic` |
| `include_kg` | `boolean` | `false` | Apply knowledge graph relevance boost before final ordering |
| `rerank` | `boolean` | `false` | Apply cross-encoder reranking. Improves precision, adds ~100-300ms |
| `expand_query` | `boolean` | `false` | Expand query with LLM-generated legal variants. Improves recall, adds ~500-1500ms |
| `weights` | `object` | `null` | Custom signal weights (see below) |
| `filters` | `object` | `null` | Post-retrieval filters (see below) |
| `page_size` | `integer` | `null` | Enable cursor pagination (1-50, must be <= `top_k`) |
| `cursor` | `string` | `null` | Continuation cursor from previous page |
| `include_stj_metadata` | `boolean` | `false` | Include STJ metadata via PostgreSQL lookup (~5-20ms extra) |

### Weights object

| Field | Type | Default | Description |
|---|---|---|---|
| `bm25` | `float` | `0.5` | BM25 lexical signal weight |
| `semantic` | `float` | `0.4` | Semantic embedding signal weight |
| `kg` | `float` | `0.1` | Knowledge graph boost weight |

### Filters object

| Field | Type | Description |
|---|---|---|
| `ministro` | `string` | Minister name (auto-normalized to uppercase) |
| `data_inicio` | `string` | Start date filter (YYYYMMDD format) |
| `data_fim` | `string` | End date filter (YYYYMMDD format) |
| `tipos_recurso` | `string[]` | Appeal type filter (array) |
| `resultado` | `string` | Outcome filter: `provido`, `improvido`, `parcialmente provido` |
| `source` | `string` | Source type filter: `corpus`, `embedding_only`, `ementa_only` |

:::note
Filters are applied **post-retrieval**, so the actual number of returned results may be less than `top_k` when filters exclude matches.
:::

### Example request

```bash
curl -X POST http://localhost:8000/v1/retrieve \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "dano moral atraso voo overbooking companhia aerea",
    "top_k": 10,
    "strategy": "weighted",
    "rerank": true,
    "filters": {
      "resultado": "provido",
      "data_inicio": "20200101"
    }
  }'
```

### Example response

```json
{
  "data": [
    {
      "id": "doc-stj-resp-1234567",
      "processo": "REsp 1.234.567/SP",
      "ministro": "NANCY ANDRIGHI",
      "data": "20230615",
      "orgao": "TERCEIRA TURMA",
      "ementa": "RECURSO ESPECIAL. TRANSPORTE AEREO. ...",
      "ementa_preview": "RECURSO ESPECIAL. TRANSPORTE AEREO...",
      "tese": "O atraso significativo de voo gera dano moral presumido...",
      "razoes_decidir": null,
      "score": 0.92,
      "has_integra": true,
      "score_breakdown": {
        "bm25": 0.78,
        "semantic": 0.95,
        "kg_boost": null,
        "rerank_score": 0.92
      },
      "matched_terms": ["dano", "moral", "atraso", "voo"],
      "stj_metadata": null
    }
  ],
  "meta": {
    "trace_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "latency_ms": 245.3,
    "cache_hit": false,
    "model_version": "legal-bertimbau-v1.0",
    "expansion_queries": null
  },
  "pagination": {
    "cursor": null,
    "has_more": false,
    "total_estimate": 8
  }
}
```

## POST /v1/similar_cases

Find cases similar to a given decision using a blend of 70% semantic similarity and 30% structural knowledge graph overlap.

:::note
The URL uses an underscore (`similar_cases`) for legacy compatibility. This cannot be renamed without breaking existing consumers.
:::

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `document_id` | `string` | **required** | Source document ID to compare against |
| `top_k` | `integer` | `10` | Number of similar cases to return (1-100) |
| `include_structural` | `boolean` | `true` | Include KG structural similarity in the score. Disabling uses semantic-only (faster). |

### Example request

```bash
curl -X POST http://localhost:8000/v1/similar_cases \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "document_id": "doc-stj-resp-1234567",
    "top_k": 5,
    "include_structural": true
  }'
```

:::tip
On timeout with structural mode enabled, the endpoint automatically retries with semantic-only fallback.
:::

## POST /v1/search/features

Structured search over AI-extracted document features with 9 combinable AND filters. At least one filter is required.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `categorias` | `string[]` | `null` | Legal categories (OR/ANY semantics within the list) |
| `dispositivo_norma` | `string` | `null` | Legal statute filter (e.g., `CDC`, `CC/2002`). Exact containment match. |
| `resultado` | `string` | `null` | Outcome filter (exact, case-sensitive) |
| `unanimidade` | `boolean` | `null` | Unanimous decision filter |
| `tipo_decisao` | `string` | `null` | Decision type (exact, case-sensitive) |
| `tipo_recurso` | `string` | `null` | Appeal type (exact, case-sensitive) |
| `ministro_relator` | `string` | `null` | Reporting minister (exact, case-sensitive) |
| `argumento_vencedor` | `string` | `null` | Winning argument text (partial match, case-insensitive) |
| `argumento_perdedor` | `string` | `null` | Losing argument text (partial match, case-insensitive) |
| `limit` | `integer` | `20` | Results per page (1-100) |
| `offset` | `integer` | `0` | Pagination offset |

:::caution
Most scalar filters use **exact case-sensitive** matching. Only `argumento_vencedor` and `argumento_perdedor` support partial case-insensitive matching (SQL `ILIKE`). The `categorias` field uses OR semantics (any listed category matches), while all other filters combine with AND.
:::

### Example request

```bash
curl -X POST http://localhost:8000/v1/search/features \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "categorias": ["Direito do Consumidor"],
    "resultado": "provido",
    "dispositivo_norma": "CDC",
    "limit": 10
  }'
```

### Example response

```json
{
  "data": [
    {
      "document_id": "doc-stj-resp-9876543",
      "processo": "REsp 9.876.543/RJ",
      "ementa_preview": "CONSUMIDOR. PRODUTO DEFEITUOSO...",
      "categorias": ["Direito do Consumidor"],
      "resultado": "provido",
      "tipo_decisao": "Acórdão",
      "unanimidade": true,
      "dispositivo_norma": ["CDC", "CC/2002"],
      "argumento_vencedor": "Responsabilidade objetiva do fornecedor..."
    }
  ],
  "total": 42,
  "meta": {
    "trace_id": "b2c3d4e5-f6a7-8901-bcde-f12345678901",
    "latency_ms": 35.8
  }
}
```

## POST /v1/factual/dual-search

Dual-vector search that separates facts from legal thesis, searches each independently, then produces a divergence report. The pipeline: text input, LLM extraction (via Groq), encode each digest into separate vectors, vector search, divergence analysis.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `text` | `string` | `null` | Legal text for analysis (50-15000 chars). Required if `document_id` is not provided. |
| `document_id` | `string` | `null` | Corpus document ID. Required if `text` is not provided. |
| `top_k` | `integer` | `10` | Max results per dimension (1-50) |
| `filters` | `object` | `null` | Same filter object as `/v1/retrieve` (ministro, resultado, source) |

:::note
Either `text` or `document_id` must be provided. When `document_id` is used, the text is assembled from the document's ementa, tese, and razoes_decidir fields.
:::

### Example request

```bash
curl -X POST http://localhost:8000/v1/factual/dual-search \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "document_id": "doc-stj-resp-1234567",
    "top_k": 5
  }'
```

### Example response

```json
{
  "data": {
    "factual_digest": {
      "bullets": [
        { "index": 0, "text": "Consumidor adquiriu produto com defeito...", "source_excerpt": "...", "uncertainty": false }
      ],
      "digest_text": "Consumidor adquiriu produto com defeito de fabricacao...",
      "extraction_model": "llama-3.3-70b-versatile"
    },
    "thesis_digest": {
      "thesis_text": "Responsabilidade objetiva do fornecedor por vicio do produto...",
      "legal_basis": ["CDC art. 12", "CDC art. 18"],
      "precedents_cited": ["REsp 1.234.567/SP"],
      "extraction_model": "llama-3.3-70b-versatile"
    },
    "factual_results": [
      { "id": "doc-001", "processo": "REsp 111.222/MG", "ministro": "NANCY ANDRIGHI", "data": "20230101", "score": 0.89 }
    ],
    "thesis_results": [
      { "id": "doc-002", "processo": "REsp 333.444/PR", "ministro": "MARCO BUZZI", "data": "20220615", "score": 0.85 }
    ],
    "overlap_ids": [],
    "fact_only_ids": ["doc-001"],
    "thesis_only_ids": ["doc-002"],
    "divergence_summary": "Os fatos sao similares a doc-001 mas a tese juridica diverge. doc-002 compartilha a mesma tese mas com fatos distintos."
  },
  "meta": {
    "trace_id": "c3d4e5f6-a7b8-9012-cdef-123456789012",
    "latency_ms": 1850.5
  }
}
```

The divergence report reveals three categories:
- **overlap_ids** -- cases matching on both facts and thesis (strong precedent).
- **fact_only_ids** -- factually similar but legally different (potential distinguishing).
- **thesis_only_ids** -- same legal thesis but different facts (thematic precedent).

---
# docs/api/graph.md
---


# Graph Endpoints

13 endpoints under `/v1/graph/` exposing Neo4j knowledge graph analytics for legal reasoning. All endpoints use `POST` and return results wrapped in `{ data, meta }` envelopes.

All graph queries have a **15-second timeout**. If Neo4j is unreachable, the API returns `503 SERVICE_UNAVAILABLE`. Programming errors propagate as `500 INTERNAL_ERROR`.

## POST /v1/graph/divergencias

Detect active jurisprudential divergences -- clusters of decisions with split outcomes (provido vs improvido) for the same legal criteria. The `divergence_score` is computed as `minority / total`, so a perfectly balanced 50/50 split scores highest.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `categoria_id` | `string` | `null` | Filter by legal category (e.g., `cat-direito-consumidor`). Omit for all categories. |
| `limit` | `integer` | `10` | Max divergence clusters returned (1-50) |

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/divergencias \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{ "categoria_id": "cat-direito-consumidor", "limit": 5 }'
```

## POST /v1/graph/divergencias/turma

Analyze how different ministers decide on the same legal topic. Returns per-minister outcome counts for criteria matching the topic.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `tema` | `string` | **required** | Legal topic for divergence analysis (1-500 chars, e.g., `dano moral`) |

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/divergencias/turma \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{ "tema": "dano moral em relacoes de consumo" }'
```

:::note
Despite the name, this endpoint currently matches criteria by topic substring rather than using explicit turma (court division) metadata.
:::

## POST /v1/graph/optimal-argument

Find arguments (criteria, legal statutes, precedents) with the highest success rate for a given legal category and desired outcome.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `categoria_id` | `string` | **required** | Legal category ID (e.g., `cat-direito-consumidor`) |
| `resultado_desejado` | `string` | `"provido"` | Target outcome: `provido`, `improvido`, or `parcialmente provido` |
| `tipo_argumento` | `string` | `"all"` | Filter by type: `criterio`, `dispositivo`, `precedente`, or `all` |
| `min_decisions` | `integer` | `2` | Minimum supporting decisions for statistical relevance (2-100, internal floor is 2) |
| `top_k` | `integer` | `10` | Max arguments returned (1-50) |

:::tip
The graph store returns at most ~11 items (5 criteria + 3 statutes + 3 precedents). Setting `top_k` higher than 11 will not yield more results.
:::

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/optimal-argument \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "categoria_id": "cat-direito-consumidor",
    "resultado_desejado": "provido",
    "tipo_argumento": "criterio",
    "min_decisions": 3
  }'
```

## POST /v1/graph/optimal-argument-by-ministro

Minister-specific variant of optimal argument. Compares a specific minister's success rates against the overall category average. The `delta` field shows the difference: positive means the argument performs better with this minister.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `categoria_id` | `string` | **required** | Legal category ID |
| `ministro` | `string` | **required** | Minister name (auto-normalized to uppercase) |
| `resultado_desejado` | `string` | `"provido"` | Target outcome |
| `tipo_argumento` | `string` | `"all"` | Argument type filter |
| `min_decisions` | `integer` | `1` | Min minister-side decisions (1-100) |
| `min_category_decisions` | `integer` | `2` | Min category-wide decisions (2-100, internal floor is 2) |
| `top_k` | `integer` | `10` | Max arguments returned (1-50) |

The response includes `recommended_arguments` (delta > 0) and `avoid_arguments` (delta < -0.1).

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/optimal-argument-by-ministro \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "categoria_id": "cat-direito-consumidor",
    "ministro": "Nancy Andrighi",
    "resultado_desejado": "provido"
  }'
```

## POST /v1/graph/ministro-profile

Comprehensive judicial behavior profile for a specific STJ minister: total decisions, date range, top criteria, outcome distribution, peer divergences, and most-cited decisions.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `ministro` | `string` | **required** | Minister name (auto-normalized to uppercase) |
| `include_divergencias` | `boolean` | `true` | Include peer divergences on the same criteria |
| `include_precedentes` | `boolean` | `true` | Include most-cited decisions by this minister |
| `limit_criterios` | `integer` | `10` | Max criteria in the ranking (1-50, store cap is 10) |

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/ministro-profile \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "ministro": "Nancy Andrighi",
    "include_divergencias": true,
    "include_precedentes": true,
    "limit_criterios": 5
  }'
```

:::note
Internal graph caps: up to 10 criteria, 20 peer divergences, 5 most-cited decisions. The `limit_criterios` parameter cannot exceed these store-level limits.
:::

## POST /v1/graph/temporal-evolution

Track how a legal criterion's application changes over time. Returns per-period buckets with provido/improvido splits and a heuristic trend label (growing, declining, or stable).

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `criterio` | `string` | **required** | Legal criterion to analyze (1-500 chars, e.g., `dano moral`) |
| `granularity` | `string` | `"year"` | Temporal granularity: `year` or `month` |
| `periodo_inicio` | `string` | `null` | Period start. Format must match granularity: `YYYY` for year, `YYYY-MM` for month. |
| `periodo_fim` | `string` | `null` | Period end. Same format requirement. |

:::caution
The period format must match the granularity. Using `YYYY-MM` with `granularity: "year"` returns a `400 INVALID_REQUEST` error.
:::

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/temporal-evolution \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "criterio": "boa-fe objetiva",
    "granularity": "year",
    "periodo_inicio": "2018",
    "periodo_fim": "2024"
  }'
```

## POST /v1/graph/citation-chain

Trace the outbound citation chain from a root decision through `CITA_PRECEDENTE` relationships up to the specified depth.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `decisao_id` | `string` | **required** | Root decision ID |
| `max_depth` | `integer` | `3` | Maximum citation hops (1-5) |

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/citation-chain \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{ "decisao_id": "doc-stj-resp-1234567", "max_depth": 3 }'
```

:::note
This endpoint traces outbound citations only (which decisions the root cites). It does not include inbound citations (who cites the root). The response includes a `max_depth_reached` flag.
:::

## POST /v1/graph/pagerank

Rank the most influential decisions in the citation graph using a simplified PageRank score: `in_citations * 10 + second_order * 3`.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `limit` | `integer` | `20` | Top-N most influential decisions (1-100) |
| `min_citations` | `integer` | `0` | Minimum direct citations post-filter (0 = no filter) |

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/pagerank \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{ "limit": 10, "min_citations": 5 }'
```

## POST /v1/graph/communities

Identify thematic communities of decisions that share legal criteria. Returns decision pairs grouped by shared criteria count (pairwise co-occurrence).

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `min_shared` | `integer` | `3` | Minimum shared criteria to form a community pair (1-20) |
| `limit` | `integer` | `20` | Maximum communities returned (1-100) |

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/communities \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{ "min_shared": 4, "limit": 10 }'
```

:::note
This is pairwise co-occurrence (size=2 per item), not full graph-theory community detection (e.g., Louvain). Each community entry represents a pair of decisions with their shared criteria.
:::

## POST /v1/graph/structural-similarity

Compare two decisions across five graph dimensions (criteria, facts, evidence, statutes, precedents) using weighted Jaccard scoring. Returns per-dimension similarity stats and a combined `weighted_score` in [0, 1].

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `source_id` | `string` | **required** | First decision ID |
| `target_id` | `string` | **required** | Second decision ID |

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/structural-similarity \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "source_id": "doc-stj-resp-1234567",
    "target_id": "doc-stj-resp-7654321"
  }'
```

## POST /v1/graph/shortest-path

Find the shortest bidirectional path between two decisions in the knowledge graph, using all relationship types.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `source_id` | `string` | **required** | Source decision ID |
| `target_id` | `string` | **required** | Target decision ID |
| `max_depth` | `integer` | `10` | Maximum path depth in hops (1-20) |

Returns nodes and edges with real relationship types, or `found: false` when no path exists within the depth limit.

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/shortest-path \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "source_id": "doc-stj-resp-1234567",
    "target_id": "doc-stj-resp-7654321",
    "max_depth": 5
  }'
```

## POST /v1/graph/embeddings

Compute 7-dimensional structural feature vectors for decisions based on graph topology: criteria count, facts count, evidence count, statutes count, inbound citations, outbound citations, and encoded outcome.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `decisao_ids` | `string[]` | `null` | Specific decision IDs. Omit for random sample mode. Max 500 IDs per request. |
| `limit` | `integer` | `100` | Sample size cap when `decisao_ids` is omitted (1-500) |

Results are cached in Redis for 1 hour.

### Example request

```bash
curl -X POST http://localhost:8000/v1/graph/embeddings \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "decisao_ids": ["doc-stj-resp-1234567", "doc-stj-resp-7654321"]
  }'
```

:::tip
Graph embeddings are useful for clustering, visualization (t-SNE/UMAP), and as features for downstream ML models. They represent structural properties in the knowledge graph, not semantic text embeddings.
:::

---
# docs/api/verify-enrich.md
---


# Verify & Enrich Endpoints

Three endpoints for validating legal references, enriching decisions with structured analysis, and extracting factual digests from legal text.

## POST /v1/verify

Anti-hallucination verification endpoint. Validates legal references in text (sumulas, minister names, process numbers, legislation mentions) against local reference data. Designed to verify LLM-generated legal arguments before presenting them to users.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `text` | `string` | **required** | Text containing legal references to verify (1-10000 chars) |
| `check_sumulas` | `boolean` | `true` | Validate sumula references against STJ/STF data |
| `check_ministros` | `boolean` | `true` | Validate minister names against reference list |
| `check_processos` | `boolean` | `true` | Validate CNJ process number format (does not confirm existence) |
| `check_legislacao` | `boolean` | `true` | Extract and classify legislation mentions (does not verify vigency) |

### Example request

```bash
curl -X POST http://localhost:8000/v1/verify \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Conforme Sumula 479 do STJ, relatada pela Ministra Nancy Andrighi no REsp 1.234.567/SP, com base no art. 14 do CDC...",
    "check_sumulas": true,
    "check_ministros": true,
    "check_processos": true,
    "check_legislacao": true
  }'
```

### Example response

```json
{
  "data": {
    "sumulas": {
      "Sumula 479": {
        "numero": 479,
        "tribunal": "STJ",
        "valid": true,
        "texto": "As instituicoes financeiras respondem objetivamente...",
        "situacao": "vigente",
        "vigente": true
      }
    },
    "ministros": {
      "Nancy Andrighi": {
        "nome": "NANCY ANDRIGHI",
        "valid": true,
        "confidence": 1.0,
        "is_aposentado": false
      }
    },
    "processos": {
      "1.234.567/SP": {
        "numero": "1234567",
        "format_valid": true
      }
    },
    "legislacao": [
      {
        "referencia": "art. 14 do CDC",
        "tipo": "artigo",
        "lei_numero": "CDC",
        "artigo": "14",
        "alias_resolucao": null
      }
    ],
    "metrics": {
      "risk_level": "low",
      "risk_score": 0.05,
      "total_citations": 3,
      "valid_count": 3,
      "invalid_count": 0
    },
    "verified_at": "2026-02-28T14:30:00Z"
  },
  "meta": {
    "trace_id": "d4e5f6a7-b8c9-0123-def0-123456789abc",
    "latency_ms": 12.5
  }
}
```

### Verification details

| Reference type | Validation method | Limitations |
|---|---|---|
| Sumulas | Matched against local STJ/STF sumula database | Checks number + tribunal, returns texto and vigency status |
| Ministers | Matched against local reference list with confidence score | Includes retirement status (`is_aposentado`) |
| Processes | CNJ format validation via regex | Format-only -- does not confirm existence in external tribunal systems |
| Legislation | Regex extraction and classification (artigo, lei, decreto, etc.) | Extracted and classified, but **not** verified for legal existence or vigency |

### Risk metrics

The `metrics` object provides an aggregate hallucination risk assessment:

- `risk_level`: `low`, `medium`, or `high`
- `risk_score`: Numeric score (0.0 = no risk, 1.0 = maximum risk)
- `total_citations` / `valid_count` / `invalid_count`: Summary counts

## POST /v1/context/enrich

Enrich a legal document with IRAC analysis (Issue, Rule, Application, Conclusion) and knowledge graph context including criteria, legal statutes, precedents, legislation, and related decisions.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `document_id` | `string` | **required** | Document ID to enrich (obtain via search endpoints first) |

### Example request

```bash
curl -X POST http://localhost:8000/v1/context/enrich \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{ "document_id": "doc-stj-resp-1234567" }'
```

### Example response

```json
{
  "data": {
    "document_id": "doc-stj-resp-1234567",
    "irac": {
      "labels": [
        { "section": "issue", "text": "Se o atraso de voo gera dano moral presumido", "confidence": 0.85 },
        { "section": "rule", "text": "CDC art. 14 - responsabilidade objetiva do fornecedor", "confidence": 0.90 },
        { "section": "application", "text": "O atraso superior a 4 horas configura falha na prestacao...", "confidence": 0.78 },
        { "section": "conclusion", "text": "Recurso especial provido para fixar indenizacao...", "confidence": 0.92 }
      ],
      "model_version": "heuristic-v1.0"
    },
    "criterios": [
      { "name": "dano moral", "peso": "high" },
      { "name": "responsabilidade objetiva", "peso": "medium" }
    ],
    "dispositivos": [
      { "id": "disp-cdc-14", "name": "CDC art. 14" }
    ],
    "precedentes": [
      { "id": "doc-stj-resp-111222", "name": "REsp 111.222/SP" }
    ],
    "legislacao": [
      {
        "source": "doc-stj-resp-1234567",
        "target": "CDC",
        "type": "APLICA_LEI",
        "attributes": { "artigo": "14" }
      }
    ],
    "related_decisions": [
      {
        "decisao_id": "doc-stj-resp-333444",
        "processo": "REsp 333.444/MG",
        "shared_criterios": ["dano moral", "responsabilidade objetiva"]
      }
    ],
    "kg_available": true
  },
  "meta": {
    "trace_id": "e5f6a7b8-c9d0-1234-ef01-23456789abcd",
    "latency_ms": 85.2
  }
}
```

:::note
The IRAC analysis uses a regex-based heuristic classifier, not an LLM. The `model_version` field indicates the classification method. Knowledge graph context (`criterios`, `dispositivos`, `precedentes`, `legislacao`, `related_decisions`) comes from Neo4j and is only populated when `kg_available` is `true`.
:::

## POST /v1/factual/extract

Extract a structured factual digest and legal thesis from legal text using Groq LLM. Produces dense, comparable representations suitable for dual-vector search.

### Parameters

| Parameter | Type | Default | Description |
|---|---|---|---|
| `text` | `string` | `null` | Legal text for extraction (50-15000 chars). Required if `document_id` is omitted. |
| `document_id` | `string` | `null` | Corpus document ID. Required if `text` is omitted. |

:::caution
Requires `VALTER_GROQ_API_KEY` and `VALTER_GROQ_ENABLED=true` in the environment. Without these, the endpoint returns `503 SERVICE_UNAVAILABLE`.
:::

### Example request

```bash
curl -X POST http://localhost:8000/v1/factual/extract \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{ "document_id": "doc-stj-resp-1234567" }'
```

### Example response

```json
{
  "data": {
    "factual_digest": {
      "bullets": [
        { "index": 0, "text": "Consumidor adquiriu passagem para voo com horario determinado", "source_excerpt": "...", "uncertainty": false },
        { "index": 1, "text": "Voo atrasou mais de 8 horas sem assistencia", "source_excerpt": "...", "uncertainty": false }
      ],
      "digest_text": "Consumidor adquiriu passagem para voo. Voo atrasou mais de 8 horas sem assistencia da companhia aerea.",
      "extraction_model": "llama-3.3-70b-versatile"
    },
    "thesis_digest": {
      "thesis_text": "O atraso significativo de voo internacional gera dano moral presumido independente de prova especifica",
      "legal_basis": ["CDC art. 14", "Convenção de Montreal art. 19"],
      "precedents_cited": ["REsp 1.584.465/SC"],
      "extraction_model": "llama-3.3-70b-versatile"
    },
    "source_text_preview": "RECURSO ESPECIAL. TRANSPORTE AEREO INTERNACIONAL..."
  },
  "meta": {
    "trace_id": "f6a7b8c9-d0e1-2345-f012-3456789abcde",
    "latency_ms": 2340.1
  }
}
```

The factual digest and thesis digest are produced independently and can be used as inputs for [dual-vector search](./search/#post-v1factualdual-search).

---
# docs/api/ingest.md
---


# Ingest Endpoints

Endpoints under `/v1/ingest/` for the full case analysis workflow: from PDF upload through automated phase analysis to human-reviewed legal analysis. Many endpoints have dual paths (`/workflow/...` and `/processo/full-analysis/...`) for backward compatibility.

## Workflow Lifecycle

### POST /v1/ingest/workflow

Start a full case analysis workflow. Upload a PDF file for asynchronous processing. This is the primary entry point for new case analysis.

Alias: `POST /v1/ingest/processo/full-analysis` (legacy, same handler).

**Content-Type:** `multipart/form-data`

| Parameter | Type | Default | Description |
|---|---|---|---|
| `file` | `UploadFile` | **required** | PDF file of the legal case |
| `source_system` | `string` | `"projudi"` | Upstream source system label |
| `rules_version` | `string` | `null` | Ruleset version override |
| `min_precedent_score` | `number` | `null` | Minimum precedent score threshold (0-100) |
| `max_matches_per_phase` | `integer` | `null` | Cap on precedent matches per phase (1-10) |
| `reason` | `string` | `null` | Operator note for audit traceability |
| `strict_infra_required` | `boolean` | `true` | Fail if required infra dependencies are unavailable |

**Returns:** `202 Accepted` with `workflow_id` and initial status.

```bash
curl -X POST http://localhost:8000/v1/ingest/workflow \
  -H "Authorization: Bearer $API_KEY" \
  -F "file=@caso-12345.pdf" \
  -F "source_system=projudi"
```

:::note
Maximum upload size is controlled by `VALTER_MAX_UPLOAD_MB` (default: 100 MB).
:::

### GET /v1/ingest/workflow/{workflow_id}

Check current state and progress of a workflow.

Alias: `GET /v1/ingest/processo/full-analysis/{workflow_id}`

| Parameter | Type | Description |
|---|---|---|
| `workflow_id` | `string` (path) | Workflow ID returned by the creation endpoint |

Returns the current state, progress details, phases, and any errors.

```bash
curl http://localhost:8000/v1/ingest/workflow/wf-abc123 \
  -H "Authorization: Bearer $API_KEY"
```

### GET /v1/ingest/workflow/{workflow_id}/result

Retrieve the consolidated result of a completed workflow.

Alias: `GET /v1/ingest/processo/full-analysis/{workflow_id}/result`

Returns the full analysis result when the workflow is complete, or a not-ready/error payload while still running.

```bash
curl http://localhost:8000/v1/ingest/workflow/wf-abc123/result \
  -H "Authorization: Bearer $API_KEY"
```

## Legacy Process Extraction

### POST /v1/ingest/processo

Start a process extraction workflow (legacy pipeline, predates the full-analysis workflow).

**Returns:** `202 Accepted` with `extraction_id`.

### GET /v1/ingest/processo/{extraction_id}

Check status and result of a legacy process extraction.

### POST /v1/ingest/processo/{extraction_id}/validate

Validate (approve/reject) a legacy extraction with an optional reason.

## PDF Conversion

### POST /v1/ingest/pdf-to-markdown

Convert a full legal process PDF to relevant Markdown. Standalone utility that does not create a workflow.

| Parameter | Type | Description |
|---|---|---|
| `file` | `UploadFile` | PDF file to convert |

Returns converted Markdown with highlighted relevant sections.

## Phase Analysis

### POST /v1/ingest/processo/{extraction_id}/phase-analysis

Run phase analysis on an extraction. Identifies procedural phases in the legal case and matches each to relevant jurisprudence.

**Returns:** `202 Accepted` with `analysis_id`.

### GET /v1/ingest/processo/{extraction_id}/phase-analysis/{analysis_id}

Check status and results of a phase analysis.

### POST /v1/ingest/processo/{extraction_id}/phase-analysis/reprocess

Reprocess phase analysis for an extraction. Creates a new immutable analysis run without modifying the previous one.

**Returns:** `202 Accepted` with new `analysis_id`.

## Human Review

### POST /v1/ingest/processo/{extraction_id}/phase-analysis/{analysis_id}/review

Submit human review for phases in a legacy phase analysis. Approve or reject individual phases with reviewer notes.

| Parameter | Type | Description |
|---|---|---|
| `phase_label` | `string` | Phase identifier to review |
| `approved` | `boolean` | Approval decision |
| `reviewer` | `string` | Reviewer identity for audit trail |
| `notes` | `string` | Review notes |

### POST /v1/ingest/workflow/{workflow_id}/review

Submit human review for a full-analysis workflow. Supports per-phase and final review.

Alias: `POST /v1/ingest/processo/full-analysis/{workflow_id}/review`

| Parameter | Type | Description |
|---|---|---|
| `phase_label` | `string` | Phase to review (omit for final review) |
| `approved` | `boolean` | Review decision |
| `reviewer` | `string` | Reviewer identity |
| `notes` | `string` | Review notes |

## Reprocessing

### POST /v1/ingest/workflow/{workflow_id}/reprocess

Start a new immutable execution for an existing workflow. Does not mutate prior executions.

Alias: `POST /v1/ingest/processo/full-analysis/{workflow_id}/reprocess`

**Returns:** `202 Accepted` with new workflow run information.

| Parameter | Type | Default | Description |
|---|---|---|---|
| `rules_version` | `string` | `null` | Ruleset version override for the new run |
| `min_precedent_score` | `number` | `null` | Minimum precedent score threshold |
| `max_matches_per_phase` | `integer` | `null` | Cap on precedent matches per phase |
| `reason` | `string` | `null` | Operator reason for audit trail |
| `strict_infra_required` | `boolean` | `null` | Override strict infrastructure requirement |

## Audit & Events

### GET /v1/ingest/workflow/{workflow_id}/events

List auditable events for a workflow. Returns timestamped events for all state transitions, phase completions, errors, and review actions.

```bash
curl http://localhost:8000/v1/ingest/workflow/wf-abc123/events \
  -H "Authorization: Bearer $API_KEY"
```

### GET /v1/ingest/workflow/{workflow_id}/interactions

List the domain interaction trail for a workflow. Includes human reviews, system decisions, and operator notes.

```bash
curl http://localhost:8000/v1/ingest/workflow/wf-abc123/interactions \
  -H "Authorization: Bearer $API_KEY"
```

## Artifacts

### GET /v1/ingest/workflow/{workflow_id}/artifacts

List all versioned artifacts generated by a workflow (PDFs, JSONs, Markdown reports, logs).

```bash
curl http://localhost:8000/v1/ingest/workflow/wf-abc123/artifacts \
  -H "Authorization: Bearer $API_KEY"
```

### POST /v1/ingest/workflow/{workflow_id}/artifacts/{artifact_id}/signed-url

Generate a time-limited signed URL for downloading a specific workflow artifact.

| Parameter | Type | Description |
|---|---|---|
| `workflow_id` | `string` (path) | Workflow ID |
| `artifact_id` | `string` (path) | Artifact identifier from the artifacts list |

Returns a signed URL with a TTL controlled by `VALTER_R2_PRESIGN_TTL_SECONDS` (default: 600 seconds / 10 minutes).

```bash
curl -X POST http://localhost:8000/v1/ingest/workflow/wf-abc123/artifacts/art-456/signed-url \
  -H "Authorization: Bearer $API_KEY"
```

```json
{
  "data": {
    "signed_url": "https://storage.example.com/artifacts/art-456?X-Amz-Signature=...",
    "expires_in_seconds": 600
  },
  "meta": {
    "trace_id": "a1b2c3d4-...",
    "latency_ms": 45.2
  }
}
```

## Endpoint Summary

| Method | Path | Description | Status |
|---|---|---|---|
| `POST` | `/v1/ingest/workflow` | Start full analysis workflow | 202 |
| `GET` | `/v1/ingest/workflow/{id}` | Get workflow status | 200 |
| `GET` | `/v1/ingest/workflow/{id}/result` | Get workflow result | 200 |
| `POST` | `/v1/ingest/workflow/{id}/review` | Submit human review | 200 |
| `POST` | `/v1/ingest/workflow/{id}/reprocess` | Reprocess workflow | 202 |
| `GET` | `/v1/ingest/workflow/{id}/events` | List audit events | 200 |
| `GET` | `/v1/ingest/workflow/{id}/artifacts` | List artifacts | 200 |
| `GET` | `/v1/ingest/workflow/{id}/interactions` | List interactions | 200 |
| `POST` | `/v1/ingest/workflow/{id}/artifacts/{aid}/signed-url` | Get signed download URL | 200 |
| `POST` | `/v1/ingest/pdf-to-markdown` | Convert PDF to Markdown | 200 |
| `POST` | `/v1/ingest/processo` | Start legacy extraction | 202 |
| `GET` | `/v1/ingest/processo/{id}` | Get extraction status | 200 |
| `POST` | `/v1/ingest/processo/{id}/validate` | Validate extraction | 200 |
| `POST` | `/v1/ingest/processo/{id}/phase-analysis` | Start phase analysis | 202 |
| `GET` | `/v1/ingest/processo/{id}/phase-analysis/{aid}` | Get phase analysis status | 200 |
| `POST` | `/v1/ingest/processo/{id}/phase-analysis/{aid}/review` | Review phase analysis | 200 |
| `POST` | `/v1/ingest/processo/{id}/phase-analysis/reprocess` | Reprocess phase analysis | 202 |

:::tip
The `/v1/ingest/processo/full-analysis/*` paths are aliases for `/v1/ingest/workflow/*` and share the same handlers. New integrations should use the `/v1/ingest/workflow/*` paths.
:::

---
# docs/api/admin.md
---


# Admin & Utility Endpoints

System health, Prometheus metrics, session memory, and dataset management endpoints.

## GET /v1/health

Comprehensive health check of all backend services. No authentication required.

Checks connectivity and latency for each service with a 5-second timeout per check:

| Service | Description |
|---|---|
| `qdrant` | Vector search engine |
| `neo4j` | Knowledge graph database |
| `postgres` | Primary relational database |
| `redis` | Cache and rate limiting |
| `artifact_storage` | File storage (R2 or local) |
| `worker_ingest` | ARQ background worker heartbeat (stored in Redis) |

### Overall status

The `status` field aggregates individual service health:

| Status | Meaning |
|---|---|
| `healthy` | All services operational |
| `degraded` | One or more services down, but system partially functional |
| `unhealthy` | All services down |

### Example request

```bash
curl http://localhost:8000/v1/health
```

### Example response

```json
{
  "status": "healthy",
  "version": "0.12.0",
  "stores": [
    { "name": "qdrant", "status": "up", "latency_ms": 3.2 },
    { "name": "neo4j", "status": "up", "latency_ms": 15.8 },
    { "name": "postgres", "status": "up", "latency_ms": 2.1 },
    { "name": "redis", "status": "up", "latency_ms": 0.8 },
    { "name": "artifact_storage", "status": "up", "latency_ms": 45.3 },
    { "name": "worker_ingest", "status": "up", "latency_ms": 1.2 }
  ],
  "uptime_seconds": 86423.15
}
```

:::note
The health endpoint also exports each service status to Prometheus via the `store_health` gauge, enabling alerting on degraded services.
:::

## GET /metrics

Prometheus-format metrics endpoint. Access is restricted by IP allowlist.

### Access control

The endpoint is protected by the `MetricsIPAllowlistMiddleware`:

- Allowed IPs are configured via `VALTER_METRICS_IP_ALLOWLIST` (CIDR notation supported, e.g., `10.0.0.0/8`).
- IP resolution respects `cf-connecting-ip` (Cloudflare) and `x-forwarded-for` (generic proxies).
- If the allowlist is empty, the endpoint returns `403 Forbidden`.
- Requests from non-allowed IPs return `403 Forbidden`.

### Exported metrics

The Prometheus endpoint exports counters, histograms, and gauges including:

- `valter_request_count` -- Total HTTP request count by method, path, status
- `valter_request_duration_seconds` -- Request latency histogram
- `valter_store_health` -- Per-service health gauge (1 = up, 0 = down)
- `valter_rate_limit_blocks_total` -- Rate limit block events
- `valter_rate_limit_redis_errors_total` -- Redis errors during rate limiting

```bash
curl http://localhost:8000/metrics
```

## Session Memory

Session-scoped key-value storage backed by PostgreSQL with configurable TTL. Used by MCP tools to maintain conversation context across tool calls.

### POST /v1/memories

Store a key-value pair with optional TTL for session context.

| Parameter | Type | Default | Description |
|---|---|---|---|
| `session_id` | `string` | **required** | Session identifier to scope memory entries |
| `key` | `string` | **required** | Memory key (upsert semantics by session_id + key) |
| `value` | `string` | **required** | Memory value payload |
| `ttl_seconds` | `integer` | `86400` | Time-to-live in seconds (60 to 2,592,000 = 30 days) |

### Example request

```bash
curl -X POST http://localhost:8000/v1/memories \
  -H "Authorization: Bearer $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "session-abc123",
    "key": "caso_atual",
    "value": "REsp 1.234.567/SP - dano moral por atraso de voo",
    "ttl_seconds": 3600
  }'
```

### Example response

```json
{
  "data": {
    "id": "mem-d4e5f6a7-b8c9-0123-def0-123456789abc",
    "session_id": "session-abc123",
    "key": "caso_atual",
    "value": "REsp 1.234.567/SP - dano moral por atraso de voo"
  },
  "meta": {
    "trace_id": "a1b2c3d4-...",
    "latency_ms": 5.3
  }
}
```

### GET /v1/memories

List all memories for a session.

| Parameter | Type | Description |
|---|---|---|
| `session_id` | `string` (query) | Session identifier (**required**) |

### Example request

```bash
curl "http://localhost:8000/v1/memories?session_id=session-abc123" \
  -H "Authorization: Bearer $API_KEY"
```

### Example response

```json
{
  "data": [
    {
      "id": "mem-d4e5f6a7-...",
      "session_id": "session-abc123",
      "key": "caso_atual",
      "value": "REsp 1.234.567/SP - dano moral por atraso de voo",
      "created_at": "2026-02-28T14:30:00+00:00",
      "expires_at": "2026-02-28T15:30:00+00:00"
    }
  ],
  "meta": {
    "trace_id": "b2c3d4e5-...",
    "latency_ms": 3.1
  }
}
```

:::tip
Memories use upsert semantics: storing a value with an existing `session_id` + `key` pair replaces the previous value. Expired memories are automatically excluded from list results.
:::

## Dataset Management

### GET /v1/datasets/uploads/{dataset_item_id}

Retrieve metadata for a previously uploaded dataset item. Returns storage details, file metadata, and provenance information.

| Parameter | Type | Description |
|---|---|---|
| `dataset_item_id` | `string` (path) | Dataset item identifier |

### Example request

```bash
curl http://localhost:8000/v1/datasets/uploads/ds-item-789 \
  -H "Authorization: Bearer $API_KEY"
```

### Example response

```json
{
  "data": {
    "id": "ds-item-789",
    "sha256": "a1b2c3d4e5f6...",
    "original_filename": "processo-12345.pdf",
    "size_bytes": 2048576,
    "storage_path": "/data/uploads/processo-12345.pdf",
    "storage_backend": "r2",
    "bucket": "valter-artifacts",
    "object_key": "uploads/ds-item-789/processo-12345.pdf",
    "storage_uri": "r2://valter-artifacts/uploads/ds-item-789/processo-12345.pdf",
    "source_system": "projudi",
    "extraction_id": "ext-abc123",
    "included_policy": "full",
    "created_at": "2026-02-28T10:00:00+00:00"
  },
  "meta": {
    "trace_id": "c3d4e5f6-...",
    "latency_ms": 8.4
  }
}
```

---
# docs/api/mcp-tools.md
---


# MCP Tools Reference

Complete reference for all 28 MCP tools exposed by Valter's Model Context Protocol server.

## Overview

Valter registers 28 tools in its MCP server, available via two transport modes:

| Transport | Client | Protocol |
|---|---|---|
| **stdio** | Claude Desktop, Claude Code | JSON-RPC over stdin/stdout |
| **HTTP/SSE** | ChatGPT (via MCP remote), custom clients | HTTP POST with Server-Sent Events |

Each tool has a name, description, JSON Schema input definition, and an async handler. Tools are rate-limited by `VALTER_MCP_RATE_LIMIT_PER_MINUTE` (default: 60).

Authentication depends on the transport: API key for stdio mode, HMAC for HTTP mode.

## Search Tools

### search_jurisprudence

Free-text jurisprudence retrieval over STJ decisions using hybrid BM25 + semantic search. Start from this tool when you need candidate cases before calling other tools.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `query` | `string` | yes | -- | Natural-language legal query in Portuguese |
| `top_k` | `integer` | no | `10` | Number of results (1-100) |
| `strategy` | `string` | no | `"weighted"` | Scoring: `weighted`, `rrf`, `bm25`, `semantic` |
| `include_kg` | `boolean` | no | `false` | Apply knowledge graph relevance boost |
| `rerank` | `boolean` | no | `false` | Apply cross-encoder reranking |
| `expand_query` | `boolean` | no | `true` | Expand query with LLM-generated legal variants (+500-1500ms) |
| `ministro` | `string` | no | -- | Minister name filter (normalized to uppercase, post-retrieval) |
| `data_inicio` | `string` | no | -- | Start date filter (YYYYMMDD, post-retrieval) |
| `data_fim` | `string` | no | -- | End date filter (YYYYMMDD, post-retrieval) |
| `include_stj_metadata` | `boolean` | no | `false` | Include STJ metadata via extra PostgreSQL lookup |
| `page_size` | `integer` | no | -- | Page size for cursor pagination (1-50, <= top_k) |
| `cursor` | `string` | no | -- | Continuation cursor from previous page |

**Returns:** Ranked decisions with scores, total_found, latency, cache status.

**Maps to:** `POST /v1/retrieve`

### verify_legal_claims

Validate legal references in text against local reference data. Uses regex extraction plus in-memory reference indices.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `text` | `string` | yes | -- | Text containing legal references to verify |
| `check_sumulas` | `boolean` | no | `true` | Validate sumulas against STJ/STF reference data |
| `check_ministros` | `boolean` | no | `true` | Validate minister names |
| `check_processos` | `boolean` | no | `true` | Validate CNJ process number format only |
| `check_legislacao` | `boolean` | no | `true` | Extract/classify legislation mentions |

**Returns:** Per-reference verification results and hallucination risk metrics.

**Maps to:** `POST /v1/verify`

### get_irac_analysis

Run heuristic IRAC analysis (regex-based) and load knowledge graph context for a legal document.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `document_id` | `string` | yes | Document ID (obtain via `search_jurisprudence` first) |

**Returns:** IRAC labels (Issue, Rule, Application, Conclusion) plus KG entity counts.

**Maps to:** `POST /v1/context/enrich`

### find_similar_cases

Find cases similar to a given decision using 70% semantic + 30% structural KG overlap.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `document_id` | `string` | yes | -- | Source document ID |
| `top_k` | `integer` | no | `10` | Number of similar cases (1-100) |
| `include_structural` | `boolean` | no | `true` | Include KG structural similarity |

**Returns:** Ranked similar cases with scores. On timeout with structural mode, retries semantic-only.

**Maps to:** `POST /v1/similar_cases`

### get_document_integra

Retrieve the full text (inteiro teor) of a specific STJ decision.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `document_id` | `string` | yes | Document ID (obtain via search first) |

**Returns:** ementa, tese, razoes_decidir, and texto_completo (when available). Check `has_integra` in search results before calling.

### search_features

Structured search over AI-extracted document features with AND-combined filters. `categorias` uses OR/ANY semantics; most scalar filters are exact case-sensitive; `argumento_vencedor`/`argumento_perdedor` use partial case-insensitive matching.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `categorias` | `string[]` | no | -- | Category filter (OR semantics) |
| `dispositivo_norma` | `string` | no | -- | Legal statute filter |
| `resultado` | `string` | no | -- | Outcome filter (exact) |
| `unanimidade` | `boolean` | no | -- | Unanimous decision filter |
| `tipo_decisao` | `string` | no | -- | Decision type (exact) |
| `tipo_recurso` | `string` | no | -- | Appeal type (exact) |
| `ministro_relator` | `string` | no | -- | Reporting minister (exact) |
| `argumento_vencedor` | `string` | no | -- | Winning argument text (ILIKE) |
| `argumento_perdedor` | `string` | no | -- | Losing argument text (ILIKE) |
| `limit` | `integer` | no | `20` | Max results (1-100) |
| `offset` | `integer` | no | `0` | Pagination offset |

**Returns:** Full features plus document summary fields.

**Maps to:** `POST /v1/search/features`

## Graph Tools

### get_divergencias

Find legal criteria with split outcomes, compute `divergence_score = minority / total`, and rank clusters.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `categoria_id` | `string` | no | -- | Exact category ID filter |
| `limit` | `integer` | no | `10` | Max clusters (1-50) |

**Returns:** Divergence clusters ranked by score (balanced splits score highest).

### get_turma_divergences

Analyze split outcomes for criteria matching a legal topic, aggregated by minister.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `tema` | `string` | yes | Topic substring match on criterion names (case-insensitive) |

**Returns:** Per-minister outcome counts for matching criteria.

### get_optimal_argument

Compute argument success rates (criteria, statutes, precedents) for a category and desired outcome.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `categoria_id` | `string` | yes | -- | Legal category ID |
| `resultado_desejado` | `string` | no | `"provido"` | Target outcome |
| `tipo_argumento` | `string` | no | `"all"` | Filter: `criterio`, `dispositivo`, `precedente`, `all` |
| `min_decisions` | `integer` | no | `2` | Minimum supporting decisions (floor: 2) |
| `top_k` | `integer` | no | `10` | Max arguments returned (1-50, effective max ~11) |

**Returns:** Argument chain with success rates per argument type.

### get_optimal_argument_by_ministro

Compare minister-specific success rates vs category averages.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `categoria_id` | `string` | yes | -- | Legal category ID |
| `ministro` | `string` | yes | -- | Minister name (auto-uppercase) |
| `resultado_desejado` | `string` | no | `"provido"` | Target outcome |
| `tipo_argumento` | `string` | no | `"all"` | Argument type filter |
| `min_decisions` | `integer` | no | `1` | Min minister-side decisions |
| `min_category_decisions` | `integer` | no | `2` | Min category support (floor: 2) |
| `top_k` | `integer` | no | `10` | Max arguments (1-50, effective max ~20) |

**Returns:** Delta per argument, recommended_arguments (delta > 0), avoid_arguments (delta < -0.1).

### get_ministro_profile

Load a minister's judicial behavior profile from the knowledge graph.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `ministro` | `string` | yes | -- | Minister name (auto-uppercase) |
| `include_divergencias` | `boolean` | no | `true` | Include peer divergences |
| `include_precedentes` | `boolean` | no | `true` | Include most-cited decisions |
| `limit_criterios` | `integer` | no | `10` | Criteria cap in response (store cap: 10) |

**Returns:** Total decisions, date range, top criteria, outcome distribution, peer divergences, most-cited decisions.

### get_temporal_evolution

Aggregate jurisprudence counts over time for a legal criterion.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `criterio` | `string` | yes | -- | Legal criterion to analyze |
| `granularity` | `string` | no | `"year"` | `year` or `month` |
| `periodo_inicio` | `string` | no | -- | Period start (`YYYY` or `YYYY-MM`) |
| `periodo_fim` | `string` | no | -- | Period end (`YYYY` or `YYYY-MM`) |

**Returns:** Per-period buckets with provido/improvido split and heuristic trend label.

### get_citation_chain

Trace outbound citation edges from a root decision through CITA_PRECEDENTE hops.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `decisao_id` | `string` | yes | -- | Root decision ID |
| `max_depth` | `integer` | no | `3` | Maximum citation hops (1-5) |

**Returns:** Citation nodes/edges and `max_depth_reached` flag. Does not include inbound citations.

### get_pagerank

Rank influential decisions using simplified PageRank: `in_citations * 10 + second_order * 3`.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `limit` | `integer` | no | `20` | Top-N results (1-100) |
| `min_citations` | `integer` | no | `0` | Minimum direct citations filter (post-processing) |

**Returns:** Ranked decisions with influence score and citation counts.

### get_communities

Return high-overlap decision pairs based on shared legal criteria. Pairwise co-occurrence, not full community detection.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `min_shared` | `integer` | no | `3` | Minimum shared criteria per pair |
| `limit` | `integer` | no | `20` | Max communities (1-100) |

**Returns:** Decision pairs with shared criteria count and names.

### get_structural_similarity

Compare two decisions across five graph dimensions using weighted Jaccard scoring.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `source_id` | `string` | yes | First decision ID |
| `target_id` | `string` | yes | Second decision ID |

**Returns:** Per-dimension stats (criteria, facts, evidence, statutes, precedents) and `weighted_score` in [0, 1].

### get_shortest_path

Find a bidirectional shortest path between two decisions using all relationship types.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `source_id` | `string` | yes | -- | Source decision ID |
| `target_id` | `string` | yes | -- | Target decision ID |
| `max_depth` | `integer` | no | `10` | Max path depth (1-20) |

**Returns:** Path nodes and edges with real relationship types, or `found: false`.

### get_graph_embeddings

Compute 7D structural vectors per decision (criteria/facts/evidence/statutes counts, in/out citations, encoded outcome).

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `decisao_ids` | `string[]` | no | -- | Specific decision IDs (max 500) |
| `limit` | `integer` | no | `100` | Sample size when `decisao_ids` is omitted (1-500) |

**Returns:** Structural embedding vectors per decision. Cached for 1 hour.

## Workflow Tools

### submit_case_pdf_analysis

Start an asynchronous PDF analysis workflow via the Valter HTTP API bridge.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `filename` | `string` | no | `"processo.pdf"` | Filename override |
| `pdf_base64` | `string` | no | -- | PDF in base64 (JSON mode, small files) |
| `local_path` | `string` | no | -- | File path for multipart upload (recommended) |
| `source_system` | `string` | no | `"projudi"` | Source system label |
| `source_mode` | `string` | no | `"chat_attachment"` | Input provenance label |
| `rules_version` | `string` | no | -- | Ruleset version override |
| `min_precedent_score` | `number` | no | -- | Min precedent score (0-100) |
| `max_matches_per_phase` | `integer` | no | -- | Cap per phase (1-10) |
| `reason` | `string` | no | -- | Operator note for audit |
| `strict_infra_required` | `boolean` | no | `true` | Fail on missing infra |

**Returns:** `workflow_id` and initial status for polling.

:::note
Requires `MCP_API_BASE_URL` to be reachable. Provide either `local_path` (multipart, recommended) or `pdf_base64` (JSON mode, better for small payloads).
:::

### get_case_pdf_analysis_status

Poll workflow status for a previously submitted PDF analysis.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `workflow_id` | `string` | yes | Workflow ID from `submit_case_pdf_analysis` |

**Returns:** Current state, progress, and any errors.

### get_case_pdf_analysis_result

Fetch the consolidated result of a completed PDF analysis workflow.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `workflow_id` | `string` | yes | Workflow ID from `submit_case_pdf_analysis` |

**Returns:** Full analysis result, or not-ready/error payload while still running.

### review_case_phase

Submit human approval/rejection for a specific workflow phase.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `workflow_id` | `string` | yes | Workflow ID |
| `phase_label` | `string` | yes | Phase identifier to review |
| `approved` | `boolean` | yes | Approval decision |
| `reviewer` | `string` | no | Reviewer identity for audit |
| `notes` | `string` | no | Review notes |

### review_case_final

Submit final human approval/rejection for the workflow outcome.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `workflow_id` | `string` | yes | Workflow ID |
| `approved` | `boolean` | yes | Final approval decision |
| `reviewer` | `string` | no | Reviewer identity for audit |
| `notes` | `string` | no | Final review notes |

### reprocess_case_analysis

Start a new immutable execution for an existing workflow. Does not mutate prior runs.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `workflow_id` | `string` | yes | Existing workflow ID |
| `rules_version` | `string` | no | Ruleset version override |
| `min_precedent_score` | `number` | no | Min precedent score (0-100) |
| `max_matches_per_phase` | `integer` | no | Cap per phase (1-10) |
| `reason` | `string` | no | Reason for audit trail |
| `strict_infra_required` | `boolean` | no | Override strict infra requirement |

### get_case_workflow_artifacts

List versioned workflow artifacts (PDF, JSON, Markdown, logs).

| Parameter | Type | Required | Description |
|---|---|---|---|
| `workflow_id` | `string` | yes | Workflow ID |

**Returns:** List of artifacts with IDs, types, and metadata.

### get_case_artifact_signed_url

Generate a temporary signed download URL for one workflow artifact.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `workflow_id` | `string` | yes | Workflow ID |
| `artifact_id` | `string` | yes | Artifact ID from `get_case_workflow_artifacts` |

**Returns:** Signed URL with time-limited access.

## Memory Tools

### remember

Store or update a session-scoped key-value memory in PostgreSQL with TTL.

| Parameter | Type | Required | Default | Description |
|---|---|---|---|---|
| `session_id` | `string` | yes | -- | Session identifier |
| `key` | `string` | yes | -- | Memory key (upsert by session_id + key) |
| `value` | `string` | yes | -- | Memory value payload |
| `ttl_seconds` | `integer` | no | `86400` | TTL in seconds (60 to 2,592,000) |

### recall

Retrieve a session-scoped memory by key.

| Parameter | Type | Required | Description |
|---|---|---|---|
| `session_id` | `string` | yes | Session identifier |
| `key` | `string` | yes | Memory key to retrieve |

**Returns:** `found: true` with the stored value, or `found: false` with `value: null` when the key is missing or expired.

## Tool Invocation

### stdio mode (Claude Desktop / Claude Code)

1. Client sends a `tool_use` message with tool name and parameters.
2. Valter's MCP server processes the request via the registered handler.
3. Server returns a `tool_result` with the response payload.

### HTTP/SSE mode (ChatGPT / remote clients)

1. Client calls the MCP remote endpoint with tool name and parameters.
2. Valter bridges the request via httpx to the REST API.
3. REST API response is formatted and returned as MCP tool result.

### Error handling

All tools return structured error responses with `trace_id` for log correlation. Common error conditions:

| Error | Description |
|---|---|
| Backend timeout | Neo4j, Qdrant, or PostgreSQL query exceeds time limit |
| Service unavailable | Backend service unreachable |
| Validation error | Invalid or missing required parameters |
| Not found | Referenced document or workflow does not exist |

## Tool Summary

| Domain | Tool | Required params |
|---|---|---|
| Search | `search_jurisprudence` | `query` |
| Search | `verify_legal_claims` | `text` |
| Search | `get_irac_analysis` | `document_id` |
| Search | `find_similar_cases` | `document_id` |
| Search | `get_document_integra` | `document_id` |
| Search | `search_features` | (at least one filter) |
| Graph | `get_divergencias` | -- |
| Graph | `get_turma_divergences` | `tema` |
| Graph | `get_optimal_argument` | `categoria_id` |
| Graph | `get_optimal_argument_by_ministro` | `categoria_id`, `ministro` |
| Graph | `get_ministro_profile` | `ministro` |
| Graph | `get_temporal_evolution` | `criterio` |
| Graph | `get_citation_chain` | `decisao_id` |
| Graph | `get_pagerank` | -- |
| Graph | `get_communities` | -- |
| Graph | `get_structural_similarity` | `source_id`, `target_id` |
| Graph | `get_shortest_path` | `source_id`, `target_id` |
| Graph | `get_graph_embeddings` | -- |
| Workflow | `submit_case_pdf_analysis` | -- |
| Workflow | `get_case_pdf_analysis_status` | `workflow_id` |
| Workflow | `get_case_pdf_analysis_result` | `workflow_id` |
| Workflow | `review_case_phase` | `workflow_id`, `phase_label`, `approved` |
| Workflow | `review_case_final` | `workflow_id`, `approved` |
| Workflow | `reprocess_case_analysis` | `workflow_id` |
| Workflow | `get_case_workflow_artifacts` | `workflow_id` |
| Workflow | `get_case_artifact_signed_url` | `workflow_id`, `artifact_id` |
| Memory | `remember` | `session_id`, `key`, `value` |
| Memory | `recall` | `session_id`, `key` |

---
# docs/roadmap/index.md
---


# Roadmap

Valter's evolution from legal search backend to legal reasoning engine — strategic vision, competitive positioning, and risk-informed planning.

## Product Vision

Valter will be the **legal reasoning engine** for LLMs and lawyers. The core differentiator is not search — it is the ability to **compose verified legal arguments** from a knowledge graph containing ~28,000 STJ decisions and 207,000 relations.

The platform combines six capabilities that do not exist together in any other legal tech product:

1. **Anti-hallucination verification** against real tribunal data
2. **Temporal intelligence** — weighting by recency, trend detection across jurisprudence
3. **Judge profiling** — delta analysis by minister, active divergence tracking
4. **Knowledge graph** — decisions connected by criteria, legal provisions, and precedents
5. **MCP-native architecture** — any LLM (Claude, ChatGPT) can use Valter as a tool
6. **Argument composition** — assembling multi-step legal reasoning chains from graph patterns

The v1.x series stabilizes production, adds resilience, and launches the Legal Reasoning Chain (the flagship feature). The v2.x series expands to multiple tribunals, integrates sister products (Leci, Juca), and establishes public presence via the ChatGPT App Directory.

## Competitive Positioning

| Dimension | Others | Valter |
|---|---|---|
| **Search** | Keyword search | Hybrid search with KG boost and cross-encoder reranking |
| **Results** | List of similar cases | Argument composition from success patterns in graph |
| **Citations** | "The jurisprudence says..." | "REsp X says Y, verified, cited Z times in graph" |
| **Weighting** | Treat all decisions equally | Temporal intelligence — weights by recency, trend detection |
| **Judges** | Ignore who judges | Judge profiling — delta by minister, active divergences |
| **Accuracy** | Hallucinate references | Anti-hallucination verification against real tribunal data |
| **Integration** | Closed systems | MCP-native — any LLM can use as tool |
| **Data model** | Documents as text blobs | Knowledge graph — decisions connected by criteria, provisions, precedents |

## Current State

As of February 2026, Valter has:

- **33 implemented features** spanning search, graph analytics, MCP server, ingestion, verification, observability, and CI/CD
- **3 features in progress**: App Directory preparation (~70%), R2 canary storage (~90%), missing integras download flow (~80%)
- **18 planned features** distributed across v1.0 through v1.2
- **11 ideas** under evaluation for v2.0+

The system indexes ~23,400 STJ decisions in PostgreSQL, with ~3,700 vectorized in Qdrant and ~28,000 nodes with ~207,000 relationships in Neo4j.

:::note
See [Milestones](./milestones) for the detailed breakdown of each version, and [Changelog](./changelog) for the history of shipped features.
:::

## Premortem Analysis

Eight failure scenarios were analyzed to identify risks before they materialize. Each scenario maps to a specific milestone that addresses it.

| # | Failure Scenario | False Premise | Mitigation | Milestone |
|---|---|---|---|---|
| 1 | Redis goes down, all requests blocked | "Redis will always be up" | Fail-open rate limiter for valid API keys | v1.0 |
| 2 | Neo4j silently returns stale data | "Graph data is always fresh" | Health check endpoint, staleness alerts | v1.0 |
| 3 | Data stagnation — corpus stops growing | "23K docs are enough" | ARQ cron ingestion, indexation gap closure | v1.0 / v1.1 |
| 4 | Neo4j query hangs, blocks entire request | "Neo4j always responds in time" | Circuit breaker with 5s timeout | v1.1 |
| 5 | Multi-tribunal is much harder than expected | "Just add more data" | TRF spike in v1.2 before committing to v2.0 | v1.2 |
| 6 | App Directory yields low ROI | "ChatGPT users will find us" | Demoted from v1.2 to v2.1, focus on core product | v2.1 |
| 7 | Single developer becomes unavailable | "Diego can always fix things" | Absence runbook, documented ops procedures | v1.0 |
| 8 | Docker compose issues in new environments | "It works on my machine" | Documented setup, pinned versions, CI validation | v1.0 |

## Pending Decisions

Nine architectural and product decisions remain open. Each will be resolved before or during its target milestone.

| # | Decision | Options Under Consideration | Target |
|---|---|---|---|
| 1 | R2 canary activation timing | Activate in v1.0 vs wait for more traffic data | v1.0 |
| 2 | Legacy route sunset | Immediate removal vs deprecation period with warnings | v1.0 |
| 3 | Privacy policy / terms of use authorship | Write in-house vs engage legal counsel | v1.0 |
| 4 | Leci integration model | Embedded library vs API calls vs shared database | v2.0 |
| 5 | Juca integration level | Read-only consumer vs bidirectional sync | v2.0 |
| 6 | Multi-tribunal starting point | TRF-4 (most data) vs TST (simpler schema) vs STF (highest impact) | v2.0 |
| 7 | Doutrina (legal doctrine) scope | Index doctrine texts vs link to external sources only | v2.0+ |
| 8 | Embedding model migration | Keep Legal-BERTimbau 768d vs upgrade to 1024d model | v1.1 |
| 9 | Reasoning chain sync vs async | Synchronous endpoint vs background job with polling | v1.2 |

---
# docs/roadmap/milestones.md
---


# Milestones

Sequential milestone plan: v1.0 through v2.1. Each milestone depends on the previous one being complete. Estimated timeline runs from March 2026 through late 2026.

## v1.0 — Stable Production

**Objective:** Stabilize production, fix premortem vulnerabilities, prevent silent degradation.

**Prerequisite:** None (current state).

**Estimated effort:** 2-3 weeks.

### Features

| Feature | Priority | Description |
|---|---|---|
| Rate limiter fail-open | P0 | When Redis is down, allow requests from valid API keys instead of blocking all traffic |
| Indexation gap closure | P0 | Batch-index the ~19,700 ementa-only documents that lack embeddings (3,673 -> 20,000+ vectors) |
| Alerting wiring | P1 | Connect Railway logs to Slack for critical errors and degradation alerts |
| HTTPS fix | P1 | Resolve certificate validation issues on production domain |
| Merge pending PRs | P1 | Close out open pull requests blocking downstream work |
| Privacy policy / terms | P1 | Add required legal pages for App Directory submission |
| Datetime migration | P2 | Migrate naive datetime fields to timezone-aware (`datetime` -> `datetime(timezone.utc)`) |
| README update | P2 | Update README to reflect current state and setup instructions |
| Absence runbook | P2 | Document operational procedures for when the primary developer is unavailable |
| R2 canary activation | P2 | Activate canary rollout for R2 artifact storage (currently at ~90% implementation) |

### Completion Criteria

- Rate limiter allows requests when Redis is down (fail-open for valid keys)
- Qdrant contains >= 20,000 indexed vectors
- Slack alerts firing on critical errors
- HTTPS certificate valid on production domain
- Zero `DeprecationWarning` from naive datetime usage


## v1.1 — Resilience + Search Quality

**Objective:** Resilience to partial infrastructure failures and measurable search quality improvements.

**Prerequisite:** v1.0 complete.

**Estimated effort:** 2-3 weeks.

### Features

| Feature | Priority | Description |
|---|---|---|
| Circuit breaker | P0 | Stop calling Neo4j after repeated failures/timeouts (>5s), allow recovery without blocking requests |
| Connection pool configuration | P1 | Tune PostgreSQL, Neo4j, and Redis connection pools for production load patterns |
| ARQ cron ingestion | P1 | Scheduled background jobs to check for new STJ decisions and ingest automatically |
| Fallback extraction to core | P1 | Move fallback text extraction logic from `stores/` into `core/` (proper layer) |
| Heuristic maps externalization | P2 | Move hardcoded classification heuristics to configuration files |
| Stopwords unification | P2 | Single stopwords source shared between BM25 and query expansion |
| Fallback metrics | P2 | Prometheus counters for how often fallback paths are exercised |
| Store unit tests | P2 | Unit test coverage for `stores/` layer (currently undertested) |

### Completion Criteria

- Circuit breaker active: Neo4j hang > 5s opens circuit, requests proceed without graph features
- Connection pools configured with explicit limits and timeouts
- ARQ checks for new decisions at least weekly
- Fallback extraction logic lives in `core/`, not `stores/`


## v1.2 — Legal Reasoning Chain

**Objective:** Transform Valter from a search backend into a **reasoning engine**. This is the flagship feature.

**Prerequisite:** v1.1 complete (circuit breaker and connection pools required for heavy multi-store queries).

**Estimated effort:** 2-3 weeks.

### Features

| Feature | Priority | Description |
|---|---|---|
| `core/reasoning_chain.py` orchestrator | P0 | Server-side orchestrator that composes verified legal arguments from knowledge graph paths |
| `POST /v1/reasoning-chain` endpoint | P0 | REST endpoint exposing the reasoning chain to frontends |
| `compose_legal_argument` MCP tool | P0 | MCP tool allowing LLMs to request composed legal arguments with provenance |
| Provenance tracking | P0 | Every step in the reasoning chain links back to specific decisions, with citation counts and graph position |
| Temporal intelligence integration | P1 | Reasoning chain weights recent decisions higher, flags overturned precedents |
| TRF spike (50 decisions) | P1 | Ingest 50 TRF decisions to test multi-tribunal feasibility before committing to v2.0 |

### How It Works

The reasoning chain orchestrator follows this flow:

1. **Query expansion** — parse the legal question, identify relevant criteria and legal provisions
2. **Multi-strategy retrieval** — hybrid search (BM25 + semantic + KG boost) for relevant decisions
3. **Graph traversal** — follow citation paths, shared criteria, and precedent chains in Neo4j
4. **Argument composition** — assemble a multi-step legal argument from the strongest graph paths
5. **Verification** — every cited decision is verified against real STJ data (anti-hallucination)
6. **Provenance attachment** — each step includes the source decision, citation count, recency, and graph connectivity score

### Completion Criteria

- Reasoning chain returns >= 3 verified steps with full provenance
- MCP tool functional and tested with Claude and ChatGPT
- Latency p95 < 5s for reasoning chain requests
- TRF spike completed with documented breakpoints and feasibility assessment


## v2.0 — Multi-Tribunal Platform

**Objective:** Expand beyond STJ to other Brazilian courts.

**Prerequisite:** v1.2 complete (TRF spike executed, multi-tribunal breakpoints documented).

**Estimated effort:** 2-3 months (scope depends on spike results from v1.2).

:::caution
This milestone is significantly more complex than it appears. The current codebase — `core/verifier.py`, `pipeline/`, `stores/stj_metadata.py` — has STJ-specific assumptions throughout. The TRF spike in v1.2 will identify exactly what needs to change.
:::

### Features

| Feature | Priority | Description |
|---|---|---|
| Multi-tribunal architecture | P0 | Abstract tribunal-specific logic behind interfaces, support multiple courts in the same deployment |
| TRF support | P0 | Federal Regional Courts — starting with the court identified in the v1.2 spike |
| TST support | P1 | Superior Labor Court |
| STF support | P1 | Supreme Federal Court (constitutional matters) |
| Leci integration | P1 | Integration with Leci (sister product) for enriched legal analysis |
| Juca integration | P1 | Integration with Juca (frontend) for seamless user experience |
| Automatic ingestion pipeline | P1 | Continuous ingestion from multiple tribunal portals without manual intervention |

### Completion Criteria

- At least 1 additional court with searchable, verified data
- Reasoning chain works across tribunals (e.g., STJ decision citing TRF precedent)
- Ingestion pipeline running for >= 2 courts


## v2.1 — Scale + Public Presence

**Objective:** Multi-consumer platform with SLA guarantees and public ChatGPT App Directory presence.

**Prerequisite:** v2.0 complete (multi-tribunal working, stable enough for external users).

**Estimated effort:** Depends on demand and App Directory review timeline.

:::note
App Directory submission was originally planned for v1.2 but was demoted to v2.1 based on premortem analysis (#6: low ROI risk if submitted before the product is mature enough).
:::

### Features

| Feature | Priority | Description |
|---|---|---|
| ChatGPT App Directory submission | P1 | Submit Valter as a public MCP tool in the ChatGPT App Directory |
| MCP hardening | P0 | Rate limiting per consumer, request validation, abuse prevention |
| Multi-tenancy | P1 | Support multiple organizations with isolated data and billing |
| SLA guarantees | P1 | Documented uptime, latency, and availability targets |
| Load testing | P0 | Validate that the system handles target concurrent load |
| Store test coverage > 80% | P2 | Comprehensive test coverage for all store implementations |

### Completion Criteria

- At least 1 external user (beyond the developer) actively using the system
- App Directory submission completed (pending review)
- Load tests validate SLA targets under concurrent load


## Timeline

```
2026-03       v1.0 — Stable Production (~2-3 weeks)
                |
2026-03/04    v1.1 — Resilience + Search Quality (~2-3 weeks)
                |
2026-04       v1.2 — Legal Reasoning Chain (~2-3 weeks) *** FLAGSHIP ***
                |
2026-05-07    v2.0 — Multi-Tribunal Platform (~2-3 months, scope from spike)
                |
2026-H2       v2.1 — Scale + Public Presence (depends on demand)
```

:::tip
The v1.x series is designed for a single developer + AI agents working in 2-3 week sprints. The v2.0 timeline is deliberately wider because multi-tribunal complexity is the highest-risk item in the roadmap.
:::

---
# docs/roadmap/changelog.md
---


# Changelog

Significant changes organized by date, newest first. For session-level development logs, see `PROGRESS.md` in the repository root.

Categories: **Added** (new feature), **Changed** (modification to existing feature), **Fixed** (bug fix), **Security** (security-related change), **Removed** (removed feature or dead code).


## February 2026

### 2026-02-28 — Reorg, Roadmap v2, Project Map

**Changed**
- Reorg Phases 0-2: security fixes, dependency cleanup, dead code removal
- Roadmap v2: premortem analysis, reasoning chain vision, milestone restructuring with risk-informed planning
- Issue organization: 5 GitHub milestones created, 9 new issues filed, 2 closed, 11 updated with labels and milestones

### 2026-02-27 — App Directory Preparation

**Added**
- Security hardening for App Directory requirements (input validation, error sanitization)
- UX improvements for MCP tool descriptions and parameter schemas
- Metadata alignment with App Directory submission format
- Dry-run validation of App Directory submission flow

### 2026-02-25 — Quality Gates

**Added**
- Enforced `ruff check` and `ruff format` as quality gates
- `mypy` strict typing on critical modules (`api/deps.py`, `api/routes/ingest.py`, `mcp/tools.py`)
- `make quality` target combining lint + type check + test

### 2026-02-24 — Rate Limit Fail-Safe, Tracing, Decoupling

**Added**
- Rate limit fail-safe mechanism (initial implementation)
- OpenTelemetry tracing with console exporter and `trace_id` propagation
- Architectural decoupling: 10 sprints separating concerns across layers

**Changed**
- Store implementations decoupled from direct core imports (dependency injection)

### 2026-02-22 — ChatGPT Integration, Search Enrichment

**Added**
- ChatGPT integration live via MCP remote (HTTP/SSE transport)
- Search result enrichment with graph metadata (citation count, connectivity score)

### 2026-02-21 — KG Quality, MCP Remote, Auth, Governance

**Added**
- MCP remote server with HTTP/SSE transport (`mcp/remote_server.py`)
- Authentication hardening (API key validation, HMAC verification)
- Operational governance documentation (`CLAUDE.md` governance section)

**Changed**
- Knowledge graph quality improvements (node deduplication, relation validation)

### 2026-02-17 — Batch Classification, Railway Deploy

**Added**
- Batch document classification pipeline
- Railway production deployment configuration

### 2026-02-16 — Pagination, STJ Metadata, Ingestion, Embeddings

**Added**
- Cursor-based pagination for search results
- STJ metadata store (`stores/stj_metadata.py`) for tribunal-specific data
- Ingestion workflow with state machine (`core/workflow_state_machine.py`)
- Shortest-path graph queries between decisions
- Embedding service with configurable model (`embeddings/`)

### 2026-02-15 — Features Search, MCP Desktop, Reranking, Graph Endpoints

**Added**
- Features search endpoint (search by extracted legal features)
- MCP server for Claude Desktop (stdio transport)
- Cross-encoder reranking for search results
- 4 additional graph analytics endpoints (temporal analysis, minister comparison, criterion evolution, citation network)

### 2026-02-13 — Graph Analytics Endpoints

**Added**
- Graph analytics endpoints via `POST /v1/graph/`:
  - Divergence detection between ministers and turmas
  - Optimal legal argument path finding
  - Minister profile analysis (voting patterns, specializations)
  - Temporal evolution of legal criteria

### 2026-02-10 — Data Ingestion at Scale

**Added**
- Large-scale data ingestion: 23,441 documents, 2,119 extracted features, 810,225 metadata records
- Ingestion pipeline with PDF extraction, text processing, and metadata parsing

### 2026-02-09 — Project Foundation

**Added**
- Hybrid search engine (BM25 lexical + semantic vector search with RRF fusion)
- Anti-hallucination verification against STJ public data (`core/verifier.py`)
- IRAC legal analysis framework (`models/irac.py`)
- MCP server skeleton with tool definitions
- Initial corpus: 3,673 documents indexed with embeddings
- PostgreSQL document store with JSONB metadata
- Qdrant vector store for semantic search
- Redis cache layer with rate limiting
- Neo4j knowledge graph with FRBR-based ontology
- FastAPI REST API with structured error handling
- structlog JSON logging with request-scoped `trace_id`

---
# docs/reference/glossary.md
---


# Glossary

Essential terms from the Brazilian legal domain and Valter's technical architecture. Understanding these terms is critical for working with the codebase and reading the documentation.

## Legal Domain

### A

**Acórdão**
A collegial (collective) court decision, issued by a panel of ministers rather than a single judge. Most STJ decisions indexed by Valter are acórdãos.

### C

**Critério**
A legal criterion or standard applied by the court to decide a case. In the knowledge graph, criteria are nodes that connect multiple decisions applying the same legal test.

### D

**Desembargador**
A justice of a regional court (TRF). Relevant for multi-tribunal expansion in v2.0. Equivalent to "judge" at the appellate level.

**Dispositivo Legal**
A legal provision — a specific article, paragraph, or clause of a statute or regulation cited in a court decision. In the knowledge graph, dispositivos connect decisions that apply the same legal text.

**Divergência**
A disagreement between ministers or turmas on how to apply a legal criterion. Valter's graph analytics can detect and track active divergences, which are valuable for identifying unsettled areas of law.

### E

**Ementa**
The official summary or headnote of a court decision. This is the most commonly indexed text in Valter's corpus. Most of the ~23,400 documents have at least an ementa; fewer have the full text (íntegra).

### F

**Fase**
A procedural phase or stage in a case (e.g., initial petition, defense, judgment). Used in the PROJUDI pipeline for classifying document types within a case file.

### I

**Íntegra**
The full text of a court decision, as opposed to just the ementa (summary). Downloading and indexing íntegras improves search quality and enables deeper analysis.

**IRAC**
Issue, Rule, Application, Conclusion — a standard legal analysis framework. Valter uses IRAC to structure the analysis of court decisions, breaking each into its constituent reasoning steps. Implemented in `models/irac.py`.

### J

**Jurisprudência**
The body of court decisions on a topic. "Case law" in English. Valter's primary domain is STJ jurisprudência on federal law matters.

### M

**Ministro**
A justice of a superior court (STJ, STF). Called "minister" in Portuguese court terminology. Valter tracks minister voting patterns, specializations, and divergences.

**Ministro Relator**
The reporting minister — the justice assigned to a case who writes the decision and presents it to the panel. The relator's identity is a key metadata field for every indexed decision.

### N

**Número de Registro**
The registration number of a decision in the court's internal system. Used as a unique identifier when linking to STJ public records.

### P

**Precedente**
A prior court decision used as the basis for a legal argument. In the knowledge graph, precedent relationships form citation chains that the reasoning engine traverses.

**PROJUDI** (Processo Judicial Digital)
The electronic judicial process system used in Paraná (PR) state courts. Valter has a specific pipeline (`core/projudi_pipeline.py`) for extracting and classifying documents from PROJUDI case files.

### R

**REsp** (Recurso Especial)
A special appeal to the STJ on matters of federal law interpretation. REsps are the most common type of decision in Valter's corpus.

### S

**Seção**
A section of the court, composed of two turmas. The STJ has three sections: 1st Section (1st + 2nd Turma), 2nd Section (3rd + 4th Turma), and 3rd Section (5th + 6th Turma). Sections hear cases when turmas disagree.

**seqDocumento**
The sequential document identifier in STJ's internal system. Used for linking directly to specific decisions on the STJ portal.

**STF** (Supremo Tribunal Federal)
Brazil's Supreme Federal Court. Handles constitutional matters. Planned for v2.0 multi-tribunal expansion.

**STJ** (Superior Tribunal de Justiça)
Brazil's Superior Court of Justice. Responsible for unifying the interpretation of federal law. Valter's primary and currently only data source.

**Súmula**
A binding legal summary issued by a superior court (STJ or STF) that consolidates the court's understanding on a specific legal question. Súmulas are numbered (e.g., Súmula 123/STJ) and carry significant precedential weight.

### T

**TRF** (Tribunal Regional Federal)
Federal Regional Courts at the appellate level. Brazil has six TRFs. Planned as the first expansion target in v2.0.

**TST** (Tribunal Superior do Trabalho)
The Superior Labor Court. Handles labor law appeals. Planned for v2.0.

**Turma**
A panel or division of ministers that hears cases. The STJ has six turmas. Each minister belongs to one turma. Turma assignment affects which cases a minister hears and can reveal patterns in voting behavior.


## Technical Terms

### A

**ARQ** (Async Redis Queue)
A Python library for background job processing using Redis as a broker. Valter uses ARQ for asynchronous ingestion tasks in `workers/`.

### B

**BM25** (Best Matching 25)
A probabilistic text ranking algorithm used for lexical (keyword-based) search. One of two retrieval strategies in Valter's hybrid search, alongside semantic vector search.

### C

**Canary Rollout**
A deployment strategy where a small percentage of traffic is gradually routed to a new system to validate it before full migration. Applied to R2 artifact storage in `stores/artifact_storage.py`.

**Circuit Breaker**
A resilience pattern that stops calling a failing service after repeated failures, allowing it to recover. Planned for v1.1 to prevent Neo4j hangs from blocking all requests.

**Cross-Encoder**
A neural model that jointly encodes a query and a document together for fine-grained relevance scoring. Used in the reranking stage of search, after initial retrieval by BM25 and semantic search.

### D

**Dual-Vector**
An encoding strategy that creates separate embeddings for facts and legal thesis within a document, enabling more targeted retrieval. Implemented in `core/dual_vector_retriever.py`.

### F

**FRBR** (Functional Requirements for Bibliographic Records)
A conceptual model for bibliographic records, adapted as the ontology for Valter's Neo4j knowledge graph. Structures how decisions, provisions, and concepts relate to each other.

### K

**KG Boost** (Knowledge Graph Boost)
A relevance boost applied to search results based on their connectivity in the Neo4j knowledge graph. Documents with more citations, shared criteria, and graph connections receive higher scores. Configurable via `VALTER_KG_BOOST_BATCH_ENABLED`.

### M

**MCP** (Model Context Protocol)
An open standard for LLM-to-tool integration. Allows language models like Claude and ChatGPT to use external tools through structured tool definitions with JSON Schema. Valter supports two MCP transports: stdio (local, for Claude Desktop) and HTTP/SSE (remote, for ChatGPT and other consumers).

### R

**RRF** (Reciprocal Rank Fusion)
A method for combining ranked lists from different retrieval strategies into a single ranking. Valter uses RRF to merge BM25 lexical results with semantic vector results, producing a hybrid ranked list.

### S

**structlog**
A structured logging library for Python. Valter uses structlog for JSON-formatted logs with request-scoped `trace_id` for tracing requests across the system.

---
# docs/reference/faq.md
---


# Frequently Asked Questions

Common questions from developers, contributors, and AI agents working with Valter.

## Architecture

### Why does Valter use 4 databases?

Each database is optimized for a fundamentally different workload:

| Database | Role | Why Not PostgreSQL? |
|---|---|---|
| **PostgreSQL** | Relational integrity, document metadata, JSONB storage, ingestion state machine | N/A — this is the relational store |
| **Qdrant** | Purpose-built vector search with cosine similarity, HNSW indexing | pgvector exists but lacks Qdrant's filtering, performance at scale, and dedicated vector operations |
| **Neo4j** | Native graph traversal, Cypher queries, relationship-first data model | Recursive CTEs in PostgreSQL cannot match native graph performance for multi-hop traversals across 207K+ relationships |
| **Redis** | Sub-millisecond cache, rate limiting, background job queue (ARQ) | PostgreSQL is too slow for per-request rate limiting and caching |

The alternative considered was a single PostgreSQL instance with pgvector and recursive CTEs. This was rejected because graph queries (shortest path between decisions, divergence detection across citation chains) and high-throughput vector search are fundamentally different workloads that benefit from purpose-built engines.

:::tip
All four databases degrade gracefully. Search works without Neo4j (no KG boost). Search works without Redis (no cache, but rate limiting currently fails closed — see the [Troubleshooting](./troubleshooting) guide). Only PostgreSQL is strictly required.
:::

### Why a monolith instead of microservices?

Three reasons:

1. **Team size** — Valter is built by one developer assisted by AI agents. The operational overhead of multiple services (separate deployments, service discovery, distributed tracing) would slow development without providing proportional benefit.
2. **Shared business logic** — The same retriever, verifier, and enricher code runs in 4 contexts (API server, MCP stdio, MCP remote, ARQ worker). A monolith shares this code naturally. Microservices would require duplicating it or adding an internal service layer.
3. **Modular structure** — The codebase follows strict layering rules (`api/ -> core/ -> models/`, stores implement protocols) that would allow future extraction into services if scale demands it.

### Why MCP instead of a custom API for LLMs?

MCP (Model Context Protocol) is an open standard. The benefits over a custom API:

- **Any MCP-compatible LLM can use Valter** without integration work. Claude and ChatGPT both support MCP today.
- **Structured tool definitions** with JSON Schema mean the LLM understands parameters, types, and descriptions without custom prompt engineering.
- **Two transports for different use cases**: stdio for local Claude Desktop usage (low latency, no network), and HTTP/SSE for remote access (ChatGPT, other consumers).
- **No client SDK needed** — the protocol handles serialization, error reporting, and tool discovery.

The tradeoff is that MCP tools have a fixed request-response pattern. For long-running operations (like the planned reasoning chain), the endpoint may need to return intermediate results or use async polling.

### What is the difference between stdio and remote MCP?

| Aspect | stdio (local) | HTTP/SSE (remote) |
|---|---|---|
| **Transport** | Standard input/output pipes | HTTP POST + Server-Sent Events |
| **Use case** | Claude Desktop on the same machine | ChatGPT, remote clients, any network consumer |
| **Authentication** | None needed (local process) | API key + HMAC verification |
| **Startup** | `python -m valter.mcp.stdio_server` | `make mcp-remote` |
| **Latency** | Lowest (no network) | Network-dependent |
| **Configuration** | `claude_desktop_config.json` | `VALTER_MCP_SERVER_API_KEYS` env var |

Both transports expose the same set of MCP tools. The tool definitions, parameters, and responses are identical regardless of transport.


## Search

### How does KG Boost work?

KG Boost is a post-retrieval relevance boost based on knowledge graph connectivity. The flow:

1. **Initial retrieval** — Hybrid search (BM25 + semantic) returns a ranked list of candidate documents
2. **Graph lookup** — Each candidate document is checked against Neo4j for graph connections (citations received, shared criteria with other results, connection to known precedents)
3. **Score adjustment** — Documents with stronger graph connectivity receive a configurable score boost
4. **Re-ranking** — The final ranking reflects both textual relevance and structural importance in the jurisprudence network

Key properties:

- **Configurable** via `VALTER_KG_BOOST_BATCH_ENABLED` and `VALTER_KG_BOOST_MAX_CONCURRENCY`
- **Graceful degradation** — If Neo4j is unavailable, search results still return without the boost. No error is raised to the user.
- **Batched** — Graph lookups are batched for performance rather than queried one document at a time

### Why Legal-BERTimbau and not a larger model?

The embedding model `rufimelo/Legal-BERTimbau-sts-base` was chosen for three reasons:

1. **Domain-specific** — Fine-tuned on Portuguese legal text, specifically for semantic textual similarity (STS). A general-purpose multilingual model (e.g., `all-MiniLM-L6-v2`) performs measurably worse on legal Portuguese.
2. **768 dimensions** — A good balance of quality versus storage and computation cost. Each of the ~23,400 documents requires a 768-float vector. Doubling the dimension doubles storage and search time.
3. **Open source** — Available on Hugging Face, can be downloaded and run locally without API dependencies.

:::note
Pending decision: whether to migrate to a 1024-dimension model for better quality. This would require re-indexing all documents and is tracked as decision #8 in the [Roadmap](../roadmap/).
:::


## Data

### How does anti-hallucination verification work?

The verifier (`core/verifier.py`) checks that cited decisions actually exist and contain what is claimed. The process:

1. **Reference extraction** — Parse decision numbers (REsp, AgRg, etc.) from text
2. **Existence check** — Verify each cited decision exists in the PostgreSQL corpus
3. **Content validation** — Confirm that the cited decision actually discusses the claimed legal point
4. **Metadata cross-reference** — Check minister, turma, date, and other metadata against STJ public records

If a reference cannot be verified, it is flagged. This prevents LLMs from citing non-existent decisions or misattributing legal positions — a common and serious problem in AI-assisted legal work.

### What is IRAC analysis?

IRAC (Issue, Rule, Application, Conclusion) is a standard framework for structuring legal analysis:

- **Issue** — The legal question the court is deciding
- **Rule** — The statute, regulation, or legal principle that applies
- **Application** — How the court applies the rule to the specific facts
- **Conclusion** — The court's decision

Valter uses IRAC to decompose court decisions into structured components (`models/irac.py`). This structure enables more precise search (search by issue or rule, not just full text) and powers the planned reasoning chain feature.

### Can Valter handle tribunals beyond STJ?

Not yet, but it is planned for v2.0.

The current codebase has STJ-specific assumptions in several places: the verifier checks against STJ's public portal, the metadata store is STJ-specific (`stores/stj_metadata.py`), and the ingestion pipeline parses STJ document formats.

The approach for multi-tribunal expansion:

1. **v1.2 TRF spike** — Ingest 50 TRF decisions to identify exactly what breaks
2. **v2.0 abstraction** — Factor out tribunal-specific logic behind interfaces
3. **Incremental rollout** — Add TRF first, then TST, then STF

:::caution
Multi-tribunal is the highest-risk item on the roadmap. The TRF spike in v1.2 exists specifically to validate feasibility before committing to v2.0.
:::

### How does the ingestion workflow work?

The ingestion pipeline transforms raw court documents into searchable, graph-connected knowledge. The stages:

1. **PDF extraction** — Extract text from court decision PDFs (`core/pdf_extraction.py`). Falls back to OCR via pytesseract for scanned documents.
2. **Text processing** — Clean, normalize, and segment the extracted text. Handle encoding issues, header/footer removal, and page boundary artifacts.
3. **Metadata parsing** — Extract structured metadata: decision number, minister, turma, date, legal provisions cited.
4. **Feature extraction** — Identify legal features: IRAC components, key arguments, cited precedents.
5. **Embedding generation** — Generate vector embeddings using Legal-BERTimbau for semantic search.
6. **Graph insertion** — Create nodes and relationships in Neo4j following the FRBR-based ontology.
7. **State tracking** — The workflow state machine (`core/workflow_state_machine.py`) tracks each document through these stages, enabling retry on failure.

The pipeline can be triggered manually via the ingest API endpoint or automatically via ARQ background workers.


## Operations

### What happens if Redis goes down?

Currently, the rate limiter is **fail-closed** — if Redis is unavailable, all requests are blocked, even from valid API keys. This is a known issue (premortem #1) and is the highest-priority fix for v1.0.

The planned fix: fail-open for requests with valid API keys when Redis is unreachable. Rate limiting will be best-effort rather than a hard gate.

Other Redis-dependent features (caching, ARQ job queue) degrade more gracefully: cache misses simply hit the database directly, and background jobs wait until Redis recovers.

### What monitoring does Valter have?

Current state:

- **30+ Prometheus metrics** instrumented across API endpoints, search latency, graph queries, and ingestion
- **structlog JSON logging** with `trace_id` on every request for tracing
- **OpenTelemetry tracing** with console exporter (traces visible in logs)

Gaps being addressed in v1.0:

- No Prometheus server scraping the metrics (metrics are exposed but not collected)
- No dashboards (Grafana or similar)
- No alert dispatcher (alerts not yet wired to Slack or PagerDuty)

---
# docs/reference/troubleshooting.md
---


# Troubleshooting

Common issues encountered during development and production operation, with diagnosis steps and solutions.

## Database Connectivity

### Neo4j returns 503 on graph endpoints

**Symptom:** Graph analytics endpoints (`/v1/graph/*`) return `503 Service Unavailable`.

**Cause:** Neo4j is not included in `docker-compose.yml` by design. It must be configured separately.

**Solution:**

Option A — Local Neo4j:

```bash
# Install and start Neo4j locally
brew install neo4j  # macOS
neo4j start
```

Option B — Neo4j Aura (cloud):

```bash
# Set these in your .env file
NEO4J_URI=neo4j+s://your-instance.databases.neo4j.io
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your-password
```

:::note
Non-graph endpoints (search, verify, enrich) work without Neo4j. Graph features degrade gracefully — search results simply lack KG boost scores.
:::

### Redis connection refused

**Symptom:** `ConnectionRefusedError` on startup or `redis.exceptions.ConnectionError` during requests.

**Cause:** Redis container is not running.

**Solution:**

```bash
# Start infrastructure containers (PostgreSQL, Redis, Qdrant)
make docker-up

# Verify Redis is running
docker compose ps redis
```

**Impact if Redis is down:**
- Cache is disabled (requests hit the database directly)
- Rate limiting fails **closed** — all requests are blocked (v1.0 will fix this to fail-open)
- ARQ background workers cannot process jobs

### PostgreSQL migration failures

**Symptom:** `alembic upgrade head` fails with schema conflicts or connection errors.

**Cause:** Database not running, or migration history is out of sync.

**Solution:**

```bash
# Ensure PostgreSQL is running
make docker-up

# Run migrations
make migrate

# If there are conflicts, check the migration history
alembic -c migrations/alembic.ini history

# To see the current database revision
alembic -c migrations/alembic.ini current
```

If a migration fails mid-way:

1. Check which revision the database is at with `alembic current`
2. Review the failing migration in `migrations/versions/`
3. If the migration has a `downgrade()` function, you can rollback: `alembic -c migrations/alembic.ini downgrade -1`
4. If the migration is marked irreversible, consult the PR that introduced it for a contingency plan

:::danger
Never run `alembic downgrade` in production without an explicit rollback plan. Irreversible migrations (those without `downgrade()`) require manual database intervention.
:::


## Embedding Model

### Model download fails or is slow

**Symptom:** `make download-model` hangs, times out, or fails with a network error.

**Cause:** Hugging Face model downloads can be large (~500MB for Legal-BERTimbau) and are affected by network conditions.

**Solution:**

```bash
# Retry the download
make download-model

# If download keeps failing, check your network and HuggingFace status
curl -I https://huggingface.co

# Alternative: use a remote embedding service instead of local model
# Set in .env:
VALTER_EMBEDDING_SERVICE_URL=https://your-embedding-service/encode
```

The model is cached at `~/.cache/huggingface/` after the first successful download. Subsequent starts will use the cache.

:::tip
If you are behind a corporate proxy, set `HTTP_PROXY` and `HTTPS_PROXY` before running `make download-model`.
:::

### Qdrant dimension mismatch

**Symptom:** Search returns an error about vector dimensions not matching the collection configuration.

**Cause:** The Qdrant collection was created with a different embedding dimension than the current model produces. This happens when switching embedding models (e.g., from a 384d model to the 768d Legal-BERTimbau).

**Solution:**

1. Check the current model's dimension:

```bash
python -c "from sentence_transformers import SentenceTransformer; m = SentenceTransformer('rufimelo/Legal-BERTimbau-sts-base'); print(m.get_sentence_embedding_dimension())"
# Expected output: 768
```

2. Verify the `VALTER_EMBEDDING_DIMENSION` environment variable matches:

```bash
# In .env
VALTER_EMBEDDING_DIMENSION=768
```

3. If the collection was created with the wrong dimension, it must be recreated:

```bash
# WARNING: This deletes all indexed vectors. You will need to re-index.
python -c "from qdrant_client import QdrantClient; c = QdrantClient('localhost', port=6333); c.delete_collection('valter_documents')"
```

After deleting the collection, restart the application — it will recreate the collection with the correct dimension on startup.


## OCR Issues

### OCR fails with ImportError

**Symptom:** `ImportError: No module named 'pytesseract'` or `FileNotFoundError: tesseract is not installed`.

**Cause:** OCR has two dependencies — the Python package and the system binary. Both must be installed.

**Solution:**

```bash
# Install the Python OCR extras
pip install -e ".[ocr]"

# Install the system Tesseract binary
# macOS:
brew install tesseract tesseract-lang

# Ubuntu/Debian:
sudo apt-get install tesseract-ocr tesseract-ocr-por

# Verify installation
tesseract --version
```

:::note
OCR is optional. If not installed, text extraction falls back to `pdfplumber`, which works well for digitally-generated PDFs but cannot extract text from scanned documents.
:::


## MCP Server

### MCP stdio not connecting to Claude Desktop

**Symptom:** Claude Desktop does not list Valter's tools, or shows a connection error.

**Cause:** Incorrect `claude_desktop_config.json` configuration.

**Solution:**

1. Verify your Claude Desktop configuration file (location depends on OS):

```json
{
  "mcpServers": {
    "valter": {
      "command": "python",
      "args": ["-m", "valter.mcp.stdio_server"],
      "env": {
        "PYTHONPATH": "/path/to/Valter/src"
      }
    }
  }
}
```

2. Common issues to check:
   - The `command` must point to the correct Python binary (use the full path if using a virtual environment: `/path/to/Valter/.venv/bin/python`)
   - `PYTHONPATH` must include the `src/` directory
   - Environment variables needed by Valter (database URLs, API keys) must be present in the `env` block or inherited from the shell

3. Restart Claude Desktop after changing the configuration.

### MCP remote returns 401 Unauthorized

**Symptom:** Remote MCP client receives `401 Unauthorized` when calling tools.

**Cause:** Invalid or missing API key in the request.

**Solution:**

1. Verify the API keys are configured on the server:

```bash
# In .env — comma-separated list of valid keys
VALTER_MCP_SERVER_API_KEYS=key1,key2
```

2. Verify the client is sending the key correctly (as a Bearer token or in the configured header).

3. Start the remote MCP server and check logs for auth errors:

```bash
make mcp-remote
# Watch for 401 entries in the structured log output
```

:::caution
API keys are validated via HMAC. Ensure there are no trailing whitespace characters in the key values in your `.env` file.
:::


## Development

### ruff not found

**Symptom:** `make lint` fails with `ruff: command not found`.

**Cause:** The virtual environment is not activated, or `ruff` is not installed.

**Solution:**

```bash
# Activate the virtual environment
source .venv/bin/activate

# If ruff is not installed
pip install ruff

# Then run lint
make lint
```

### Tests fail with async errors

**Symptom:** Tests fail with `RuntimeError: no current event loop` or `PytestUnraisableExceptionWarning` related to asyncio.

**Cause:** `pytest-asyncio` mode is not configured correctly.

**Solution:**

Verify that `pyproject.toml` has the correct asyncio mode:

```toml
[tool.pytest.ini_options]
asyncio_mode = "auto"
```

If the setting is correct but tests still fail, check that `pytest-asyncio` is installed:

```bash
pip install pytest-asyncio
```

### Type checking errors with mypy

**Symptom:** `make quality` fails at the mypy step with type errors.

**Cause:** Missing type stubs or strict typing violations.

**Solution:**

The `quality` target runs mypy only on a scoped subset of files (defined in `MYPY_QUALITY_SCOPE` in the Makefile). If you are adding new files to the scope, ensure they have complete type annotations on all public functions.

```bash
# Run mypy on just the scoped files
mypy --follow-imports=silent src/valter/api/deps.py src/valter/api/routes/ingest.py src/valter/mcp/tools.py
```


## Production

### Rate limiting blocks all requests

**Symptom:** All API requests return `429 Too Many Requests` or `503 Service Unavailable`, even at low traffic.

**Cause:** Redis is down and the rate limiter is configured to **fail-closed**. When Redis is unreachable, no rate limit checks can pass, so all requests are rejected.

**Solution (immediate):**

```bash
# Restart Redis
docker compose restart redis

# Verify Redis is responding
docker compose exec redis redis-cli ping
# Expected: PONG
```

**Solution (permanent):** This is tracked as the highest-priority fix for v1.0 — switching the rate limiter to fail-open for valid API keys when Redis is unavailable.

:::danger
Until v1.0 ships the fail-open fix, a Redis outage in production means **100% of traffic is blocked**. Ensure Redis health monitoring and restart automation are in place.
:::

### High latency on graph endpoints

**Symptom:** Graph analytics endpoints take > 10s to respond or time out.

**Cause:** Complex graph traversals on large subgraphs, or Neo4j is under memory pressure.

**Solution:**

1. Check Neo4j memory allocation — the default may be too low for the ~28,000 node / ~207,000 relationship graph
2. Verify that graph indexes exist for frequently-queried properties (decision number, minister name, legal provision)
3. For Neo4j Aura: check the instance tier and whether you are hitting query limits

> **Planned Feature** -- v1.1 will add a circuit breaker that opens after Neo4j hangs for > 5s, allowing requests to proceed without graph features rather than blocking indefinitely.

