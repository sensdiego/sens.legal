---
# docs/CONTENT_MAP.md
---


# Content Map ‚Äî Douto Documentation

> Master index for the documentation writing phase. Each entry describes what the file will contain, where the source information lives, and its priority for the initial launch.

## Priority Legend

- **P0** ‚Äî Essential for launch. Without this, docs are not usable.
- **P1** ‚Äî Important. Should be written soon after launch.
- **P2** ‚Äî Can wait. Nice to have, not blocking.


## docs/index.md
- **Contains**: Home page ‚Äî project overview, key capabilities, current status, quick links, ecosystem diagram
- **Sources**: PROJECT_MAP.md ¬ß1, ROADMAP.md "Implementadas", PROJECT_MAP.md ¬ß5 ecosystem diagram
- **Priority**: **P0**
- **Notes**: First thing any visitor sees. Must be concise and compelling. Include Mermaid ecosystem diagram.


## Getting Started

### docs/getting-started/introduction.md
- **Contains**: What Douto is, the problem it solves, target audiences, ecosystem position, core concepts, boundaries
- **Sources**: PROJECT_MAP.md ¬ß1, ¬ß5; AGENTS.md "Identidade", "Limites"; INNOVATION_LAYER.md market analysis
- **Priority**: **P0**
- **Notes**: Sets mental model for all other docs. Must explain both pipeline and knowledge base modes.

### docs/getting-started/quickstart.md
- **Contains**: Minimal setup to run a search query in under 5 minutes (search-only mode)
- **Sources**: search_doutrina_v2.py docstring and CLI args; PROJECT_MAP.md ¬ß4 (env vars)
- **Priority**: **P0**
- **Notes**: Assumes pre-built corpus JSONs exist. Skip full pipeline setup. Show concrete example with output.

### docs/getting-started/installation.md
- **Contains**: Complete setup for running the entire pipeline end-to-end (PDF extraction through search)
- **Sources**: PROJECT_MAP.md ¬ß3 (stack), ¬ß4 (env vars); pipeline/requirements.txt; all 5 scripts --help
- **Priority**: **P1**
- **Notes**: This is longer and more complex than quickstart. Include troubleshooting section at bottom.


## Architecture

### docs/architecture/overview.md
- **Contains**: Architectural pattern (batch ETL + markdown knowledge graph), pipeline data flow, knowledge base structure, output artifacts, design principles, known limitations
- **Sources**: PROJECT_MAP.md ¬ß5 (all subsections); CLAUDE.md "Priority Order"; PREMORTEM.md known limitations
- **Priority**: **P0**
- **Notes**: The most important architecture doc. Must include at least one Mermaid diagram (pipeline flow).

### docs/architecture/stack.md
- **Contains**: Complete technology inventory with versions, justifications, dependency graph
- **Sources**: PROJECT_MAP.md ¬ß3 (complete); pipeline/requirements.txt
- **Priority**: **P1**
- **Notes**: Table-heavy. Flag unpinned versions as tech debt.

### docs/architecture/decisions.md
- **Contains**: ADRs for 7 architectural decisions + table of 9 pending decisions
- **Sources**: Code analysis (ADR-001 through ADR-006 inferred from code); ROADMAP.md "Decisoes Pendentes" D01-D09
- **Priority**: **P1**
- **Notes**: Critical for future contributors to understand why things are the way they are. ADR-007 (repo vs module) is the most impactful pending decision.

### docs/architecture/diagrams.md
- **Contains**: All Mermaid diagrams ‚Äî pipeline flow, components, ecosystem, knowledge hierarchy, enrichment schema, search architecture, planned architecture
- **Sources**: PROJECT_MAP.md ¬ß5 (existing diagrams); enrich_chunks.py (metadata schema); search_doutrina_v2.py (search flow); ROADMAP.md F29-F30 (planned MCP)
- **Priority**: **P1**
- **Notes**: Consolidates all visual representations. Some diagrams already exist in PROJECT_MAP.md ‚Äî adapt, don't duplicate.


## Features

### docs/features/index.md
- **Contains**: Feature inventory with status badges, organized by domain (pipeline, knowledge base, quality, planned, innovation)
- **Sources**: ROADMAP.md "Funcionalidades" (all sections F01-F48)
- **Priority**: **P0**
- **Notes**: Master feature list. Every feature links to its detailed page (if one exists) or to the ROADMAP.

### docs/features/pipeline/pdf-extraction.md
- **Contains**: How process_books.py works ‚Äî LlamaParse integration, chapter splitting, frontmatter generation, config, limitations
- **Sources**: process_books.py (full code); PROJECT_MAP.md ¬ß5 entry point 1
- **Priority**: **P1**
- **Notes**: First pipeline step. Important for anyone wanting to add new books.

### docs/features/pipeline/intelligent-chunking.md
- **Contains**: rechunk_v3.py's 5-pass algorithm ‚Äî section splitting, classification, merge, split, cleanup. Domain-specific features (running headers, footnotes, law articles).
- **Sources**: rechunk_v3.py (full code ‚Äî 890 lines); PROJECT_MAP.md ¬ß5, ¬ß10
- **Priority**: **P1**
- **Notes**: The most complex pipeline component. Deserves detailed documentation. Highlight the 0% test coverage.

### docs/features/pipeline/enrichment.md
- **Contains**: Chunk classification via MiniMax M2.5 ‚Äî metadata schema (13 fields), how it works, configuration, CRITICAL known issues (missing prompt, no validation)
- **Sources**: enrich_chunks.py (full code); PREMORTEM.md RT01, RT02, RT07, RT11, PF01
- **Priority**: **P0**
- **Notes**: The most risk-laden component. The missing enrich_prompt.md makes this documentation especially critical ‚Äî it may be the only record of how enrichment works. Document the metadata schema exhaustively.

### docs/features/pipeline/embeddings.md
- **Contains**: Legal-BERTimbau embedding generation ‚Äî text composition strategy, output files, configuration, limitations
- **Sources**: embed_doutrina.py (full code); PROJECT_MAP.md ¬ß3 "Modelo de ML"
- **Priority**: **P1**
- **Notes**: The text composition strategy (metadata prefix) is a key design decision that should be well-documented.

### docs/features/pipeline/hybrid-search.md
- **Contains**: Three search modes (semantic, BM25, hybrid), metadata filtering, interactive CLI, usage examples, performance limitations
- **Sources**: search_doutrina_v2.py (full code); PREMORTEM.md RT05, RT06, RP03
- **Priority**: **P0**
- **Notes**: This is what end-users interact with. Must include concrete usage examples and explain the interactive mode commands.

### docs/features/knowledge-base/skill-graph.md
- **Contains**: INDEX_DOUTO.md structure, 8 legal domains, MOC status, navigation pattern, conventions
- **Sources**: INDEX_DOUTO.md; knowledge/mocs/*.md; CLAUDE.md "Knowledge Base Conventions"
- **Priority**: **P1**
- **Notes**: Important for understanding the knowledge organization model.

### docs/features/knowledge-base/mocs.md
- **Contains**: MOC format, active MOCs (4) with statistics, planned MOCs (4), how to add new MOCs
- **Sources**: knowledge/mocs/MOC_CIVIL.md, MOC_PROCESSUAL.md, MOC_EMPRESARIAL.md, MOC_CONSUMIDOR.md; ROADMAP F25
- **Priority**: **P1**
- **Notes**: Include the impressive corpus statistics (35 books, 22k+ chunks).

### docs/features/knowledge-base/atomic-notes.md
- **Contains**: Planned atomic notes feature ‚Äî proposed format, generation strategy (pending D03), relationship to Synthesis Engine
- **Sources**: ROADMAP F21, F36; INNOVATION_LAYER.md; CLAUDE.md
- **Priority**: **P2**
- **Notes**: Feature doesn't exist yet. Document the design intent and pending decisions.


## Configuration

### docs/configuration/environment.md
- **Contains**: Complete env var reference table, known path issues, setup by use case, example .env file
- **Sources**: PROJECT_MAP.md ¬ß4; all 5 pipeline scripts (grep for os.environ and hardcoded paths)
- **Priority**: **P0**
- **Notes**: Critical for anyone trying to run the pipeline. The path inconsistency is the #1 onboarding blocker.

### docs/configuration/settings.md
- **Contains**: All tunable parameters per script (chunk sizes, thread counts, BM25 params, model names, etc.), knowledge base conventions
- **Sources**: All 5 pipeline scripts (grep for hardcoded values); CLAUDE.md
- **Priority**: **P2**
- **Notes**: Most settings are currently hardcoded. This doc serves as a reference AND a roadmap for externalization.

### docs/configuration/integrations.md
- **Contains**: Setup for each external service (LlamaParse, MiniMax, HuggingFace), current and planned sens.legal integration
- **Sources**: PROJECT_MAP.md ¬ß3 "Servicos externos"; enrich_chunks.py (MiniMax setup); ROADMAP F29-F30
- **Priority**: **P1**
- **Notes**: The MiniMax integration via Anthropic SDK is fragile and needs careful documentation.


## Development

### docs/development/setup.md
- **Contains**: Dev environment setup ‚Äî prerequisites, clone/install, configure, verify, project structure, workflow, IDE recommendations
- **Sources**: PROJECT_MAP.md ¬ß2, ¬ß3; AGENTS.md "Git Protocol"
- **Priority**: **P1**
- **Notes**: Overlaps with installation.md but focused on development workflow, not just running the pipeline.

### docs/development/conventions.md
- **Contains**: All coding standards ‚Äî Python conventions, pipeline script conventions, knowledge base conventions, embedding conventions, git conventions
- **Sources**: CLAUDE.md (all sections); AGENTS.md "Git Protocol"
- **Priority**: **P1**
- **Notes**: This is essentially a reformatted CLAUDE.md for external consumption.

### docs/development/testing.md
- **Contains**: Current state (0% coverage), planned testing strategy (3 phases), how to run/write tests, test data management
- **Sources**: PROJECT_MAP.md ¬ß9; ROADMAP F26, F27, F40; PREMORTEM M08
- **Priority**: **P2**
- **Notes**: Mostly forward-looking. Critical when tests start being written (v0.3).

### docs/development/contributing.md
- **Contains**: Contribution guide ‚Äî types of contributions, workflow, PR guidelines, knowledge base contribution rules
- **Sources**: AGENTS.md "Git Protocol"; CLAUDE.md; ROADMAP D04 (issue tracking decision)
- **Priority**: **P2**
- **Notes**: Not urgent since Douto is currently single-developer, but important for future.


## Roadmap

### docs/roadmap/index.md
- **Contains**: Product vision, current status summary, milestone overview table, pending decisions summary, top risks
- **Sources**: ROADMAP.md (all sections); PREMORTEM.md (top 5 risks)
- **Priority**: **P0**
- **Notes**: Important for investors and stakeholders. Keep updated as milestones are reached.

### docs/roadmap/milestones.md
- **Contains**: Detailed breakdown of each milestone (v0.2 through v1.0) ‚Äî features, criteria, prerequisites, estimates
- **Sources**: ROADMAP.md "Milestones" (verbatim, reformatted); INNOVATION_LAYER.md proposed milestones
- **Priority**: **P1**
- **Notes**: Include Gantt chart (Mermaid). Mark proposed milestones (v0.2.5, v0.3.5, v0.6) clearly.

### docs/roadmap/changelog.md
- **Contains**: History of changes ‚Äî initial commits, documentation session, pre-history
- **Sources**: git log; ROADMAP.md; session history
- **Priority**: **P2**
- **Notes**: Sparse for now (only 4 commits). Will grow over time.


## Reference

### docs/reference/glossary.md
- **Contains**: Definitions of legal terms (instituto, doutrina, ramo, etc.), technical terms (chunk, embedding, MOC, etc.), ecosystem terms (Valter, Juca, etc.), acronyms
- **Sources**: Knowledge base content; enrichment metadata schema; PROJECT_MAP.md ecosystem section
- **Priority**: **P0**
- **Notes**: Essential for LLM consumption and for non-legal-expert developers. Keep alphabetized within sections.

### docs/reference/faq.md
- **Contains**: Questions from three audiences (developers, lawyers, investors) with honest answers
- **Sources**: PROJECT_MAP.md; ROADMAP.md; PREMORTEM.md; INNOVATION_LAYER.md
- **Priority**: **P1**
- **Notes**: Include uncomfortable truths (missing prompt, unvalidated metadata, IP concerns). Honesty builds trust.

### docs/reference/troubleshooting.md
- **Contains**: Common problems and fixes ‚Äî pipeline startup, enrichment, search, embeddings, knowledge base
- **Sources**: PROJECT_MAP.md ¬ß10; PREMORTEM.md (all technical risks); pipeline script error handling
- **Priority**: **P1**
- **Notes**: The hardcoded paths issue alone will generate 90% of initial support requests.


## Summary

| Priority | Files | Description |
|----------|-------|-------------|
| **P0** | 8 files | Core docs needed for the site to be useful: index, introduction, quickstart, architecture overview, features index, enrichment, hybrid search, environment vars, roadmap index, glossary |
| **P1** | 13 files | Important supporting docs: installation, stack, decisions, diagrams, pipeline features (4), knowledge base features (2), integrations, dev setup, conventions, milestones, FAQ, troubleshooting |
| **P2** | 5 files | Can wait: settings, atomic notes, testing, contributing, changelog |

**Total: 26 skeleton files across 8 sections.**

### Recommended Writing Order (P0 first)

1. `reference/glossary.md` ‚Äî establishes shared vocabulary for all other docs
2. `getting-started/introduction.md` ‚Äî sets mental model
3. `architecture/overview.md` ‚Äî explains how everything fits together
4. `configuration/environment.md` ‚Äî unblocks anyone trying to run the project
5. `getting-started/quickstart.md` ‚Äî first hands-on experience
6. `features/index.md` ‚Äî feature inventory
7. `features/pipeline/enrichment.md` ‚Äî most critical (missing prompt makes this the only record)
8. `features/pipeline/hybrid-search.md` ‚Äî what users interact with
9. `roadmap/index.md` ‚Äî where the project is going
10. `index.md` ‚Äî home page (write last, since it links to everything)

---
# docs/architecture/decisions.md
---


# Architecture Decision Records

This page documents the "why" behind Douto's architectural choices. Each ADR follows a consistent format: context, decision, consequences, and current status.

## ADR-001: Legal-BERTimbau as Embedding Model

**Status:** Accepted (pending evaluation)

**Context:** Douto needs a Portuguese-language embedding model optimized for legal text. The model must produce embeddings suitable for cosine similarity search.

**Options considered:**
- `rufimelo/Legal-BERTimbau-sts-base` ‚Äî Portuguese legal BERT, 768-dim, trained on legal corpora
- `multilingual-e5-large` ‚Äî Multilingual, 1024-dim, general-purpose
- `Cohere embed-multilingual-v3.0` ‚Äî Commercial, excellent multilingual performance
- `text-embedding-ada-002` (OpenAI) ‚Äî Commercial, general-purpose

**Decision:** Legal-BERTimbau. It's the only model specifically trained on Portuguese legal text, it's free, and it runs locally without API calls.

**Consequences:**
- 512-token limit means chunks longer than ~2,000 characters are truncated
- Trained on PT-PT (Portugal), not PT-BR (Brazil) ‚Äî potential subtle mismatches
- No benchmark comparison exists for Douto's specific domain
- Evaluation is planned as [F40](../roadmap/milestones#v05--knowledge-graph--automation)

## ADR-002: MiniMax M2.5 for Chunk Enrichment

**Status:** Accepted (under review ‚Äî see [D06](#pending-decisions))

**Context:** Each chunk needs classification with structured legal metadata (instituto, tipo_conteudo, ramo, etc.). This requires an LLM capable of understanding legal concepts and producing valid JSON.

**Decision:** MiniMax M2.5, accessed via the Anthropic Python SDK with a custom `base_url`:

```python
# enrich_chunks.py line 30-31
MINIMAX_BASE_URL = "https://api.minimax.io/anthropic"
MINIMAX_MODEL = "MiniMax-M2.5"
```

**Consequences:**
- Low cost compared to Claude or GPT-4
- Uses an **undocumented compatibility layer** ‚Äî the Anthropic SDK was not designed to talk to MiniMax
- No quality validation has been performed on the enrichment output
- Breaking changes in either the Anthropic SDK or MiniMax API could silently break enrichment

## ADR-003: JSON Flat Files Instead of Vector Database

**Status:** Accepted (migration planned for v0.4)

**Context:** The pipeline produces embeddings that need to be stored and queried. Valter (the backend service) already runs Qdrant as its vector database.

**Decision:** Serialize everything as JSON files on disk:
- `embeddings_{area}.json` ‚Äî vectors as nested arrays
- `search_corpus_{area}.json` ‚Äî metadata as array of objects
- `bm25_index_{area}.json` ‚Äî tokenized documents as array of strings

**Consequences:**
- Zero infrastructure dependency ‚Äî no database to run
- Portable ‚Äî files can be copied anywhere
- **Does not scale** ‚Äî all data loaded into memory on each query (~1 GB for 31,500 chunks)
- No HNSW or FAISS indexing ‚Äî search is brute-force O(n) cosine similarity
- Migration to Qdrant planned as [M12](../roadmap/milestones#v04--senslegal-integration)

## ADR-004: Custom BM25 Implementation

**Status:** Accepted (optimization planned)

**Context:** Hybrid search needs a keyword component alongside semantic search. Options included the `rank-bm25` library, Elasticsearch, or a custom implementation.

**Decision:** Hand-rolled BM25 in `search_doutrina_v2.py` with parameters `k1=1.5, b=0.75`.

**Consequences:**
- No external dependency
- Document frequencies are **recalculated per query** ‚Äî O(N √ó T) where N=docs, T=query terms
- With 31,500 documents, each query recomputes token frequencies across the entire corpus
- Pre-computing the inverted index is planned as [M13](../roadmap/milestones#v04--senslegal-integration)

## ADR-005: Five-Pass Intelligent Chunking

**Status:** Accepted

**Context:** Legal books have complex structure that naive chunking (by token count or paragraph) destroys. Running headers from PDF extraction, footnotes separated from their text, law articles split from their commentary ‚Äî all degrade search quality.

**Decision:** A five-pass algorithm in `rechunk_v3.py` (890 lines):

1. **Section Split** ‚Äî detect sections using 14 regex patterns (markdown headers, chapters, articles, etc.)
2. **Classify** ‚Äî identify block type: noise, bibliography, summary, running header, content
3. **Merge Small** ‚Äî combine undersized chunks (< 1,500 chars) with neighbors
4. **Split Oversized** ‚Äî break chunks exceeding 15,000 chars at sentence boundaries
5. **Cleanup** ‚Äî remove empty chunks, normalize whitespace

**Consequences:**
- High-quality chunks that preserve legal context
- 890 lines of complex regex-based code with **0% test coverage**
- Running header detection uses frequency analysis ‚Äî heuristic, not deterministic
- Footnote aggregation and law article preservation are domain-specific and fragile

## ADR-006: Obsidian-Style Knowledge Base

**Status:** Accepted

**Context:** The knowledge base needs to be navigable by humans (in Obsidian) and by AI agents (via file reads).

**Decision:** Markdown files with YAML frontmatter, wikilinks (`[[target]]`), and a hierarchical MOC structure.

**Consequences:**
- Human-readable and version-controllable
- Compatible with Obsidian for visual navigation
- No programmatic query layer ‚Äî searching requires reading files or using the pipeline's search
- Frontmatter parsed by a custom regex parser (not PyYAML), which is fragile with special characters

## ADR-007: Separate Repository vs. Valter Module

**Status:** PENDING ‚Äî blocks v0.4

**Context:** Douto could exist as a module inside Valter (which already has Qdrant, Neo4j, and search infrastructure) or remain a separate repository with its own MCP server.

**Options:**
- **(A)** Separate repo with own MCP server
- **(B)** Module at `valter/stores/doutrina/` inside Valter
- **(C)** Separate repo, but Valter proxies all queries

This decision has not been made. See [D02](#pending-decisions).

## Pending Decisions

These decisions are blocking or influencing future milestones:

| # | Question | Options | Blocks |
|---|---------|---------|--------|
| D01 | Integration protocol: MCP stdio, MCP HTTP/SSE, REST, or JSON files? | A) MCP stdio B) MCP HTTP C) REST D) Keep JSON | v0.4 |
| D02 | Separate repo or Valter module? | A) Separate + MCP B) valter/stores/doutrina/ C) Separate + proxy | v0.4 |
| D03 | Atomic notes: auto-generated or curated? | A) Auto B) Manual C) Hybrid | v0.5 |
| D04 | Issue tracking: Linear (SEN-XXX) or GitHub Issues? | A) Linear B) GitHub C) Both | ‚Äî |
| D05 | Neo4j schema for doctrine nodes? | A) Doctrine node B) Authority + DoctrineClaim C) Reuse Criterion | v1.0 |
| D06 | Keep MiniMax M2.5 or migrate enrichment model? | A) Keep B) Claude C) Local model D) Evaluate later | ‚Äî |
| D07 | Project owner's actual priorities? | Roadmap is entirely inferred ‚Äî needs validation | All |
| D08 | Which LLM for the Synthesis Engine? | A) Claude B) MiniMax C) Other | v0.3.5 |
| D09 | Doctrine Briefs: on-demand or pre-computed? | A) On-demand B) Pre-computed C) Hybrid | v0.3.5 |

---
# docs/architecture/diagrams.md
---


# Architecture Diagrams

All visual representations of Douto's architecture, rendered in Mermaid.

## Pipeline Data Flow

The complete five-stage pipeline, showing data formats at each transition:

```mermaid
flowchart TD
    PDF["üìÑ Legal PDFs"]
    MD["üìù Markdown<br/>(chapters)"]
    CK["üì¶ Intelligent Chunks<br/>(1,500‚Äì15,000 chars)"]
    ECK["üè∑Ô∏è Enriched Chunks<br/>(+ 13 metadata fields)"]
    EMB["üî¢ Embeddings<br/>(768-dim float32)"]
    RES["üîç Ranked Results"]

    PDF -->|"process_books.py<br/>LlamaParse API"| MD
    MD -->|"rechunk_v3.py<br/>5-pass algorithm"| CK
    CK -->|"enrich_chunks.py<br/>MiniMax M2.5"| ECK
    ECK -->|"embed_doutrina.py<br/>Legal-BERTimbau"| EMB
    EMB -->|"search_doutrina_v2.py<br/>Semantic + BM25"| RES
```

## Component Diagram

All Douto components and their connections to external services and the sens.legal ecosystem:

```mermaid
graph TB
    subgraph "Douto ‚Äî Pipeline"
        PB["process_books.py<br/>PDF ‚Üí Markdown"]
        RC["rechunk_v3.py<br/>Chunking (890 lines)"]
        EN["enrich_chunks.py<br/>LLM Classification"]
        EM["embed_doutrina.py<br/>Embedding Generation"]
        SE["search_doutrina_v2.py<br/>Hybrid Search"]
    end

    subgraph "Douto ‚Äî Knowledge Base"
        IX["INDEX_DOUTO.md<br/>(8 domains)"]
        MC["MOC_CIVIL<br/>35 books, ~9,365 chunks"]
        MP["MOC_PROCESSUAL<br/>8 books, ~22,182 chunks"]
        ME["MOC_EMPRESARIAL<br/>7 books"]
        MCo["MOC_CONSUMIDOR<br/>(placeholder)"]
    end

    subgraph "sens.legal Ecosystem"
        VA["Valter<br/>Backend API<br/>FastAPI + Neo4j + Qdrant"]
        JU["Juca<br/>Frontend Hub<br/>Next.js 16"]
        LE["Leci<br/>Legislation<br/>Next.js 15 + PostgreSQL"]
    end

    subgraph "External Services"
        LP["LlamaParse API"]
        MM["MiniMax M2.5 API"]
        HF["HuggingFace Hub"]
    end

    PB --> RC --> EN --> EM --> SE
    SE -.->|"JSON files"| VA
    VA --> JU
    IX --> MC & MP & ME & MCo
    PB -.-> LP
    EN -.-> MM
    EM -.-> HF
```

## Ecosystem Position

How Douto fits within the sens.legal platform from the user's perspective:

```mermaid
graph LR
    USER["üë§ Lawyer"]
    JU["Juca<br/>Frontend"]
    VA["Valter<br/>Case Law<br/>23,400+ STJ"]
    LE["Leci<br/>Legislation"]
    DO["Douto<br/>Doctrine<br/>~50 books"]
    JO["Joseph<br/>Orchestrator"]

    USER --> JU
    JU --> VA
    JU --> LE
    JU --> DO
    JO -.->|"coordinates"| VA & LE & DO
    VA <-.->|"embeddings"| DO
```

## Knowledge Base Hierarchy

The skill graph structure from root to leaf:

```mermaid
graph TD
    IX["INDEX_DOUTO.md<br/>(root)"]

    MC["MOC_CIVIL.md<br/>35 books, ~9,365 chunks ‚úÖ"]
    MP["MOC_PROCESSUAL.md<br/>8 books, ~22,182 chunks ‚úÖ"]
    ME["MOC_EMPRESARIAL.md<br/>7 books ‚úÖ"]
    MCo["MOC_CONSUMIDOR.md<br/>(placeholder) üî®"]
    MT["MOC_TRIBUTARIO ‚ùå"]
    MCn["MOC_CONSTITUCIONAL ‚ùå"]
    MCp["MOC_COMPLIANCE ‚ùå"]
    MS["MOC_SUCESSOES ‚ùå"]

    ND["nodes/<br/>(atomic notes ‚Äî planned)"]

    IX --> MC & MP & ME & MCo & MT & MCn & MCp & MS
    MC & MP & ME -.-> ND
```

## Enrichment Metadata Schema

The 13 metadata fields added to each chunk during enrichment:

```mermaid
erDiagram
    CHUNK {
        string knowledge_id
        string titulo
        string livro_titulo
        string autor
        int chunk_numero
        int chunk_total
    }

    ENRICHMENT {
        string categoria
        list instituto
        list sub_instituto
        list tipo_conteudo
        list fase
        string ramo
        list fontes_normativas
        string confiabilidade
        string utilidade
        boolean jurisdicao_estrangeira
        string justificativa
        string status_enriquecimento
        string modelo_enriquecimento
    }

    CHUNK ||--|| ENRICHMENT : "enriched by"
```

Field descriptions:

| Field | Type | Example Values |
|-------|------|---------------|
| `instituto` | `list[str]` | `["exceptio non adimpleti contractus", "contrato bilateral"]` |
| `sub_instituto` | `list[str]` | `["inadimplemento relativo"]` |
| `tipo_conteudo` | `list[str]` | `["definicao", "requisitos", "jurisprudencia_comentada"]` |
| `fase` | `list[str]` | `["formacao", "execucao", "extincao"]` |
| `ramo` | `str` | `"direito_civil"` |
| `fontes_normativas` | `list[str]` | `["CC art. 476", "CC art. 477"]` |
| `confiabilidade` | `str` | `"alta"`, `"media"`, `"baixa"` |
| `categoria` | `str` | `"doutrina"` |

## Search Architecture

How hybrid search combines semantic and keyword scoring:

```mermaid
flowchart LR
    Q["Query"]
    SEM["Semantic Search<br/>(cosine similarity<br/>on 768-dim vectors)"]
    BM["BM25 Search<br/>(keyword matching<br/>k1=1.5, b=0.75)"]
    NORM["Min-Max<br/>Normalization"]
    COMB["Weighted<br/>Combination<br/>(0.7 sem + 0.3 bm25)"]
    FILT["Metadata<br/>Filtering<br/>(instituto, tipo,<br/>ramo, livro, fase)"]
    RES["Ranked<br/>Results"]

    Q --> SEM & BM
    SEM --> NORM
    BM --> NORM
    NORM --> COMB --> FILT --> RES
```

## Planned Architecture (v0.4+)

When the MCP server is implemented, Douto will be queryable in real time:

```mermaid
graph TB
    subgraph "Douto MCP Server (planned)"
        T1["search_doutrina"]
        T2["get_chunk"]
        T3["list_areas"]
    end

    subgraph "Consumers"
        VA["Valter"]
        CD["Claude Desktop"]
        CC["Claude Code"]
    end

    subgraph "Storage (planned)"
        QD["Qdrant<br/>(vector DB)"]
    end

    VA -->|"MCP protocol"| T1 & T2 & T3
    CD -->|"MCP stdio"| T1 & T2 & T3
    CC -->|"MCP stdio"| T1 & T2 & T3
    T1 & T2 & T3 --> QD
```

---
# docs/architecture/overview.md
---


# Architecture Overview

Douto operates in two complementary modes: a batch ETL pipeline that transforms legal PDFs into searchable data, and a markdown-based knowledge graph navigable by humans and AI agents. It is not a web application or a running service ‚Äî it is a set of processing tools and a structured knowledge base.

## Architectural Pattern

**Batch Processing Pipeline** ‚Äî Five independent Python scripts executed sequentially. Each reads from disk, processes data, and writes back to disk. No database, no message queue, no orchestrator.

**Markdown Knowledge Graph** ‚Äî An Obsidian-compatible hierarchy using YAML frontmatter, wikilinks, and Maps of Content (MOCs). Designed for dual consumption: human navigation in Obsidian and programmatic querying by AI agents.

## Pipeline Data Flow

```mermaid
flowchart TD
    PDF["üìÑ Legal PDFs<br/>(staging/input/)"]
    PB["process_books.py<br/>LlamaParse API"]
    RC["rechunk_v3.py<br/>5-pass algorithm"]
    EN["enrich_chunks.py<br/>MiniMax M2.5 LLM"]
    EM["embed_doutrina.py<br/>Legal-BERTimbau"]
    SE["search_doutrina_v2.py<br/>Hybrid: Semantic + BM25"]
    JSON["üì¶ JSON Artifacts<br/>embeddings + corpus + BM25"]

    PDF -->|"PDF files"| PB
    PB -->|"markdown<br/>(chapters)"| RC
    RC -->|"intelligent<br/>chunks"| EN
    EN -->|"enriched chunks<br/>(13 metadata fields)"| EM
    EM -->|"768-dim vectors"| JSON
    JSON --> SE
```

Each arrow represents a file-system handoff ‚Äî there is no in-memory pipeline or streaming. Scripts can be re-run independently with `--force` or `--dry-run`.

## Knowledge Base Structure

The knowledge base has three layers:

| Layer | File | Purpose | Status |
|-------|------|---------|--------|
| **Root** | `knowledge/INDEX_DOUTO.md` | Skill graph entry point ‚Äî maps 8 legal domains | Active |
| **Domain Maps** | `knowledge/mocs/MOC_*.md` | Books per domain with metadata and status | 3 active, 1 placeholder, 4 missing |
| **Atomic Notes** | `knowledge/nodes/*.md` | One note per legal concept (instituto) | Planned (directory exists, no content) |

The hierarchy uses Obsidian conventions: `[[wikilinks]]` for navigation, YAML frontmatter for structured metadata.

## Output Artifacts

The pipeline produces three JSON files per legal area (e.g., `contratos`, `processo_civil`):

| File | Contents | Estimated Size |
|------|----------|---------------|
| `embeddings_{area}.json` | `doc_ids[]` + `embeddings[][]` (768-dim float32 vectors) | ~500 MB for 31,500 chunks |
| `search_corpus_{area}.json` | Full metadata per chunk (title, author, instituto, tipo, etc.) | ~200 MB |
| `bm25_index_{area}.json` | `doc_ids[]` + `documents[]` (tokenized text for BM25) | ~300 MB |

These files are loaded entirely into memory by `search_doutrina_v2.py` at startup.

## Position in the sens.legal Ecosystem

```mermaid
graph TB
    subgraph "Douto ‚Äî Pipeline"
        PB["process_books.py"]
        RC["rechunk_v3.py"]
        EN["enrich_chunks.py"]
        EM["embed_doutrina.py"]
        SE["search_doutrina_v2.py"]
    end

    subgraph "Douto ‚Äî Knowledge Base"
        IX["INDEX_DOUTO.md"]
        MC["MOC_CIVIL<br/>35 books"]
        MP["MOC_PROCESSUAL<br/>8 books"]
        ME["MOC_EMPRESARIAL<br/>7 books"]
    end

    subgraph "sens.legal Ecosystem"
        VA["Valter<br/>FastAPI + Neo4j + Qdrant"]
        JU["Juca<br/>Next.js Frontend"]
        LE["Leci<br/>Legislation"]
    end

    subgraph "External Services"
        LP["LlamaParse API"]
        MM["MiniMax M2.5 API"]
        HF["HuggingFace<br/>Legal-BERTimbau"]
    end

    PB --> RC --> EN --> EM --> SE
    SE -.->|"JSON files"| VA
    VA --> JU
    IX --> MC & MP & ME
    PB -.-> LP
    EN -.-> MM
    EM -.-> HF
```

Currently, Douto integrates with the ecosystem via JSON files deposited in a shared directory. There is no API, MCP server, or real-time query capability. MCP integration is planned for [v0.4](../roadmap/milestones#v04--senslegal-integration).

## Design Principles

From `CLAUDE.md`, in priority order:

1. **Correctness** ‚Äî especially doctrinal data, citations, legal metadata
2. **Simplicity** ‚Äî code another agent understands without context
3. **Maintainability** ‚Äî easy to change without breaking
4. **Reversibility** ‚Äî decisions that can be undone
5. **Performance** ‚Äî optimize only with evidence of a problem

Operational principles:

- **Idempotent** ‚Äî every script is safe to re-run (skip markers, `--force` to override)
- **Dry-run first** ‚Äî every script supports `--dry-run`
- **Structured logging** ‚Äî events go to `processing_log.jsonl`

## Known Limitations

These are architectural constraints, not bugs. Each has a tracking reference:

| Limitation | Impact | Tracking |
|-----------|--------|----------|
| No database ‚Äî JSON flat files | Doesn't scale past ~100 books, full load into memory | [ADR-003](decisions#adr-003-json-flat-files-instead-of-vector-database) |
| No API or MCP server | No real-time queries from other agents | [F30, v0.4](../roadmap/milestones#v04--senslegal-integration) |
| No CI/CD | No automated testing or linting | [F39, v0.5](../roadmap/milestones#v05--knowledge-graph--automation) |
| Hardcoded paths in 2 scripts | Pipeline runs only on creator's machine | [F22, v0.2](../roadmap/milestones#v02--stable-pipeline) |
| 0% test coverage | Regressions undetectable except by manual inspection | [F26-F27, v0.3](../roadmap/milestones#v03--quality--coverage) |
| Missing enrichment prompt | `enrich_prompt.md` not in repo ‚Äî enrichment unreproducible | [M01](../roadmap/milestones#v02--stable-pipeline) |

---
# docs/architecture/stack.md
---


# Technology Stack

Every technology used in Douto, why it was chosen, and where it's used.

## Languages

| Language | Version | Usage |
|----------|---------|-------|
| Python 3 | 3.10+ (required for `tuple[dict, str]` type hints) | All 5 pipeline scripts |
| Markdown | ‚Äî | Knowledge base (Obsidian conventions, YAML frontmatter, wikilinks) |

## Core Dependencies

From `pipeline/requirements.txt`:

| Package | Version | Purpose | Used In |
|---------|---------|---------|---------|
| `sentence-transformers` | unpinned | Embedding generation via Legal-BERTimbau | `embed_doutrina.py` |
| `torch` | unpinned | ML backend for sentence-transformers | `embed_doutrina.py` |
| `numpy` | unpinned | Vector math (cosine similarity, score normalization) | `embed_doutrina.py`, `search_doutrina_v2.py` |
| `anthropic` | unpinned | Python SDK used as HTTP client for MiniMax M2.5 API | `enrich_chunks.py` |
| `llama-parse` | unpinned | PDF ‚Üí markdown extraction via LlamaIndex cloud | `process_books.py` |

:::caution
All versions are unpinned. Running `pip install -r requirements.txt` at different times may produce different environments. Pinning is planned for [F24](../roadmap/milestones#v02--stable-pipeline).
:::

## ML Models

| Model | Provider | Dimensions | Max Tokens | Purpose |
|-------|----------|-----------|------------|---------|
| `rufimelo/Legal-BERTimbau-sts-base` | HuggingFace | 768 | 512 | Semantic embeddings for legal text |
| MiniMax-M2.5 | MiniMax (via Anthropic SDK) | ‚Äî | ~8,000 | Chunk enrichment and classification |
| LlamaParse | LlamaIndex | ‚Äî | ‚Äî | PDF ‚Üí markdown extraction |

**Legal-BERTimbau** was trained on Portuguese legal corpora. It's the standard choice for Portuguese legal NLP, though it was trained on PT-PT (Portugal), not PT-BR (Brazil). No benchmark comparison with alternatives (multilingual-e5, nomic-embed, Cohere embed v3) has been performed for Douto's specific domain.

## External Services

| Service | Purpose | Authentication | Required For |
|---------|---------|---------------|-------------|
| LlamaParse API | PDF ‚Üí markdown conversion | `LLAMA_CLOUD_API_KEY` | `process_books.py` only |
| MiniMax M2.5 API | Chunk classification with legal metadata | `MINIMAX_API_KEY` | `enrich_chunks.py` only |
| HuggingFace Hub | Model download (auto on first run) | None (public model) | `embed_doutrina.py` (first run) |

:::danger
The MiniMax integration uses the Anthropic Python SDK with `base_url="https://api.minimax.io/anthropic"`. This is an undocumented compatibility layer ‚Äî not officially supported by either Anthropic or MiniMax. It can break without notice.
:::

## Standard Library Usage

The following stdlib modules are used across pipeline scripts ‚Äî no external packages needed:

`re`, `json`, `pathlib`, `argparse`, `asyncio`, `shutil`, `threading`, `math`, `collections`, `os`, `sys`, `time`, `datetime`

Notably, `rechunk_v3.py` (the most complex script at 890 lines) uses **only** stdlib modules.

:::note
The project uses a custom regex-based YAML parser (`parse_frontmatter()`) instead of `PyYAML`. This parser is duplicated in `enrich_chunks.py` and `embed_doutrina.py`. Extracting it to a shared `utils.py` and optionally replacing with PyYAML is tracked as [F23](../roadmap/milestones#v02--stable-pipeline).
:::

## Infrastructure

| Category | Current State | Planned |
|----------|-------------|---------|
| Build system | None ‚Äî scripts run manually | Makefile ([F31](../roadmap/milestones#v03--quality--coverage)) |
| Database | None ‚Äî JSON flat files | Vector DB migration ([M12](../roadmap/milestones#v04--senslegal-integration)) |
| Containerization | None | Docker ([F38](../roadmap/milestones#v10--integrated-platform)) |
| CI/CD | None | GitHub Actions ([F39](../roadmap/milestones#v05--knowledge-graph--automation)) |
| Linting | None | ruff ([F32](../roadmap/milestones#v03--quality--coverage)) |
| Testing | None (0% coverage) | pytest ([F26-F27](../roadmap/milestones#v03--quality--coverage)) |

## Dependency Graph

```mermaid
graph LR
    LP["LlamaParse API"] --> PB["process_books.py"]
    PB -->|"stdlib only"| RC["rechunk_v3.py"]
    AN["anthropic SDK"] --> EN["enrich_chunks.py"]
    EN -->|"via base_url"| MM["MiniMax API"]
    ST["sentence-transformers"] --> EM["embed_doutrina.py"]
    TO["torch"] --> ST
    NP["numpy"] --> EM
    NP --> SE["search_doutrina_v2.py"]
    HF["HuggingFace Hub"] -.->|"model download"| ST
```

---
# docs/configuration/environment.md
---


# Environment Variables

Every environment variable used by Douto, which scripts read it, default values, and whether it is required.

## Variable Reference

| Variable | Required | Default | Used In | Description |
|----------|----------|---------|---------|-------------|
| `VAULT_PATH` | Yes | varies (see Known Issues) | `enrich_chunks.py`, `embed_doutrina.py` | Path to the Obsidian vault containing processed markdown chunks in `Knowledge/_staging/processed/` |
| `OUTPUT_PATH` | No | `~/.openclaw/workspace/juca/data` | `embed_doutrina.py` | Directory where embedding, corpus, and BM25 index JSON files are written |
| `DATA_PATH` | No | `~/.openclaw/workspace/juca/data` | `search_doutrina_v2.py` | Directory containing pre-built search data (embeddings, corpus, BM25 index) |
| `MINIMAX_API_KEY` | Yes (for enrichment) | -- | `enrich_chunks.py` | API key for MiniMax M2.5, used via the Anthropic SDK with a custom `base_url` |
| `LLAMA_CLOUD_API_KEY` | Yes (for extraction) | -- | `process_books.py` | API key for LlamaParse (LlamaIndex), loaded implicitly by the SDK |

:::caution[Two scripts use hardcoded paths]
`process_books.py` and `rechunk_v3.py` do **not** read `VAULT_PATH` from the environment. They have hardcoded absolute paths. Setting `VAULT_PATH` has no effect on these scripts until [F22](/docs/roadmap/milestones/#v02--stable-pipeline) is completed.
:::

### Optional / Implicit Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `HF_HOME` | No | `~/.cache/huggingface` | Override the HuggingFace cache directory where `Legal-BERTimbau` is downloaded |
| `SENTENCE_TRANSFORMERS_HOME` | No | `$HF_HOME` | Override the sentence-transformers model cache specifically |
| `CUDA_VISIBLE_DEVICES` | No | all GPUs | Restrict which GPU(s) PyTorch uses for embedding generation |

## Known Issues with Paths

:::danger[Three different default paths across scripts]
The pipeline scripts currently reference three different hardcoded environments, making the pipeline runnable **only on the creator's machine**:

| Script | Path | Environment |
|--------|------|-------------|
| `process_books.py` (line 27) | `/home/sensd/.openclaw/workspace/vault` | Linux native |
| `rechunk_v3.py` (line 29) | `/mnt/c/Users/sensd/vault` | WSL (Windows mount) |
| `enrich_chunks.py` (line 25) | `os.environ.get("VAULT_PATH", "/mnt/c/Users/sensd/vault")` | WSL fallback |
| `embed_doutrina.py` (line 21) | `os.environ.get("VAULT_PATH", "/mnt/c/Users/sensd/vault")` | WSL fallback |
| `search_doutrina_v2.py` (line 23) | `os.environ.get("DATA_PATH", "/home/sensd/.openclaw/workspace/juca/data")` | Linux native |

This inconsistency is the **#1 technical debt item** (rated P0) and is tracked as F22 in the roadmap. The resolution is to standardize all scripts to use `os.environ.get()` with a single consistent fallback.
:::

### Path Relationship

The scripts expect a specific directory structure under `VAULT_PATH`:

```
$VAULT_PATH/
  Knowledge/
    _staging/
      input/         # PDFs to process (process_books.py reads from here)
      processed/     # Markdown chapters + enriched chunks (all scripts)
      failed/        # Failed PDF extractions
      processing_log.jsonl
```

`OUTPUT_PATH` and `DATA_PATH` both default to the same directory (`~/.openclaw/workspace/juca/data`), where the embedding and search index files live:

```
$OUTPUT_PATH/  (== $DATA_PATH/)
  embeddings_doutrina.json
  search_corpus_doutrina.json
  bm25_index_doutrina.json
  embeddings_processo_civil.json
  search_corpus_processo_civil.json
  bm25_index_processo_civil.json
```

## Setup by Use Case

### Search Only (querying existing data)

You only need `DATA_PATH`:

```bash
export DATA_PATH="/path/to/juca/data"
python3 pipeline/search_doutrina_v2.py --interativo
```

### Embedding Generation

You need `VAULT_PATH` (source chunks) and `OUTPUT_PATH` (where to write embeddings):

```bash
export VAULT_PATH="/path/to/vault"
export OUTPUT_PATH="/path/to/juca/data"
python3 pipeline/embed_doutrina.py
```

### Chunk Enrichment

You need `VAULT_PATH` (source chunks) and `MINIMAX_API_KEY`:

```bash
export VAULT_PATH="/path/to/vault"
export MINIMAX_API_KEY="your-minimax-api-key"
python3 pipeline/enrich_chunks.py all
```

### Full Pipeline (PDF to search)

All variables are required:

```bash
export VAULT_PATH="/path/to/vault"
export OUTPUT_PATH="/path/to/juca/data"
export MINIMAX_API_KEY="your-minimax-api-key"
export LLAMA_CLOUD_API_KEY="your-llamaparse-key"

python3 pipeline/process_books.py          # PDF -> markdown
python3 pipeline/rechunk_v3.py             # markdown -> chunks
python3 pipeline/enrich_chunks.py all      # chunks -> classified
python3 pipeline/embed_doutrina.py         # chunks -> embeddings
python3 pipeline/search_doutrina_v2.py -i  # interactive search
```

:::caution
Remember that `process_books.py` and `rechunk_v3.py` currently **ignore** `VAULT_PATH` and use hardcoded paths. You must edit these files directly or wait for [F22](/docs/roadmap/milestones/#v02--stable-pipeline).
:::

## Example .env File

:::note
Douto does **not** use `python-dotenv`. Environment variables must be set in your shell (e.g., `export` in bash/zsh, or sourced from a file). The `.env` file below is a template for your shell profile or a `source`-able file.
:::

```bash
# Douto ‚Äî Environment Variables
# Copy this to .env and run: source .env

# Path to the Obsidian vault with Knowledge/_staging/ structure
export VAULT_PATH="/path/to/your/vault"

# Where embedding and search index JSONs are written/read
export OUTPUT_PATH="/path/to/juca/data"
export DATA_PATH="/path/to/juca/data"  # typically same as OUTPUT_PATH

# API Keys ‚Äî required for specific pipeline stages
export MINIMAX_API_KEY="your-minimax-api-key"        # enrichment (enrich_chunks.py)
export LLAMA_CLOUD_API_KEY="your-llamaparse-key"      # PDF extraction (process_books.py)

# Optional: HuggingFace model cache (Legal-BERTimbau is ~500MB)
# export HF_HOME="/path/to/hf-cache"

# Optional: GPU control
# export CUDA_VISIBLE_DEVICES="0"  # use only GPU 0
```

:::tip
Add `.env` to your `.gitignore` to avoid committing API keys. The project's `.gitignore` already excludes `.env` files.
:::

---
# docs/configuration/integrations.md
---


# External Integrations

How Douto connects to external services and the sens.legal ecosystem.

## LlamaParse (PDF Extraction)

**Service:** [LlamaParse](https://cloud.llamaindex.ai/) by LlamaIndex
**Purpose:** Convert legal PDF textbooks into structured markdown
**Used by:** `process_books.py`
**Auth:** `LLAMA_CLOUD_API_KEY` environment variable (loaded implicitly by the SDK)

### Setup

1. Create a free account at [cloud.llamaindex.ai](https://cloud.llamaindex.ai/).
2. Generate an API key from the dashboard.
3. Set the environment variable:

```bash
export LLAMA_CLOUD_API_KEY="llx-your-key-here"
```

### Tiers

LlamaParse offers three processing tiers. The default in Douto is `cost_effective`:

| Tier | Best for | Speed | Cost |
|------|----------|-------|------|
| `agentic` | Scanned PDFs, complex tables, multi-column layouts | Slowest | Highest |
| `cost_effective` | Clean-text legal textbooks (default) | Medium | Medium |
| `fast` | Simple text-only documents | Fastest | Lowest |

Override the tier per run:

```bash
python3 pipeline/process_books.py --tier agentic livro.pdf
```

### Usage Notes

- PDF extraction is a **one-time operation** per book. Once converted to markdown, the original PDF is not needed again by the pipeline.
- Processed markdown is saved to `$VAULT_PATH/Knowledge/_staging/processed/{slug}/`.
- If extraction fails, the PDF is moved to `$VAULT_PATH/Knowledge/_staging/failed/`.
- LlamaParse uses `asyncio` internally. This is the only async component in the pipeline.

:::tip
Since extraction is one-time, even if LlamaParse changes pricing or becomes unavailable, previously extracted books are unaffected. Only processing new books requires an active API key.
:::


## MiniMax M2.5 (Chunk Enrichment)

**Service:** MiniMax M2.5 LLM
**Purpose:** Classify chunks with structured legal metadata (instituto, tipo_conteudo, ramo, etc.)
**Used by:** `enrich_chunks.py`
**Auth:** `MINIMAX_API_KEY` environment variable

### Setup

1. Obtain an API key from [MiniMax](https://www.minimax.io/).
2. Set the environment variable:

```bash
export MINIMAX_API_KEY="your-minimax-api-key"
```

### The Anthropic SDK Hack

:::caution[Fragile integration -- not officially supported]
Douto uses the **Anthropic Python SDK** to call MiniMax's API. This works because MiniMax exposes an Anthropic-compatible endpoint, but it is **not an officially documented or supported integration** by either Anthropic or MiniMax.

```python
# From enrich_chunks.py (line 30-31)
MINIMAX_BASE_URL = "https://api.minimax.io/anthropic"
MINIMAX_MODEL = "MiniMax-M2.5"

# The client is instantiated as:
client = anthropic.Anthropic(
    api_key=os.environ["MINIMAX_API_KEY"],
    base_url=MINIMAX_BASE_URL,
)
```

Any change to MiniMax's API compatibility layer, or a breaking change in the Anthropic SDK, will silently break enrichment. There will be no deprecation warning.
:::

### Concurrency Settings

Enrichment runs with 5 concurrent threads and a 0.5-second delay between requests to avoid rate limiting:

| Parameter | Value |
|-----------|-------|
| `WORKERS` | 5 threads |
| `DELAY_BETWEEN_REQUESTS` | 0.5 seconds |
| Model | `MiniMax-M2.5` |

### Missing Prompt File

:::danger
The enrichment prompt file (`enrich_prompt.md`) is referenced in the code at line 27 but is **not present in the repository**. Without this file, `enrich_chunks.py` will exit with an error. Recovering or reconstructing this prompt is tracked as mitigation action M01 (P0 priority).
:::

### Pending Decision: D06

The choice of MiniMax M2.5 as the enrichment model is under review. Options being evaluated:

| Option | Pros | Cons |
|--------|------|------|
| **Keep MiniMax M2.5** | Works, cheap | Fragile SDK hack, generic model |
| Migrate to Claude | Ecosystem consistency | Higher cost |
| Local model | Zero cost, no dependency | Slower, setup complexity |
| Evaluate later | No effort now | Risk compounds |


## HuggingFace (Embedding Model)

**Service:** HuggingFace Hub (public model)
**Purpose:** Download and cache the Legal-BERTimbau embedding model
**Used by:** `embed_doutrina.py`, `search_doutrina_v2.py`
**Auth:** None required (public model)

### Model Details

| Property | Value |
|----------|-------|
| Model ID | `rufimelo/Legal-BERTimbau-sts-base` |
| Dimensions | 768 |
| Max tokens | 512 |
| Language | Portuguese (trained on PT-PT legal corpus) |
| Size on disk | ~500 MB |
| License | Open source |

### Setup

The model is **automatically downloaded** on first run by the `sentence-transformers` library. No manual setup is needed.

To pre-download the model (useful for offline environments or Docker):

```bash
python3 -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('rufimelo/Legal-BERTimbau-sts-base')"
```

To control where the model is cached:

```bash
export HF_HOME="/path/to/cache"
# or specifically:
export SENTENCE_TRANSFORMERS_HOME="/path/to/cache"
```

:::note
The model was trained on Portuguese (PT-PT) legal text, not specifically Brazilian Portuguese. This may cause minor differences in semantic similarity for BR-specific legal terms (see PREMORTEM PF04). No benchmark comparison against alternatives exists yet (planned in F40).
:::


## sens.legal Ecosystem Integration

### Current State (v0.1)

Integration with the broader sens.legal ecosystem is currently through **static JSON files**:

```
embed_doutrina.py
      |
      v
embeddings_doutrina.json  ‚îÄ‚îÄ‚îÄ deposited in ‚îÄ‚îÄ‚Üí  Juca/Valter data directory
search_corpus_doutrina.json                      ($OUTPUT_PATH)
bm25_index_doutrina.json
```

- No real-time query capability from other agents.
- No API or protocol.
- Valter and Juca read the JSON files from a shared filesystem path.
- Updates require re-running the embedding pipeline and restarting consumers.

### Ecosystem Components

| Component | Role | Stack | Douto's relationship |
|-----------|------|-------|---------------------|
| **Valter** | Backend API -- STJ case law, knowledge graph, vector search | FastAPI, PostgreSQL, Qdrant, Neo4j, Redis | Primary consumer of Douto's embeddings |
| **Juca** | Frontend -- user interface for lawyers | Next.js | Accesses doctrine through Valter |
| **Leci** | Legislation service | Next.js, PostgreSQL, Drizzle | Future cross-reference target (F35) |
| **Joseph** | Orchestrator -- coordinates agents | -- | Future coordination with Douto queries |

### Planned Integration (v0.4)

> **Planned Feature** -- MCP server for doctrine search is on the roadmap (F30) but not yet implemented.

The v0.4 milestone will establish programmatic integration between Douto and the sens.legal ecosystem:

**MCP Server** with at least 3 tools:

| Tool | Description |
|------|-------------|
| `search_doutrina` | Hybrid search across doctrine corpus |
| `get_chunk` | Retrieve a specific chunk by ID with full metadata |
| `list_areas` | List available legal domains with corpus statistics |

**Protocol Decision (D01) -- not yet resolved:**

| Option | Description | Pros | Cons |
|--------|-------------|------|------|
| MCP stdio | Standard MCP transport | Aligned with Valter's MCP | Process-per-query overhead |
| MCP HTTP/SSE | Persistent MCP connection | More flexible, lower latency | More infrastructure |
| REST API (FastAPI) | Conventional HTTP API | Simple, well-understood | Not aligned with MCP ecosystem |
| Keep JSON files | Current approach | Zero effort | No real-time queries, doesn't scale |

**Architecture Decision (D02) -- not yet resolved:**

Whether Douto remains an independent service or is absorbed as a module within Valter (`valter/stores/doutrina/`). This decision blocks v0.4.

:::tip
Valter already has Qdrant (vector DB) and Neo4j (knowledge graph) infrastructure. When Douto integrates, it should leverage these existing services rather than maintaining its own JSON-based storage.
:::

### Integration Diagram

```mermaid
graph TB
    subgraph "Current (v0.1)"
        ED["embed_doutrina.py"] -->|JSON files| DIR["Shared directory"]
        DIR -->|reads| VA1["Valter"]
    end

    subgraph "Planned (v0.4)"
        MCP["Douto MCP Server"] -->|search_doutrina| VA2["Valter"]
        MCP -->|get_chunk| VA2
        MCP -->|list_areas| VA2
        VA2 -->|via Valter| JU["Juca"]
        CD["Claude Desktop"] -->|MCP| MCP
    end
```

---
# docs/configuration/settings.md
---


# Settings & Configuration

Douto's configurable parameters. Most are currently hardcoded in the source files and need to be externalized in future milestones.

## Pipeline Settings

### process_books.py -- PDF Extraction

| Setting | Value | Location | Configurable |
|---------|-------|----------|--------------|
| `DEFAULT_TIER` | `"cost_effective"` | Line 37 | Yes, via `--tier` CLI arg |
| Chapter split pattern | H1/H2 markdown headers | Hardcoded in `split_into_chapters()` | No |
| Input directory | `$VAULT_PATH/Knowledge/_staging/input/` | Hardcoded | No |
| Output directory | `$VAULT_PATH/Knowledge/_staging/processed/` | Hardcoded | No |
| Failed directory | `$VAULT_PATH/Knowledge/_staging/failed/` | Hardcoded | No |

**LlamaParse Tiers:**

| Tier | Quality | Speed | Cost | When to use |
|------|---------|-------|------|-------------|
| `agentic` | Best | Slowest | Highest | Scanned PDFs, complex layouts, tables |
| `cost_effective` | Good (default) | Medium | Medium | Clean text PDFs, most legal textbooks |
| `fast` | Basic | Fastest | Lowest | Simple text-only documents |

```bash
python3 pipeline/process_books.py --tier agentic livro.pdf
```


### rechunk_v3.py -- Intelligent Chunking

| Setting | Value | Location | Configurable |
|---------|-------|----------|--------------|
| `MIN_CHUNK_CHARS` | `1500` | Line 32 | Yes, via `--min-chars` CLI arg |
| `MAX_CHUNK_CHARS` | `15000` | Line 33 | No (hardcoded) |
| `SECTION_PATTERNS` | 16 regex patterns | Lines 41-72 | No (hardcoded) |
| Running header threshold | Frequency-based detection | Hardcoded heuristic | No |

**Section Detection Patterns (16 total):**

The rechunker recognizes the following structural patterns in legal markdown:

| Pattern Type | Example | Regex ID |
|-------------|---------|----------|
| Markdown headers | `## Section Title` | `md_header` |
| English chapters | `**Chapter 5:** Title` | `chapter_en` |
| Portuguese chapters | `**Capitulo X** Title` | `capitulo_pt` |
| All-caps CHAPTER | `CHAPTER 5 Title` | `chapter_caps` |
| All-caps CAPITULO | `CAPITULO X` | `capitulo_caps` |
| Titulo (title/book) | `TITULO VI` | `titulo` |
| Parte (part) | `PARTE GERAL` | `parte` |
| English Part | `Part One` | `part_en` |
| Legal article | `Art. 481.` or `### Art. 481` | `artigo` |
| Portuguese section | `Secao I` | `secao` |
| English section | `Section X` | `section_en` |
| Numbered caps | `1. TITULO EM MAIUSCULAS` | `numbered_caps` |
| Numbered bold | `**1.** Title` | `numbered_bold` |
| All-caps title line | `RESPONSABILIDADE CIVIL OBJETIVA` | `allcaps_title` |
| Bold caps title | `**SOME TITLE HERE**` | `bold_caps_title` |

:::note
Patterns are evaluated in order. The first match wins. This means markdown headers (`## Title`) take priority over all other patterns.
:::

**Chunking Rules (hardcoded, not configurable):**

- Footnotes are grouped with their parent paragraph
- Law articles + commentary are never separated
- Practical examples stay with the principle they illustrate
- Running headers (repeated title/author) are filtered by frequency
- Bibliographies are extracted as separate chunks with type `"bibliografia"`
- Prefaces, acknowledgments, cataloging cards are filtered as noise


### enrich_chunks.py -- Metadata Enrichment

| Setting | Value | Location | Configurable |
|---------|-------|----------|--------------|
| `MINIMAX_BASE_URL` | `"https://api.minimax.io/anthropic"` | Line 30 | No (hardcoded) |
| `MINIMAX_MODEL` | `"MiniMax-M2.5"` | Line 31 | No (hardcoded) |
| `WORKERS` | `5` | Line 34 | No (hardcoded) |
| `DELAY_BETWEEN_REQUESTS` | `0.5` seconds | Line 35 | No (hardcoded) |
| `PROMPT_PATH` | `pipeline/enrich_prompt.md` | Line 27 | No (hardcoded) |

:::danger[Missing prompt file]
`enrich_prompt.md` is referenced at line 27 but is **not present in the repository**. This means enrichment cannot be run on new chunks until the prompt is recovered or reconstructed. This is tracked as mitigation action M01 (P0 priority).
:::

**Enrichment Metadata Fields:**

The LLM classifies each chunk into these structured fields:

| Field | Type | Description | Example values |
|-------|------|-------------|----------------|
| `categoria` | string | High-level category | `"doutrina"`, `"legislacao_comentada"` |
| `instituto` | list[string] | Legal institutes | `["boa-fe objetiva", "exceptio non adimpleti"]` |
| `tipo_conteudo` | string | Content type | `"definicao"`, `"requisitos"`, `"exemplo"`, `"critica"` |
| `fase` | string | Procedural phase | `"formacao"`, `"execucao"`, `"extincao"` |
| `ramo` | string | Branch of law | `"civil"`, `"processual_civil"`, `"empresarial"` |
| `fontes_normativas` | list[string] | Statutory references | `["CC art. 476", "CPC art. 300"]` |


### embed_doutrina.py -- Embedding Generation

| Setting | Value | Location | Configurable |
|---------|-------|----------|--------------|
| `MODEL_NAME` | `"rufimelo/Legal-BERTimbau-sts-base"` | Line 24 | No (hardcoded) |
| Embedding dimensions | 768 | Model-determined | No |
| `BATCH_SIZE` | `32` | Line 25 | No (hardcoded) |
| `MAX_TOKENS` | `512` | Line 26 | No (hardcoded, BERTimbau limit) |
| Normalization | `True` | Hardcoded | No |

**Text Composition Template:**

Embeddings are generated from a composed text, not the raw chunk body:

```
[categoria | instituto_1, instituto_2 | tipo_conteudo | titulo | corpo_truncado_at_512_tokens]
```

:::caution
If enrichment metadata is incorrect (see PREMORTEM PF01), the composed text will contain wrong information, causing the embedding to represent something other than the actual chunk content. The model's 512-token limit means the body is silently truncated.
:::

**Output Files:**

| File | Contents |
|------|----------|
| `embeddings_{area}.json` | 768-dim vectors per chunk, normalized |
| `search_corpus_{area}.json` | Chunk text + metadata for display |
| `bm25_index_{area}.json` | Pre-tokenized terms for BM25 ranking |


### search_doutrina_v2.py -- Hybrid Search

| Setting | Value | Location | Configurable |
|---------|-------|----------|--------------|
| `semantic_weight` | `0.7` | Line 163 (function default) | Yes, via `/weight` in interactive mode |
| BM25 `k1` | `1.5` | Line 126 | No (hardcoded) |
| BM25 `b` | `0.75` | Line 126 | No (hardcoded) |
| Default `top_k` | `5` | Line 263 | Yes, via `--top` CLI arg or `/top` command |

**Search Modes (interactive):**

| Command | Mode | Description |
|---------|------|-------------|
| `/hybrid` | Hybrid (default) | `semantic_weight * cosine + (1 - semantic_weight) * BM25` |
| `/sem` | Semantic only | Pure cosine similarity on embeddings |
| `/bm25` | BM25 only | Pure keyword ranking |
| `/area contratos` | Area filter | Restrict search to a specific legal area |
| `/filtro instituto=X` | Metadata filter | Filter by enrichment metadata field |
| `/verbose` | Verbose output | Show full chunk text in results |
| `/top N` | Top-K | Change number of results returned |

**BM25 Parameters Explained:**

- `k1 = 1.5` -- Controls term frequency saturation. Higher values give more weight to repeated terms. Standard range: 1.2-2.0.
- `b = 0.75` -- Controls document length normalization. `b = 1.0` means full normalization; `b = 0.0` means no normalization. Standard value for general text.

:::note
BM25 document frequencies are recalculated on every query (O(N * T) complexity). This is a known performance issue tracked in PREMORTEM RT06. Pre-computation is planned for mitigation action M13.
:::


## Knowledge Base Settings

These conventions are defined in `CLAUDE.md` and enforced manually:

### Frontmatter Schema

**MOC files** require:

```yaml
type: moc
domain: civil          # legal domain
description: "..."     # brief description
```

**Chunk files** require:

```yaml
knowledge_id: "contratos-orlando-gomes-cap05-001"
tipo: chunk
titulo: "Exceptio non adimpleti contractus"
livro_titulo: "Contratos"
autor: "Orlando Gomes"
area_direito: civil
status_enriquecimento: completo  # or "pendente" or "lixo"
instituto: ["exceptio non adimpleti contractus"]
tipo_conteudo: definicao
ramo: civil
```

### File Naming Conventions

| Type | Pattern | Example |
|------|---------|---------|
| MOC | `MOC_{DOMAIN}.md` | `MOC_CIVIL.md` |
| Book directory | `{author}-{title}` (slugified) | `contratos-orlando-gomes/` |
| Chunk file | `chunk_{NNN}.md` | `chunk_001.md` |

### Link Format

Always use Obsidian-style wikilinks for internal references:

```markdown
[[MOC_CIVIL]]           # correct
[MOC Civil](mocs/MOC_CIVIL.md)  # incorrect ‚Äî never use relative markdown links
```


## Configuration Roadmap

Current settings are scattered across 5 scripts as hardcoded constants. The roadmap includes several steps toward centralized configuration:

| Milestone | Feature | What changes |
|-----------|---------|--------------|
| v0.2 | F22 | All paths use `os.environ.get()` with consistent fallbacks |
| v0.2 | F23 | Shared settings extracted to `pipeline/utils.py` |
| v0.3 | F31 | `Makefile` with configurable targets (`make pipeline`, `make test`) |
| v0.3 | F32 | `ruff` linter configuration |

> **Planned Feature** -- A centralized `config.yaml` or `pyproject.toml` for all pipeline settings is under consideration but not yet on the roadmap. Currently, editing source files is the only way to change most parameters.

---
# docs/development/contributing.md
---


# Contributing to Douto

Guidelines for contributing code, knowledge base content, or documentation to Douto.

## Before You Start

1. **Read the introduction** -- understand what Douto does and its position in the sens.legal ecosystem. See the [project overview](/docs/getting-started/introduction/).
2. **Set up your environment** -- follow the [Development Setup](/docs/development/setup/) guide.
3. **Review the conventions** -- read the [Coding Conventions](/docs/development/conventions/) page.
4. **Check the roadmap** -- see [current priorities](/docs/roadmap/) to find high-impact work.

## Types of Contributions

### 1. Pipeline Improvements

Bug fixes, optimizations, and new features for the 5 pipeline scripts. This is where most engineering work happens.

**High-impact areas right now:**
- Standardizing environment variable usage across all scripts (F22, P0)
- Extracting shared functions to `pipeline/utils.py` (F23, P1)
- Pinning dependency versions in `requirements.txt` (F24, P1)

### 2. Tests (Highest Impact)

:::tip[Best way to contribute right now]
Adding test coverage is the single highest-impact contribution you can make. Douto has **0% test coverage**. Even one test for `rechunk_v3.py` is a meaningful improvement. See the [Testing](/docs/development/testing/) page for the planned strategy and priority functions.
:::

### 3. Knowledge Base Content

- Populate empty MOCs (MOC_CONSUMIDOR, MOC_TRIBUTARIO, MOC_CONSTITUCIONAL, MOC_COMPLIANCE, MOC_SUCESSOES)
- Catalog new books into existing MOCs
- Create atomic notes (when the `nodes/` system is implemented)

### 4. Documentation

- Improve these docs
- Add inline code comments
- Update README sections

### 5. Bug Reports

File issues with clear reproduction steps. Include:
- Which script and command you ran
- The error message or unexpected behavior
- Your environment (OS, Python version, dependency versions)
- The values of relevant environment variables (redact API keys)

## Contribution Workflow

### Step 1: Find or Create an Issue

Check the existing issues and roadmap for work to pick up. If you're fixing a new bug or proposing a feature, open an issue first to discuss the approach.

<!-- NEEDS_INPUT: Clarify whether GitHub Issues or Linear (SEN-XXX) is the primary tracker. Decision D04 is pending. -->

### Step 2: Create a Branch

```bash
git fetch origin
git checkout main
git pull origin main

# Feature branch
git checkout -b feat/SEN-XXX-short-description

# Bug fix branch
git checkout -b fix/SEN-XXX-short-description

# Documentation branch
git checkout -b docs/short-description
```

### Step 3: Make Changes

Follow the [Coding Conventions](/docs/development/conventions/). Key checkpoints:

- [ ] No hardcoded absolute paths
- [ ] Type hints on public functions
- [ ] `--dry-run` support if the script modifies data
- [ ] Specific exception handling (no broad `except Exception`)
- [ ] Structured logging for important events

### Step 4: Test Your Changes

Since there are no automated tests yet, verify manually:

```bash
# For pipeline changes: test with a small subset
python3 pipeline/rechunk_v3.py --dry-run --limit 5

# For search changes: run a known query and check results
python3 pipeline/search_doutrina_v2.py "boa-fe objetiva" --area contratos
```

When tests exist (v0.3+):

```bash
make test
make lint
```

### Step 5: Commit

Use [Conventional Commits](https://www.conventionalcommits.org/) format:

```bash
git add pipeline/rechunk_v3.py
git commit -m "feat: add bibliography detection to rechunker -- SEN-XXX"
```

| Prefix | When to use |
|--------|------------|
| `feat:` | New functionality |
| `fix:` | Bug fix |
| `docs:` | Documentation changes |
| `refactor:` | Code restructuring without behavior change |
| `test:` | Adding or updating tests |
| `chore:` | Build, dependencies, tooling |

### Step 6: Push and Open a PR

```bash
git push -u origin feat/SEN-XXX-short-description
```

Then open a pull request on GitHub targeting `main`.

## Pull Request Guidelines

### PR Checklist

- [ ] **Focused scope** -- one feature or fix per PR. Don't mix unrelated changes.
- [ ] **Clear description** -- explain *what* changed and *why*. Link to the issue or roadmap feature.
- [ ] **No breaking changes** -- unless discussed and approved in the issue.
- [ ] **Tests** -- add tests for new functionality (when the test framework exists).
- [ ] **Documentation** -- update docs if user-facing behavior changes.
- [ ] **No secrets** -- double-check that `.env` files, API keys, or large data files are not included.
- [ ] **No formatting noise** -- don't include unrelated whitespace or style changes.

### PR Title Format

```
feat: short description of what this PR does
fix: what was broken and how it's fixed
docs: what documentation was updated
```

### PR Body Template

```markdown
## What

Brief description of the change.

## Why

Link to issue, roadmap feature, or explain the motivation.

## How

Key implementation decisions. What alternatives were considered.

## Testing

How you verified the change works (manual steps or test commands).
```

## Knowledge Base Contributions

Special guidelines for changes to the `knowledge/` directory:

### Adding a New Book to a MOC

1. Open the relevant MOC file (e.g., `knowledge/mocs/MOC_CIVIL.md`)
2. Add the book entry with all required metadata:

```markdown
### Titulo do Livro
- **Autor:** Nome do Autor
- **Editora:** Editora
- **Edicao:** Xa edicao, ANO
- **Chunks:** (pending processing)
- **Status:** catalogado
```

3. Verify the book's legal domain matches the MOC
4. Do **not** update `INDEX_DOUTO.md` unless adding a new domain

### Adding a New Domain/MOC

1. Create `knowledge/mocs/MOC_{DOMAIN}.md` with required frontmatter
2. Add the domain to `knowledge/INDEX_DOUTO.md` with a wikilink
3. Update the documentation

### Rules

- Use **wikilinks** (`[[target]]`) for all internal references
- Follow the **frontmatter schema** defined in [Conventions](/docs/development/conventions/#knowledge-base-conventions)
- Set `status_enriquecimento` correctly -- never leave it as `"pendente"` after enrichment
- Include book metadata: title, author, edition, publisher
- Verify links resolve correctly (open in Obsidian to check)

## Issue Tracking

:::note[Pending decision -- D04]
The source of truth for issue tracking is currently unclear. Commits reference Linear issues (SEN-XXX), but GitHub Issues is empty (0 open, 0 closed). Decision D04 will resolve whether to use Linear, GitHub Issues, or a hybrid approach.
:::

**For now:**
- File issues on GitHub with descriptive labels
- Reference Linear ticket numbers in commits and PRs when applicable

**Suggested labels:**

| Label | Color | Use for |
|-------|-------|---------|
| `tech-debt` | red | Hardcoded paths, missing tests, duplicated code |
| `pipeline` | blue | Changes to pipeline scripts |
| `knowledge-base` | green | MOC updates, new books, atomic notes |
| `integration` | purple | sens.legal ecosystem integration |
| `testing` | yellow | Test infrastructure and coverage |
| `documentation` | gray | Docs improvements |

## Ownership & Attribution

Douto is the exclusive property of Diego Sens (@sensdiego). All contributions are made under the project's license terms.

When commits involve AI assistance, use the mandatory format:

```
Co-Authored-By (execucao): Claude Opus 4.6 <noreply@anthropic.com>
```

The term **(execucao)** indicates AI assisted with implementation. Conception, architecture, product decisions, and intellectual property remain with the author.

---
# docs/development/conventions.md
---


# Coding Conventions

Standards and patterns for all code and knowledge base contributions to Douto. These are extracted from `CLAUDE.md` and enforced during code review.

## Priority Order

When principles conflict, prioritize in this order:

| Priority | Principle | In Practice |
|----------|-----------|-------------|
| 1 | **Correctness** | Legal data, citations, and metadata must be accurate. A wrong `instituto` classification is worse than a slow query. |
| 2 | **Simplicity & clarity** | Code that another agent (or a human six months from now) understands without context. No clever tricks. |
| 3 | **Maintainability** | Easy to change without breaking. Small functions, clear interfaces, minimal coupling between scripts. |
| 4 | **Reversibility** | Prefer decisions that can be undone. Use `--dry-run`, keep original data, avoid destructive operations. |
| 5 | **Performance** | Only optimize when there is evidence of a problem. Never sacrifice correctness or clarity for speed. |

:::tip
When in doubt, choose the simplest and most reversible option. Propose the minimal approach and explain alternatives with trade-offs. Ask before implementing if requirements are unclear.
:::

## Python Conventions

### Language & Stack

- **Python 3.10+** -- required for modern type hint syntax
- **Type hints** are mandatory on all public functions
- **`async/await`** only where necessary (LlamaParse). The pipeline is mostly synchronous.
- **Testes:** `pytest` (when implemented)
- **Linting:** `ruff` (when configured)

### Style

| Convention | Rule | Example |
|-----------|------|---------|
| Functions/variables | `snake_case` | `parse_frontmatter()`, `chunk_text` |
| Constants | `UPPER_SNAKE_CASE` | `MAX_CHUNK_CHARS`, `MODEL_NAME` |
| Type hints | Modern syntax | `tuple[dict, str]`, not `Tuple[Dict, str]` |
| String formatting | f-strings preferred | `f"Processed {count} chunks"` |
| Imports | stdlib, then third-party, then local, separated by blank lines | See below |
| Line length | Follow `ruff` defaults (when configured) | ~88 characters |

```python
# Import order example
import os
import json
import re
from pathlib import Path

import numpy as np
from sentence_transformers import SentenceTransformer

from pipeline.utils import parse_frontmatter, slugify
```

### Error Handling

- **Specific exceptions** -- never use broad `except Exception as e`. Catch the specific error type.
- **Log with context** -- include `doc_id`, `chunk_id`, and traceback when logging errors.
- **Fail loudly** -- if a chunk is corrupted, log and skip it rather than silently producing garbage.

:::caution
The current codebase has 4 broad `except Exception` catches (PREMORTEM RT07). New code must not introduce more. Existing ones should be narrowed during refactoring.
:::

### Docstrings

Required for all public functions. Use Google-style format:

```python
def parse_frontmatter(content: str) -> tuple[dict, str]:
    """Parse YAML frontmatter and body from a markdown string.

    Args:
        content: Full markdown content with optional frontmatter.

    Returns:
        Tuple of (metadata dict, body text without frontmatter).
        If no frontmatter found, returns ({}, original content).
    """
```

## Pipeline Script Conventions

Every pipeline script must follow these patterns:

### Required Features

| Feature | Implementation | Purpose |
|---------|---------------|---------|
| `--help` | `argparse` | Document CLI usage |
| `--dry-run` | Check before write | Show what would happen without modifying data |
| `--force` | Skip idempotency checks | Reprocess already-completed items |
| Idempotent | Processing markers, skip logic | Safe to re-run without side effects |
| Structured logging | Append to `processing_log.jsonl` | Track successes, errors, skips |

### Path Handling

```python
# CORRECT: use os.environ.get() with documented fallback
VAULT_PATH = Path(os.environ.get("VAULT_PATH", "/default/fallback/path"))

# INCORRECT: hardcoded absolute path
VAULT_PATH = Path("/home/sensd/.openclaw/workspace/vault")

# CORRECT: relative to script location
PROMPT_PATH = Path(__file__).parent / "enrich_prompt.md"
```

:::danger
No hardcoded absolute paths in pipeline scripts. Every path must use `os.environ.get()` or be relative to the script location (`Path(__file__).parent`). This is a pre-commit review checkpoint.
:::

### Output Conventions

- **JSON** for data output (embeddings, corpus, BM25 index)
- **YAML frontmatter** for metadata in markdown chunks
- **JSONL** for structured logs (append-only)
- **Progress** output goes to `stderr`; results go to `stdout`

### Resource Limits

Do not run processes that consume more than 50% CPU locally. For validation:

```bash
python3 pipeline/rechunk_v3.py --limit 5 --dry-run  # test with small subset
```

## Knowledge Base Conventions

### INDEX_DOUTO.md

- **Source of truth** for domains and navigation
- Lists all 8 legal domains with wikilinks to their MOCs
- Must be updated when adding new domains

### MOC Files

Required frontmatter fields:

```yaml
type: moc
domain: civil
description: "Obrigacoes, contratos, responsabilidade civil, propriedade"
```

### Chunk Files (Enriched)

Required frontmatter fields:

```yaml
knowledge_id: "contratos-orlando-gomes-cap05-001"
tipo: chunk
titulo: "Titulo do chunk"
livro_titulo: "Contratos"
autor: "Orlando Gomes"
area_direito: civil
status_enriquecimento: completo  # "completo" | "pendente" | "lixo"
```

Rules:
- Chunks with `status_enriquecimento: "completo"` must have `instituto` and `tipo_conteudo` filled
- Never overwrite enriched chunks without explicit `--force`
- Never leave `status_enriquecimento: "pendente"` after enrichment runs
- Use `"lixo"` for noise chunks (prefaces, acknowledgments, catalog cards)

### Links

- **Always** use wikilinks: `[[MOC_CIVIL]]`
- **Never** use relative markdown links: `[text](../mocs/MOC_CIVIL.md)`
- Wikilinks enable Obsidian graph view and backlink tracking

### Encoding

- UTF-8 for all files
- LF line endings (Unix-style)

## Embedding Conventions

| Rule | Detail |
|------|--------|
| Single model | `rufimelo/Legal-BERTimbau-sts-base` (768-dim) |
| Normalization | Always `normalize_embeddings=True` for cosine similarity |
| Text composition | Use `compose_embedding_text()` -- never embed raw chunk text |
| Output naming | `embeddings_{area}.json`, `search_corpus_{area}.json`, `bm25_index_{area}.json` |
| Compatibility | Output must be compatible with Valter/Juca infrastructure |

The composed text format:

```
[categoria | instituto_1, instituto_2 | tipo_conteudo | titulo | corpo]
```

This ensures the embedding captures both the metadata context and the actual content.

## Git Conventions

### Branch Naming

```
feat/SEN-XXX-description    # new feature linked to Linear issue
fix/SEN-XXX-description     # bug fix linked to Linear issue
docs/description            # documentation changes
refactor/description        # code restructuring
```

Append `-claude` for branches created by Claude Code, `-codex` for branches created by Codex.

### Commit Messages

Follow [Conventional Commits](https://www.conventionalcommits.org/):

```
feat: add bibliography detection to rechunker
fix: handle colons in frontmatter values
docs: update environment variable reference
refactor: extract parse_frontmatter to utils.py
test: add fixtures for smart_split edge cases
chore: pin dependency versions
```

Include the Linear ticket reference when applicable:

```
feat: standardize env vars across pipeline -- SEN-358
```

### Co-Authorship

When commits are produced with AI assistance:

```
Co-Authored-By (execucao): Claude Opus 4.6 <noreply@anthropic.com>
```

The term **(execucao)** indicates the AI assisted with implementation. All conception, architecture, product decisions, and intellectual property belong to the project owner.

### Never Commit

- `.env` files or API keys
- Embedding JSON files (too large, generated artifacts)
- `__pycache__/` directories
- Model weights or HuggingFace cache
- Node modules (if any frontend tooling is added)

## What NOT To Do (Non-Goals)

Do not do any of the following without explicit authorization:

| Non-goal | Reason |
|----------|--------|
| Introduce abstractions without clear need | Simplicity over elegance |
| Add dependencies for problems already solved in the codebase | Minimize dependency surface |
| Refactor working code without a specific issue | If it works, leave it |
| Optimize without evidence of performance problems | Premature optimization is the root of all evil |
| Expand scope beyond the issue being worked on | Stay focused |
| Create API/MCP server before the pipeline is stable | Foundation before features |
| Manage cases (Joseph's job) | Douto's scope is doctrine only |
| Search case law (Juca/Valter's job) | Douto's scope is doctrine only |
| Search legislation (Leci's job) | Douto's scope is doctrine only |
| Manage infrastructure (Valter's job) | Douto is a processing pipeline, not a service |

---
# docs/development/setup.md
---


# Development Setup

Everything you need to start developing on Douto -- from cloning to running your first pipeline pass.

## Prerequisites

| Requirement | Version | Notes |
|-------------|---------|-------|
| Python | 3.10+ | Required for modern type hints (`tuple[dict, str]`) |
| pip | Latest | Package installer |
| Git | Any recent | Version control |
| RAM | 8 GB+ | PyTorch + Legal-BERTimbau model loading |
| Disk | ~2 GB free | Model cache (~500 MB) + embeddings data |

**Optional:**

| Tool | Purpose |
|------|---------|
| CUDA-compatible GPU | Faster embedding generation (CPU works, just slower) |
| [Obsidian](https://obsidian.md/) | Navigate the knowledge base visually with wikilinks |
| API keys | See below -- only needed for specific pipeline stages |

## Clone and Install

```bash
# Clone the repository
git clone https://github.com/sensdiego/douto.git
cd douto

# Create and activate a virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows

# Install dependencies
pip install -r pipeline/requirements.txt
```

:::caution[Unpinned dependencies]
The `requirements.txt` does not specify version numbers. Dependency versions may drift over time and cause incompatibilities. Pinning versions is tracked as F24 (v0.2). If you encounter issues, try matching the versions from a known working environment.

Current dependencies:
- `sentence-transformers`
- `torch`
- `numpy`
- `anthropic`
- `llama-parse`
:::

## Configure Environment

Set environment variables based on what you plan to do. See the [Environment Variables](/docs/configuration/environment/) reference for full details.

### Minimal (search only)

```bash
export DATA_PATH="/path/to/juca/data"
```

### Full pipeline

```bash
export VAULT_PATH="/path/to/your/vault"
export OUTPUT_PATH="/path/to/juca/data"
export DATA_PATH="/path/to/juca/data"
export MINIMAX_API_KEY="your-minimax-key"
export LLAMA_CLOUD_API_KEY="your-llamaparse-key"
```

:::tip
Create a `.env` file in the project root and `source .env` before working. See the [Environment Variables](/docs/configuration/environment/#example-env-file) page for a template.
:::

## Verify Setup

Run these commands to confirm each component is working:

```bash
# Check Python version (need 3.10+)
python3 --version

# Check sentence-transformers
python3 -c "import sentence_transformers; print('sentence-transformers OK')"

# Check PyTorch and CUDA availability
python3 -c "import torch; print(f'torch OK, CUDA: {torch.cuda.is_available()}')"

# Check anthropic SDK
python3 -c "import anthropic; print('anthropic SDK OK')"

# Check pipeline scripts are accessible
python3 pipeline/search_doutrina_v2.py --help
python3 pipeline/rechunk_v3.py --help
python3 pipeline/enrich_chunks.py --help
```

If all checks pass, your environment is ready.

## Project Structure

```
douto/
‚îú‚îÄ‚îÄ AGENTS.md                 # Agent identity, responsibilities, boundaries
‚îú‚îÄ‚îÄ CLAUDE.md                 # Coding guidelines for AI agents
‚îú‚îÄ‚îÄ ROADMAP.md                # Product roadmap with milestones
‚îú‚îÄ‚îÄ PROJECT_MAP.md            # Full project diagnostic
‚îú‚îÄ‚îÄ PREMORTEM.md              # Risk analysis and failure scenarios
‚îú‚îÄ‚îÄ knowledge/
‚îÇ   ‚îú‚îÄ‚îÄ INDEX_DOUTO.md        # Skill graph index ‚Äî entry point
‚îÇ   ‚îú‚îÄ‚îÄ mocs/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MOC_CIVIL.md      # 35 books, ~9,365 chunks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MOC_PROCESSUAL.md # 8 books, ~22,182 chunks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MOC_EMPRESARIAL.md# 7 books
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MOC_CONSUMIDOR.md # Placeholder (not yet populated)
‚îÇ   ‚îî‚îÄ‚îÄ nodes/
‚îÇ       ‚îî‚îÄ‚îÄ .gitkeep          # Atomic notes (future ‚Äî F36)
‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îú‚îÄ‚îÄ process_books.py      # Step 1: PDF ‚Üí markdown (LlamaParse)
‚îÇ   ‚îú‚îÄ‚îÄ rechunk_v3.py         # Step 2: markdown ‚Üí intelligent chunks
‚îÇ   ‚îú‚îÄ‚îÄ enrich_chunks.py      # Step 3: chunks ‚Üí classified (MiniMax M2.5)
‚îÇ   ‚îú‚îÄ‚îÄ embed_doutrina.py     # Step 4: chunks ‚Üí 768-dim embeddings
‚îÇ   ‚îú‚îÄ‚îÄ search_doutrina_v2.py # Step 5: hybrid semantic + BM25 search
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt      # Python dependencies (unpinned)
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep              # Auxiliary tools (future)
‚îî‚îÄ‚îÄ docs/                     # Starlight documentation site
```

### Key Directories

| Directory | Purpose |
|-----------|---------|
| `pipeline/` | ETL scripts that process legal textbooks into searchable data |
| `knowledge/` | Obsidian-compatible markdown skill graph (MOCs, index, future atomic notes) |
| `tools/` | Reserved for auxiliary scripts (empty) |
| `docs/` | Project documentation (Starlight/Astro) |

### Pipeline Data Flow

```
process_books.py ‚Üí rechunk_v3.py ‚Üí enrich_chunks.py ‚Üí embed_doutrina.py ‚Üí search_doutrina_v2.py
     PDF‚ÜíMD          MD‚Üíchunks      chunks‚Üímetadata    chunks‚Üívectors      vectors‚Üíresults
```

Each script depends on the output of the previous one. They must be run sequentially.

## Development Workflow

### 1. Create a Branch

```bash
git fetch origin
git checkout -b feat/SEN-XXX-description  # feature
git checkout -b fix/SEN-XXX-description   # bug fix
git checkout -b docs/description          # documentation
```

Branch naming follows the pattern `{type}/SEN-{ticket}-{description}` when linked to a Linear issue, or `{type}/{description}` otherwise.

### 2. Make Changes

Follow the [Coding Conventions](/docs/development/conventions/). Key rules:

- Type hints on all public functions
- `--dry-run` support for scripts that modify data
- `os.environ.get()` for all paths (no hardcoded absolutes)
- Idempotent operations (safe to re-run)

### 3. Test Your Changes

:::note
There are currently **no automated tests** (0% coverage). Testing is manual. When tests are added (v0.3, F26-F27), the workflow will include `make test`.
:::

For pipeline changes, test with a small subset:

```bash
python3 pipeline/rechunk_v3.py --dry-run --limit 5
```

### 4. Commit

Use [Conventional Commits](https://www.conventionalcommits.org/):

```bash
git add pipeline/rechunk_v3.py
git commit -m "feat: add bibliography detection to rechunker"
```

Commit types: `feat:`, `fix:`, `docs:`, `refactor:`, `test:`, `chore:`

### 5. Push and Open a PR

```bash
git push -u origin feat/SEN-XXX-description
```

Create a pull request targeting `main`. Include a description of what changed and why.

### Multi-Agent Coordination

This project is developed by two AI code agents:

- **Claude Code** -- local execution
- **Codex (OpenAI)** -- cloud execution

**Rule:** Never work on the same branch that Codex is using.

```bash
git branch -a | grep codex  # check for active Codex branches
```

## IDE Recommendations

| Tool | Use |
|------|-----|
| **VS Code** | Primary editor, with Python extension for type checking and linting |
| **Obsidian** | Navigate `knowledge/` visually -- wikilinks, graph view, frontmatter |
| **ruff extension** | Linting (when configured -- F32) |

Open the `knowledge/` directory as an Obsidian vault to see the skill graph in graph view and follow wikilinks between MOCs and future atomic notes.

---
# docs/development/testing.md
---


# Testing

Current testing status and the planned testing strategy for Douto.

## Current State

:::danger[Zero test coverage]
Douto has **no automated tests**. There is no test framework configured, no test directory, no CI/CD pipeline running tests. The test coverage is **0%**.

This is the #1 technical debt item identified in the PREMORTEM (RT04). The most complex script in the pipeline (`rechunk_v3.py` -- 890 lines, 16 regex patterns, 5 processing passes) has no test coverage at all. Any refactoring risks introducing silent regressions that can only be detected by manual inspection of thousands of chunks.
:::

## Planned Testing Strategy

Testing will be introduced in three phases across milestones v0.3 and v0.5.

### Phase 1: Critical Path Tests (v0.3 -- F26)

**Framework:** pytest
**Target:** `rechunk_v3.py` -- the most complex and fragile component

Priority functions to test:

| Function | Lines | Why it's critical |
|----------|-------|-------------------|
| `smart_split()` | ~200 | The 5-pass chunking algorithm. Core of the entire pipeline. |
| `detect_section()` | ~20 | Uses 16 regex patterns to identify legal document structure. Wrong detection = wrong chunk boundaries. |
| `classify_title()` | ~50 | Distinguishes content from noise, bibliography, summary, and running headers. False positives lose real content. |
| `aggregate_footnotes()` | ~30 | Groups footnotes with parent paragraphs. Broken grouping = orphaned footnotes. |
| `detect_tail()` | ~20 | Identifies trailing content (bibliography, index). Misdetection = bibliography mixed with content. |
| `classify_block_content()` | ~40 | Classifies blocks as example, table, characteristics, law_article, or bibliography. |

**Test fixtures:** Real markdown samples from legal books (anonymized if needed to avoid IP concerns).

**Minimum acceptance:** At least one test per function above, covering both the happy path and at least one edge case.

### Phase 2: Utility Tests (v0.3 -- F27)

**Target:** Shared functions (currently duplicated, to be extracted to `utils.py` in F23)

| Function | Edge cases to cover |
|----------|-------------------|
| `parse_frontmatter()` | Values with colons (`titulo: "Codigo Civil: Comentado"`), hash symbols, multiline values, missing frontmatter, malformed YAML |
| `slugify()` | Unicode characters, accented letters, special characters, collision-prone inputs (e.g., "Contratos - Vol. 1" vs. "Contratos - Vol 1") |
| `extract_json()` | Valid JSON in markdown code blocks, JSON with extra text around it, malformed JSON, nested objects |
| `compose_embedding_text()` | Missing metadata fields, empty body, very long text (truncation at 512 tokens) |

### Phase 3: Search Quality Tests (v0.5 -- F40)

**Target:** End-to-end search quality evaluation

| Component | What to measure |
|-----------|----------------|
| **Evaluation set** | 30+ queries with expected result chunks (manually curated) |
| **recall@5** | What fraction of expected results appear in top 5? |
| **recall@10** | What fraction appear in top 10? |
| **nDCG** | Are the most relevant results ranked highest? |
| **Regression detection** | Before/after comparison when pipeline changes (new model, new prompt, rechunking) |

This phase also includes metadata quality validation (M06):
- Sample 200 random chunks
- Verify `instituto`, `tipo_conteudo`, `ramo` against actual content
- Establish accuracy baseline
- **Quality gate:** if accuracy < 85%, halt and re-enrich before proceeding

## How to Run Tests

:::note
Tests do not exist yet. The commands below describe the planned workflow after F26/F27 are implemented.
:::

> **Planned Feature** -- Test infrastructure is on the roadmap for v0.3 but not yet implemented.

```bash
# Run all tests
make test
# or directly:
pytest

# Run only pipeline tests
pytest tests/pipeline/

# Run a specific test
pytest -k "test_smart_split"

# Run with coverage report
pytest --cov=pipeline --cov-report=html

# Run with verbose output
pytest -v
```

## How to Write Tests

### Directory Structure (Planned)

```
tests/
‚îú‚îÄ‚îÄ conftest.py                    # Shared fixtures
‚îú‚îÄ‚îÄ fixtures/
‚îÇ   ‚îú‚îÄ‚îÄ markdown/                  # Sample legal markdown files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ contract_chapter.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cpc_commentary.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bibliography.md
‚îÇ   ‚îú‚îÄ‚îÄ frontmatter/               # Frontmatter edge cases
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ valid.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ colon_in_value.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ missing_frontmatter.md
‚îÇ   ‚îî‚îÄ‚îÄ json/                      # Sample JSON responses
‚îÇ       ‚îú‚îÄ‚îÄ enrichment_response.json
‚îÇ       ‚îî‚îÄ‚îÄ malformed_response.json
‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îú‚îÄ‚îÄ test_rechunk_v3.py
‚îÇ   ‚îú‚îÄ‚îÄ test_enrich_chunks.py
‚îÇ   ‚îú‚îÄ‚îÄ test_embed_doutrina.py
‚îÇ   ‚îú‚îÄ‚îÄ test_search_doutrina.py
‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py
‚îî‚îÄ‚îÄ evaluation/
    ‚îî‚îÄ‚îÄ test_search_quality.py     # Phase 3 eval set
```

### Conventions

| Convention | Rule |
|-----------|------|
| File naming | `test_{module}.py` |
| Function naming | `test_{function}_{scenario}()` |
| Fixtures | Place in `tests/fixtures/`, use `pytest` fixture mechanism |
| External APIs | **Always mock.** Never call MiniMax, LlamaParse, or HuggingFace in tests. |
| Assertions | Test behavior and output, not implementation details |
| Independence | Each test must be independent -- no shared mutable state between tests |

### Example Test (Preview)

```python
# tests/pipeline/test_rechunk_v3.py
import pytest
from pipeline.rechunk_v3 import detect_section, classify_title

class TestDetectSection:
    def test_markdown_header(self):
        result = detect_section("## Responsabilidade Civil")
        assert result is not None
        title, ptype = result
        assert ptype == "md_header"
        assert "Responsabilidade Civil" in title

    def test_legal_article(self):
        result = detect_section("**Art. 481.** O vendedor obriga-se...")
        assert result is not None
        title, ptype = result
        assert ptype == "artigo"

    def test_plain_text_not_section(self):
        result = detect_section("Este principio fundamenta a boa-fe objetiva.")
        assert result is None

class TestClassifyTitle:
    def test_bibliography_detection(self):
        assert classify_title("REFERENCIAS BIBLIOGRAFICAS") == "bibliography"
        assert classify_title("Bibliografia") == "bibliography"

    def test_noise_detection(self):
        assert classify_title("Agradecimentos") == "noise"
        assert classify_title("PREFACIO") == "noise"
```

## Test Data

### Creating Fixtures

- Use **real legal markdown** for chunking tests (from the processed corpus)
- **Anonymize** if there are IP concerns -- replace proper names, paraphrase content while preserving structure
- Use **synthetic frontmatter** for parser tests (craft specific edge cases)
- Use **pre-computed small embeddings** for search tests (a few dozen vectors, not the full corpus)
- **Never commit full corpus data** to test fixtures (too large, potential IP issues)

### Evaluation Set Guidelines (Phase 3)

Each entry in the evaluation set should include:

```json
{
  "query": "exceptio non adimpleti contractus requisitos",
  "expected_chunks": ["contratos-orlando-gomes-cap12-003", "contratos-civil-cap08-001"],
  "area": "contratos",
  "notes": "Both chunks define the requirements for the defense"
}
```

Target: at least 30 queries covering:
- Different legal domains (civil, processual, empresarial)
- Different query types (conceptual, specific article, comparative)
- Edge cases (cross-domain concepts like "boa-fe", very specific terms)

---
# docs/features/index.md
---


# Features

Every feature in Douto, organized by implementation status. Each pipeline stage and knowledge-base component links to a dedicated page with architecture details, code examples, and known limitations.

## Feature Status Legend

| Badge | Meaning |
|-------|---------|
| **Implemented** | In production, functional, used in the current pipeline |
| **In Progress** | Partially implemented -- structure exists but incomplete |
| **Planned** | On the roadmap with an assigned milestone |
| **Idea** | Proposed but not yet scheduled |
| **Innovation** | Strategic proposals from `INNOVATION_LAYER.md` |


## Pipeline Features (Implemented)

The core processing pipeline transforms legal textbook PDFs into searchable embeddings through five sequential stages.

| # | Feature | Status | Script | Page |
|---|---------|--------|--------|------|
| F01 | PDF Extraction | Implemented | `process_books.py` | [pdf-extraction](pipeline/pdf-extraction.md) |
| F02 | Intelligent Chunking v3 | Implemented | `rechunk_v3.py` | [intelligent-chunking](pipeline/intelligent-chunking.md) |
| F03 | Chunk Enrichment | Implemented | `enrich_chunks.py` | [enrichment](pipeline/enrichment.md) |
| F04 | Embedding Generation | Implemented | `embed_doutrina.py` | [embeddings](pipeline/embeddings.md) |
| F05 | Hybrid Search | Implemented | `search_doutrina_v2.py` | [hybrid-search](pipeline/hybrid-search.md) |
| F06 | Multi-Area Search | Implemented | `search_doutrina_v2.py` | [hybrid-search](pipeline/hybrid-search.md) |
| F07 | Interactive Search CLI | Implemented | `search_doutrina_v2.py` | [hybrid-search](pipeline/hybrid-search.md) |

```mermaid
flowchart LR
    PDF["PDF files"]
    MD["Markdown + chapters"]
    CHK["Semantic chunks"]
    ENR["Enriched chunks"]
    EMB["Embeddings + corpus"]
    SRCH["Search results"]

    PDF -->|"F01 process_books.py"| MD
    MD -->|"F02 rechunk_v3.py"| CHK
    CHK -->|"F03 enrich_chunks.py"| ENR
    ENR -->|"F04 embed_doutrina.py"| EMB
    EMB -->|"F05 search_doutrina_v2.py"| SRCH
```

## Knowledge Base Features

The knowledge base is an Obsidian-style skill graph that organizes the doctrinal corpus by legal domain.

| # | Feature | Status | Artifact | Page |
|---|---------|--------|----------|------|
| F08 | Skill Graph INDEX | Implemented | `INDEX_DOUTO.md` | [skill-graph](knowledge-base/skill-graph.md) |
| F09 | MOC Direito Civil | Implemented | `MOC_CIVIL.md` | [mocs](knowledge-base/mocs.md) |
| F10 | MOC Processual Civil | Implemented | `MOC_PROCESSUAL.md` | [mocs](knowledge-base/mocs.md) |
| F11 | MOC Empresarial | Implemented | `MOC_EMPRESARIAL.md` | [mocs](knowledge-base/mocs.md) |
| F19 | MOC Consumidor | In Progress | `MOC_CONSUMIDOR.md` | [mocs](knowledge-base/mocs.md) |
| F21 | Atomic Notes (nodes/) | In Progress | `knowledge/nodes/` | [atomic-notes](knowledge-base/atomic-notes.md) |

## Pipeline Quality Features (Implemented)

Cross-cutting capabilities that ensure reliability and debuggability across all pipeline stages.

| # | Feature | Status | Description |
|---|---------|--------|-------------|
| F12 | Idempotent Processing | Implemented | Markers prevent re-processing; `--force` to override |
| F13 | Structured Logging | Implemented | `processing_log.jsonl` with append-only events |
| F14 | Dry-Run Mode | Implemented | `--dry-run` flag on all scripts that mutate data |
| F15 | Standardized YAML Frontmatter | Implemented | Consistent schema: `knowledge_id`, `tipo`, `titulo`, `livro_titulo`, `autor`, `area_direito`, `status_enriquecimento` |

## Project Infrastructure (Implemented)

| # | Feature | Status | Description |
|---|---------|--------|-------------|
| F16 | AGENTS.md | Implemented | Agent identity, responsibilities, limits, git protocol |
| F17 | CLAUDE.md | Implemented | Coding guidelines for AI agents aligned with the sens.legal ecosystem |
| F18 | PROJECT_MAP.md | Implemented | Full project diagnostic and architecture map |


## Planned Features

Organized by target milestone. Source: `ROADMAP.md`.

### v0.2 -- Pipeline Estavel

| # | Feature | Priority | Description |
|---|---------|----------|-------------|
| F22 | Standardize paths | **P0** | Eliminate hardcoded absolute paths in `process_books.py` and `rechunk_v3.py` |
| F23 | Extract `pipeline/utils.py` | **P1** | Deduplicate `parse_frontmatter()`, `slugify()`, `build_frontmatter()` |
| F24 | Pin dependency versions | **P1** | `requirements.txt` with exact versions |
| F20 | Complete env var standardization | **P1** | 2/5 scripts use `os.environ.get()`, 3 have hardcoded paths |

### v0.3 -- Qualidade & Cobertura

| # | Feature | Priority | Description |
|---|---------|----------|-------------|
| F25 | Create missing MOCs | **P1** | `MOC_TRIBUTARIO`, `MOC_CONSTITUCIONAL`, `MOC_COMPLIANCE`, `MOC_SUCESSOES` |
| F26 | Tests for `rechunk_v3.py` | **P1** | pytest with real legal markdown fixtures |
| F27 | Tests for utility functions | **P2** | `parse_frontmatter`, `slugify`, `extract_json`, `compose_embedding_text` |
| F28 | Complete README | **P2** | Setup, prerequisites, env vars, usage, architecture |
| F31 | Pipeline Makefile | **P2** | `make pipeline`, `make search`, `make test`, `make lint` |
| F32 | Linting with ruff | **P2** | Configure ruff, integrate into Makefile |
| F42 | Version enrich prompt | **P1** | `enrich_prompt.md` referenced in code but missing from repo |

### v0.4 -- Integracao sens.legal

| # | Feature | Priority | Description |
|---|---------|----------|-------------|
| F29 | Douto-to-Valter integration | **P1** | Define protocol: file, API, or MCP |
| F30 | MCP Server for doctrine | **P1** | Expose search via Model Context Protocol |

## Ideas

Features inferred from the sens.legal ecosystem architecture.

| # | Feature | Priority | Milestone | Description |
|---|---------|----------|-----------|-------------|
| F33 | Doutrina in Neo4j | **P2** | v1.0 | Ingest doctrine nodes into Valter's knowledge graph |
| F34 | Doutrina-jurisprudencia cross-ref | **P2** | v1.0 | Auto-link when STJ decisions cite a doctrinal author |
| F35 | Doutrina-legislacao cross-ref | **P3** | v1.0 | Link doctrinal commentary to statutory provisions in Leci |
| F36 | Auto-generate atomic notes | **P2** | v0.5 | One note per `instituto` from enriched chunks |
| F37 | Progressive Briefing support | **P2** | v1.0 | Feed Juca's 4-phase briefing with doctrinal sources |
| F38 | Docker pipeline | **P3** | v1.0 | Containerize with PyTorch + pre-downloaded models |
| F39 | Basic CI/CD | **P3** | v0.5 | GitHub Actions: ruff lint + pytest on PRs |
| F40 | Embedding quality eval set | **P2** | v0.5 | Query-answer pairs to measure recall@k and nDCG |
| F41 | Unified ingestion CLI | **P3** | v0.5 | `douto ingest livro.pdf` runs the full pipeline |


## Innovation Layer (Proposed)

Strategic proposals from `INNOVATION_LAYER.md`. These would transform Douto from a "book search engine" into a "doctrinal reasoning engine."

| # | Feature | Priority | Milestone | Description |
|---|---------|----------|-----------|-------------|
| F43 | Doctrine Synthesis Engine | **P1** | v0.3.5 | Synthesize all chunks for a given legal concept across all books |
| F44 | Synthesis prompt | **P1** | v0.3.5 | Carefully designed prompt for generating Doctrine Briefs |
| F45 | Doctrine Brief template | **P1** | v0.3.5 | Standardized output format (Markdown + JSON) |
| F46 | Ontological concept extraction | **P2** | v0.6 | Collect all institutos and co-occurrences from the corpus |
| F47 | Relationship typing | **P2** | v0.6 | LLM classifies relationships (IS_A, APPLIES_TO, REQUIRES, etc.) |
| F48 | Ontology export and visualization | **P3** | v0.6 | GraphML, RDF/Turtle, JSON, interactive visualization |

:::tip
The Doctrine Synthesis Engine (F43) is the single highest-impact proposed feature. It leverages the existing enriched corpus to produce structured Doctrine Briefs that no competitor currently offers. See `INNOVATION_LAYER.md` for the full proposal.
:::

---
# docs/features/knowledge-base/atomic-notes.md
---


# Atomic Notes (F21, F36)

`knowledge/nodes/` -- The planned third and most granular layer of the skill graph. Each atomic note would represent a single legal concept (`instituto juridico`) with definitions from multiple authors, requirements, related concepts, and provenance links to source chunks.

> **Planned Feature** -- This feature is on the roadmap but not yet implemented. The `nodes/` directory exists with only a `.gitkeep` file. No atomic notes have been created. Implementation depends on pending decision **D03**.

## Current State

| Property | Value |
|----------|-------|
| **Directory** | `knowledge/nodes/` |
| **Files** | 1 (`.gitkeep` only) |
| **Status** | In Progress (~5%) -- directory created, schema defined, no content |
| **Milestone** | v0.5 |
| **Blocking decision** | D03: auto-generate vs. curate vs. hybrid |
| **Pre-requisite** | Metadata quality gate (M06: accuracy >= 85%) |

## What Atomic Notes Would Be

An atomic note represents a single legal concept -- an `instituto juridico` -- synthesized from all chunks across all books that discuss that concept. Where a chunk is a passage from a single book, an atomic note is a **cross-book synthesis** of a single idea.

### Example: "Exceptio Non Adimpleti Contractus"

An atomic note for this concept would contain:
- **Definition** synthesized from Orlando Gomes, Fabio Ulhoa Coelho, Chitty, and other authors in the corpus
- **Requirements/elements** as enumerated by the doctrine
- **Statutory basis** (`CC art. 476`, `CC art. 477`)
- **Related concepts** (contrato bilateral, inadimplemento, resolucao contratual, pacta sunt servanda)
- **Provenance** linking to every chunk that discusses this concept, with author and book attribution

```mermaid
graph TD
    NOTE["Atomic Note:<br/>exceptio_non_adimpleti_contractus"]

    C1["Chunk: Orlando Gomes<br/>Contratos, ch. 26"]
    C2["Chunk: Fabio Ulhoa<br/>Curso de Dir. Civil, ch. 31"]
    C3["Chunk: Chitty<br/>Chitty on Contracts, ch. 24"]
    C4["Chunk: Eisenberg<br/>Foundational Principles, ch. 15"]

    MOC["MOC_CIVIL.md"]
    REL1["Nota: contrato_bilateral"]
    REL2["Nota: inadimplemento"]
    LEG["CC art. 476-477"]

    C1 -->|"source"| NOTE
    C2 -->|"source"| NOTE
    C3 -->|"source"| NOTE
    C4 -->|"source"| NOTE
    NOTE -->|"parent"| MOC
    NOTE -->|"relates_to"| REL1
    NOTE -->|"requires"| REL2
    NOTE -->|"regulated_by"| LEG
```

## Proposed Format

Based on the schema defined in `CLAUDE.md`, atomic notes would use this structure:

### Frontmatter

```yaml
tipo: nota
instituto: exceptio_non_adimpleti_contractus
sub_instituto: []
ramo: direito_civil
fontes_normativas:
  - "CC art. 476"
  - "CC art. 477"
autores:
  - "Orlando Gomes"
  - "Fabio Ulhoa Coelho"
  - "Chitty"
  - "Melvin Eisenberg"
chunks_origem:
  - "contratos-orlando-gomes/026-contratos-bilaterais"
  - "curso-de-direito-civil-contratos/031-exceptio"
  - "chitty-on-contracts/024-bilateral-contracts"
total_fontes: 4
confiabilidade: alta
data_criacao: "2026-03-15T10:00:00"
data_atualizacao: "2026-03-15T10:00:00"
```

### Content

```markdown
# Exceptio Non Adimpleti Contractus

## Definicao
[Synthesized definition from multiple authors, with inline citations]

## Requisitos
1. Contrato bilateral (sinalagmatico)
2. Inadimplemento da parte contraria
3. ...

## Posicao doutrinaria
[Where authors agree and where they diverge]

## Fontes normativas
- CC art. 476 ‚Äî ...
- CC art. 477 ‚Äî ...

## Conceitos relacionados
- [[contrato_bilateral]]
- [[inadimplemento]]
- [[resolucao_contratual]]
- [[pacta_sunt_servanda]]

## Fontes
| Autor | Obra | Chunk | Tipo |
|-------|------|-------|------|
| Orlando Gomes | Contratos | [[026-contratos-bilaterais]] | definicao, requisitos |
| Fabio Ulhoa | Curso Dir. Civil | [[031-exceptio]] | doutrina_comparada |
```

## Generation Strategy (Pending Decision D03)

Decision D03 from the ROADMAP evaluates three approaches:

### Option A: Auto-Generate

Generate one note per unique `instituto` value from the enrichment metadata. A script would:

1. Collect all unique `instituto` values from enriched chunks
2. For each instituto, gather all chunks tagged with it
3. Generate a note with provenance links and a placeholder for synthesis

**Pros:** Fast, comprehensive (covers every instituto in the corpus), no human effort.
**Cons:** Potentially noisy -- if enrichment metadata is inaccurate, notes inherit those errors. No synthesis or cross-referencing, just aggregation.

### Option B: Manual Curation

A legal expert reviews chunks and writes each atomic note by hand.

**Pros:** High quality, accurate synthesis, correct cross-references.
**Cons:** Extremely slow. At ~500+ unique institutos in the corpus, manual curation would take weeks of expert time. Does not scale.

### Option C: Hybrid (Recommended)

Auto-generate draft notes from enrichment metadata, then flag them for human review. A reviewer validates, corrects, and enriches each note.

**Pros:** Combines speed of automation with quality of human judgment. Reviewers have a starting point instead of a blank page.
**Cons:** Still requires significant review effort. Needs a workflow for tracking review status.

:::tip
The ROADMAP recommends Option C (hybrid), but with an important **pre-condition**: the metadata quality gate (mitigation M06) must confirm that enrichment accuracy is >= 85% before generating draft notes. Generating notes from bad metadata would create a systematically misleading knowledge base.
:::

## Relationship to the Doctrine Synthesis Engine (F43)

The [Doctrine Synthesis Engine](https://github.com/sensdiego/douto/blob/main/INNOVATION_LAYER.md) (F43 from `INNOVATION_LAYER.md`) proposes a more sophisticated approach to atomic notes. Instead of simple aggregation, it would use an LLM to synthesize a **Doctrine Brief** for each legal concept:

| Aspect | Atomic Notes (F21/F36) | Doctrine Brief (F43) |
|--------|----------------------|---------------------|
| **Content** | Aggregated links + placeholder text | LLM-synthesized analysis with citations |
| **Quality** | Depends on metadata accuracy | Depends on LLM + metadata accuracy |
| **Output** | Markdown note with wikilinks | Structured JSON + Markdown |
| **Effort** | Low (automated) to Medium (hybrid) | Medium (prompt engineering + LLM costs) |
| **Value** | Navigation and organization | Actual doctrinal synthesis -- "what do authors say about X?" |

If F43 is implemented, Doctrine Briefs could **become** the content of atomic notes. The workflow would be:

```mermaid
flowchart LR
    CHUNKS["Enriched chunks<br/>(tagged with instituto)"]
    SYNTH["Doctrine Synthesis Engine<br/>(F43)"]
    BRIEF["Doctrine Brief<br/>(structured output)"]
    NOTE["Atomic Note<br/>(in nodes/)"]
    CACHE["Cached & versioned"]

    CHUNKS -->|"query by instituto"| SYNTH
    SYNTH -->|"LLM synthesis"| BRIEF
    BRIEF -->|"write to nodes/"| NOTE
    NOTE --> CACHE
```

This makes atomic notes the **cached, versioned output** of the synthesis engine rather than just an index of sources. Each note would contain genuine doctrinal analysis, not just metadata aggregation.

:::note
From `INNOVATION_LAYER.md`: "The Synthesis Engine is the answer to 'what does Douto DO that nobody else does?' -- the atomic notes would contain synthesized analysis that no legal tech competitor currently offers."
:::

## Implementation Timeline

| Milestone | Feature | What Happens |
|-----------|---------|-------------|
| v0.2.5 | M06 (metadata quality gate) | Validate 200 chunks for enrichment accuracy. Must be >= 85%. |
| v0.3 | F26, F27 (tests) | Test infrastructure for pipeline components |
| v0.3.5 | F43 (Doctrine Synthesis Engine) | If approved, generates the content for atomic notes |
| v0.5 | F36 (auto-generate atomic notes) | Generate draft notes from enriched chunks |
| v0.5 | F21 (complete knowledge nodes) | Populate `nodes/` directory |

### Pre-requisites

1. **Metadata quality gate (M06) must pass.** If enrichment accuracy is below 85%, auto-generated notes would systematically mislead users. The re-enrichment action enters the pipeline before note generation.
2. **Decision D03 must be made.** The generation strategy (auto/curated/hybrid) determines the implementation approach, timeline, and resource requirements.
3. **Tests must exist (F26, F27).** The atomic note generator will depend on `parse_frontmatter()`, corpus traversal, and possibly `extract_json()`. These must be tested before building on top of them.

## What Exists Today

The only file in `knowledge/nodes/` is `.gitkeep`, which preserves the empty directory in git:

```
knowledge/nodes/
  .gitkeep       # Placeholder ‚Äî no atomic notes exist yet
```

The directory structure is ready. The enriched corpus contains the data. The schema is defined. What remains is the generation strategy decision (D03) and the quality gate (M06) before implementation can begin.

---
# docs/features/knowledge-base/mocs.md
---


# Maps of Content -- MOCs (F09, F10, F11, F19)

`knowledge/mocs/MOC_*.md` -- The second layer of the skill graph. Each MOC catalogs all books within a legal domain, organizing them by sub-topic with wikilinks to book entries, processing statistics, and cross-domain connections.

## What MOCs Are

A Map of Content (MOC) is a curated index file that serves as the entry point for a specific legal domain. Unlike a flat list, MOCs organize books thematically (e.g., "Doutrina Brasileira", "Drafting e Redacao Contratual", "Doutrina Internacional") so researchers can navigate by sub-topic rather than alphabetically.

Each MOC answers three questions:
1. **What books do we have** in this domain?
2. **How are they organized** thematically?
3. **What other domains** does this one connect to?

## MOC Format

All MOCs follow a standard structure defined in `CLAUDE.md`:

### Frontmatter

```yaml
type: moc
domain: civil
description: Direito Civil ‚Äî teoria geral dos contratos, obrigacoes,
             responsabilidade civil, interpretacao contratual
key_authors: [Orlando Gomes, Fabio Ulhoa Coelho, Pontes de Miranda,
              Melvin Eisenberg]
total_obras: 35
total_chunks: ~9365
```

| Field | Type | Description |
|-------|------|-------------|
| `type` | `str` | Always `"moc"` |
| `domain` | `str` | Domain identifier (matches INDEX_DOUTO.md) |
| `description` | `str` | Brief description of the domain coverage |
| `key_authors` | `list[str]` | Principal authors in the domain |
| `total_obras` | `int` | Number of cataloged books |
| `total_chunks` | `str` | Approximate chunk count (prefixed with `~`) |
| `key_legislation` | `list[str]` | Key statutes (used in MOC_CONSUMIDOR) |

### Content Structure

```markdown
# Domain Title

Brief description of the domain and its focus.

## Sub-topic 1
- [[book-slug]] ‚Äî Author, brief description

## Sub-topic 2
- [[book-slug]] ‚Äî Author, brief description

## Conexoes
- -> [[MOC_OTHER]] (relationship description)
```

Books are referenced via wikilinks (`[[book-slug]]`) that resolve to book directories in the vault staging area.


## Active MOCs

### MOC_CIVIL.md (F09)

**File:** `knowledge/mocs/MOC_CIVIL.md`

| Property | Value |
|----------|-------|
| Books cataloged | 35 |
| Approximate chunks | ~9,365 |
| Key authors | Orlando Gomes, Fabio Ulhoa Coelho, Pontes de Miranda, Melvin Eisenberg |
| Status | Active -- largest by book count |

The largest MOC by number of books. Organized into 5 thematic sub-sections:

| Sub-section | Books | Coverage |
|-------------|-------|----------|
| Doutrina Brasileira | 8 | Orlando Gomes, Fabio Ulhoa Coelho, factoring, opcoes, GVLaw, compilacoes |
| Doutrina Internacional -- Teoria | 14 | Chitty on Contracts, Anson's Law, McKendrick, Eisenberg, Benson, Singh, choice of law |
| Interpretacao e Hermeneutica | 3 | Contract interpretation, Scalia, UNIDROIT Principles |
| Drafting e Redacao Contratual | 8 | Kenneth Adams, Fontaine (international), copyright drafting, legal writing |
| Gestao e Negociacao Contratual | 5 | IACCM, contract management, financial elements, negotiation |

**Cross-domain connections:**
- MOC_EMPRESARIAL (contratos empresariais, venture capital)
- MOC_PROCESSUAL (cumprimento de sentenca, execucao contratual)
- MOC_COMPLIANCE (smart contracts, legal tech)

### MOC_PROCESSUAL.md (F10)

**File:** `knowledge/mocs/MOC_PROCESSUAL.md`

| Property | Value |
|----------|-------|
| Books cataloged | 8 |
| Approximate chunks | ~22,182 |
| Key authors | Nelson Nery Jr., Wambier, Marinoni, Mitidiero, Arenhart |
| Status | Active -- largest by chunk count |

The largest MOC by chunk count. CPC commentaries are massive multi-volume works that produce many chunks per book. Organized into 4 sub-sections:

| Sub-section | Books | Coverage |
|-------------|-------|----------|
| CPC Comentado | 2 | Gaio Jr./Cleyson Mello, Nelson Nery Jr./Rosa Maria Nery |
| Teoria Geral do Processo | 2 | Wambier vol. 1, Marinoni/Mitidiero vol. 1 |
| Cognicao e Procedimento Comum | 2 | Wambier vol. 2, Marinoni et al. vol. 2 |
| Procedimentos Diferenciados e Tutelas | 1 | Marinoni et al. vol. 3 |
| Estrategia Processual | 1 | Serie GVLaw |

**Cross-domain connections:**
- MOC_CIVIL (direito material subjacente)
- MOC_CONSUMIDOR (inversao do onus, tutelas de urgencia em consumo)

### MOC_EMPRESARIAL.md (F11)

**File:** `knowledge/mocs/MOC_EMPRESARIAL.md`

| Property | Value |
|----------|-------|
| Books cataloged | 7 |
| Approximate chunks | -- <!-- NEEDS_INPUT: total chunk count for empresarial MOC --> |
| Key authors | Paula Forgioni, Doug Cumming |
| Status | Active |

Organized into 4 sub-sections:

| Sub-section | Books | Coverage |
|-------------|-------|----------|
| Venture Capital e Private Equity | 2 | Doug Cumming (international), economic valuation |
| Contratos Empresariais Especializados | 4 | Agile contracts, construction claims, commercial drafting, international contracts |
| Legal Tech | 3 | Smart contracts/blockchain, IT acquisitions, contract design |
| Litigio Comercial | 1 | Fentiman (international commercial litigation) |

:::note
Some book entries in MOC_EMPRESARIAL appear in multiple sub-sections. The total unique books listed in the file is 7, but 10 wikilinks appear because the Legal Tech section overlaps with specialized contracts.
:::

**Cross-domain connections:**
- MOC_CIVIL (teoria geral dos contratos)
- MOC_COMPLIANCE (governanca, LGPD em contratos tech)


## Placeholder MOC

### MOC_CONSUMIDOR.md (F19)

**File:** `knowledge/mocs/MOC_CONSUMIDOR.md`

| Property | Value |
|----------|-------|
| Books cataloged | 0 |
| Approximate chunks | 0 |
| Key legislation | CDC (Lei 8.078/90) |
| Status | In Progress (~10%) -- placeholder with structure only |

The file exists with the correct frontmatter and section structure but no books have been cataloged:

```markdown
## Fundamentos
(a preencher -- relacao de consumo, principios do CDC)

## Responsabilidade Civil
(a preencher -- fato/vicio do produto/servico)

## Praticas Abusivas
(a preencher -- clausulas abusivas, publicidade enganosa)
```

**Cross-domain connections (defined):**
- MOC_CIVIL (responsabilidade civil geral)
- MOC_PROCESSUAL (inversao do onus, tutela de urgencia)

**To complete:** Identify and catalog consumer law books in the existing vault, or process new PDFs through the pipeline.


## Planned MOCs

> **Planned Feature** -- These MOCs are on the roadmap (F25, P1, milestone v0.3) but have not been created yet.

| MOC | Domain | Description | Known Books |
|-----|--------|-------------|-------------|
| `MOC_TRIBUTARIO` | Direito Tributario | Obrigacao tributaria, credito tributario, processo administrativo fiscal | <!-- NEEDS_INPUT: any tax law books in the vault? --> |
| `MOC_CONSTITUCIONAL` | Direito Constitucional | Direitos fundamentais, controle de constitucionalidade, hermeneutica constitucional | <!-- NEEDS_INPUT: any constitutional law books in the vault? --> |
| `MOC_COMPLIANCE` | Compliance & Governanca | LGPD, governanca corporativa, due diligence, anticorrupcao | <!-- NEEDS_INPUT: any compliance books in the vault? --> |
| `MOC_SUCESSOES` | Sucessoes & Planejamento Patrimonial | Inventario, testamento, holdings familiares, planejamento sucessorio | <!-- NEEDS_INPUT: any succession law books in the vault? --> |

:::tip
Even without books to catalog, creating placeholder MOCs (like `MOC_CONSUMIDOR.md`) is valuable because it completes the navigational structure in `INDEX_DOUTO.md` and makes the domain visible to agents and scripts that traverse the knowledge base.
:::


## How to Add a New MOC

### Step 1: Create the MOC file

Create `knowledge/mocs/MOC_{DOMAIN}.md` with the standard frontmatter:

```yaml
type: moc
domain: {domain_id}
description: {brief description of the domain}
key_authors: []
total_obras: 0
total_chunks: 0
```

### Step 2: Add section structure

Define thematic sub-sections relevant to the domain. Use other MOCs as templates.

### Step 3: Catalog existing books

If books for this domain already exist in the vault (processed and enriched), add wikilinks:

```markdown
## Sub-topic
- [[book-slug]] ‚Äî Author, brief description
```

### Step 4: Link from INDEX_DOUTO.md

Verify that `INDEX_DOUTO.md` already has a wikilink `[[MOC_{DOMAIN}]]` pointing to the new file. All 8 domains are already listed in the index.

### Step 5: Process new books (if needed)

For new PDFs:

```bash
# Place PDF in staging
cp new-book.pdf $VAULT_PATH/Knowledge/_staging/input/

# Run the full pipeline
python3 pipeline/process_books.py
python3 pipeline/rechunk_v3.py
python3 pipeline/enrich_chunks.py all
python3 pipeline/embed_doutrina.py
```

Then add the book entry to the MOC file and update `total_obras` and `total_chunks` in the frontmatter.


## Corpus Statistics Summary

| MOC | Books | Chunks | % of Total Chunks |
|-----|-------|--------|-------------------|
| MOC_CIVIL | 35 | ~9,365 | ~30% |
| MOC_PROCESSUAL | 8 | ~22,182 | ~70% |
| MOC_EMPRESARIAL | 7 | -- | -- |
| MOC_CONSUMIDOR | 0 | 0 | 0% |
| **Total (active)** | **50** | **~31,547** | **100%** |

:::note
The chunk count disparity between Civil (35 books, ~9K chunks) and Processual (8 books, ~22K chunks) reflects the nature of the source material: CPC commentaries are massive multi-volume works where each article generates multiple chunks, while contract law textbooks are typically shorter and more focused.
:::

---
# docs/features/knowledge-base/skill-graph.md
---


# Skill Graph (F08)

`knowledge/INDEX_DOUTO.md` -- The root node of Douto's knowledge base. It maps 8 legal domains into a navigable hierarchy following the Obsidian knowledge management pattern: INDEX (root) leads to MOCs (Maps of Content) which lead to individual book entries, which will eventually lead to atomic notes.

## Overview

The skill graph serves two purposes:

1. **Human navigation** -- A lawyer or researcher can start at `INDEX_DOUTO.md`, click through to a domain MOC, find a relevant book, and drill down to the specific chapter or legal concept they need.
2. **Programmatic access** -- Each file has YAML frontmatter with structured metadata (`type`, `domain`, `description`), enabling scripts and agents to traverse the knowledge base, discover content, and build aggregations.

```mermaid
graph TD
    INDEX["INDEX_DOUTO.md<br/>(root node)"]

    CIVIL["MOC_CIVIL.md<br/>35 books, ~9,365 chunks"]
    PROC["MOC_PROCESSUAL.md<br/>8 books, ~22,182 chunks"]
    EMP["MOC_EMPRESARIAL.md<br/>7 books"]
    CONS["MOC_CONSUMIDOR.md<br/>placeholder"]

    TRIB["MOC_TRIBUTARIO.md<br/>NOT YET CREATED"]
    CONST["MOC_CONSTITUCIONAL.md<br/>NOT YET CREATED"]
    COMP["MOC_COMPLIANCE.md<br/>NOT YET CREATED"]
    SUC["MOC_SUCESSOES.md<br/>NOT YET CREATED"]

    INDEX --> CIVIL
    INDEX --> PROC
    INDEX --> EMP
    INDEX --> CONS
    INDEX --> TRIB
    INDEX --> CONST
    INDEX --> COMP
    INDEX --> SUC

    style CIVIL fill:#2d5016,color:#fff
    style PROC fill:#2d5016,color:#fff
    style EMP fill:#2d5016,color:#fff
    style CONS fill:#7a6c00,color:#fff
    style TRIB fill:#8b0000,color:#fff
    style CONST fill:#8b0000,color:#fff
    style COMP fill:#8b0000,color:#fff
    style SUC fill:#8b0000,color:#fff
```

## The 8 Domains

| # | Domain | MOC File | Status | Books | Chunks |
|---|--------|----------|--------|-------|--------|
| 1 | Direito Civil | `MOC_CIVIL.md` | Active | 35 | ~9,365 |
| 2 | Direito Processual Civil | `MOC_PROCESSUAL.md` | Active | 8 | ~22,182 |
| 3 | Direito Empresarial | `MOC_EMPRESARIAL.md` | Active | 7 | -- |
| 4 | Direito do Consumidor | `MOC_CONSUMIDOR.md` | Placeholder | 0 | 0 |
| 5 | Direito Tributario | -- | Missing | -- | -- |
| 6 | Direito Constitucional | -- | Missing | -- | -- |
| 7 | Compliance & Governanca | -- | Missing | -- | -- |
| 8 | Sucessoes & Planejamento Patrimonial | -- | Missing | -- | -- |

**Summary:** 4 MOC files exist (3 active, 1 placeholder). 4 MOC files are missing. Creating them is tracked as **F25** (P1, milestone v0.3).

## INDEX_DOUTO.md Structure

The index file uses a specific format defined in `CLAUDE.md`:

### Frontmatter

```yaml
type: skill-graph-index
description: Mapa do conhecimento juridico doutrinario da FS.
              Ponto de entrada para navegacao tematica.
domains: [civil, consumidor, empresarial, tributario,
          processual, constitucional, compliance, sucessorio]
```

### Content

Each domain is listed under a heading with a wikilink to its MOC and a brief description of coverage:

```markdown
### Direito Civil
[[MOC_CIVIL]] ‚Äî obrigacoes, contratos, responsabilidade civil, propriedade

### Direito do Consumidor
[[MOC_CONSUMIDOR]] ‚Äî relacao de consumo, responsabilidade objetiva, praticas abusivas
```

### Cross-Domain Links

The index also references cross-cutting concepts that span multiple domains:

```markdown
## Cross-Domain
- [[teoria_geral_do_direito]] ‚Äî conceitos que cruzam areas
- [[principios_constitucionais]] ‚Äî base para todos os ramos
```

:::note
The cross-domain notes (`teoria_geral_do_direito`, `principios_constitucionais`) are referenced via wikilinks but do not yet exist as files. They are part of the planned atomic notes layer (F21/F36).
:::

## Navigation Pattern

Douto's knowledge base follows three conventions from `CLAUDE.md`:

### 1. Wikilinks for Internal Navigation

All internal references use Obsidian-style wikilinks (`[[target]]`), never markdown relative links. This ensures compatibility with Obsidian's graph view and link resolution, and makes it trivial for scripts to parse and traverse connections.

```markdown
# In INDEX_DOUTO.md:
[[MOC_CIVIL]] ‚Äî obrigacoes, contratos...

# In MOC_CIVIL.md:
[[contratos-orlando-gomes]] ‚Äî Orlando Gomes, tratado classico

# In a chunk file:
[[MOC_CIVIL]] (back-link to parent MOC)
```

### 2. Frontmatter for Programmatic Access

Every knowledge base file has YAML frontmatter with structured metadata:

| File Type | Required Fields |
|-----------|----------------|
| INDEX | `type: skill-graph-index`, `description`, `domains[]` |
| MOC | `type: moc`, `domain`, `description`, `key_authors[]`, `total_obras`, `total_chunks` |
| Book entry | `tipo: livro_chunk`, `titulo`, `livro_titulo`, `autor`, `area_direito[]`, `status_enriquecimento` |
| Atomic note (planned) | `tipo: nota`, `instituto`, `sub_instituto`, `ramo`, `fontes_normativas[]`, `autores[]`, `chunks_origem[]` |

### 3. Hierarchical Traversal

The knowledge base forms a strict tree:

```
INDEX_DOUTO.md          (1 file ‚Äî root)
  |
  +-- mocs/MOC_*.md     (8 files ‚Äî one per domain)
  |     |
  |     +-- book entries  (per-book directories in staging)
  |           |
  |           +-- chunks   (individual chapter .md files)
  |
  +-- nodes/            (planned ‚Äî one file per legal concept)
```

This hierarchy enables both top-down navigation (domain exploration) and bottom-up aggregation (collecting all chunks for a given legal concept across books).

## Relationship to Other Features

| Feature | Relationship |
|---------|-------------|
| [MOCs](mocs.md) (F09-F11, F19) | MOCs are the second layer of the skill graph, directly linked from INDEX |
| [Atomic Notes](atomic-notes.md) (F21, F36) | Planned third layer -- one note per legal concept |
| F25 (Create missing MOCs) | Completes the skill graph by filling in 4 missing domains |
| F43 (Doctrine Synthesis Engine) | Would generate Doctrine Briefs that feed into atomic notes |

## Planned Improvements

> **Planned Feature** -- The following improvements are on the roadmap but not yet implemented.

- **F25: Create 4 missing MOCs** (P1, v0.3) -- `MOC_TRIBUTARIO`, `MOC_CONSTITUCIONAL`, `MOC_COMPLIANCE`, `MOC_SUCESSOES`. Even as placeholders with minimal structure, their existence completes the navigational map.
- **F36: Atomic notes from enriched chunks** (P2, v0.5) -- Generate one note per unique `instituto` from enrichment metadata, creating the third layer of the hierarchy.
- **F21: Complete knowledge nodes** (In Progress, v0.5) -- The `nodes/` directory exists with only a `.gitkeep`. Populating it depends on decision **D03** (auto-generate vs. curate vs. hybrid).

---
# docs/features/pipeline/embeddings.md
---


# Embedding Generation (F04)

`pipeline/embed_doutrina.py` -- Generates 768-dimensional semantic embeddings using Legal-BERTimbau, with a metadata-enriched text composition strategy that embeds not just the chunk body but also its legal classification. This is what enables semantic search over the doctrinal corpus.

## Overview

| Property | Value |
|----------|-------|
| **Script** | `pipeline/embed_doutrina.py` (345 lines) |
| **Input** | Enriched chunks with `status_enriquecimento: "completo"` |
| **Output** | Three JSON files: embeddings, search corpus, BM25 index |
| **Model** | `rufimelo/Legal-BERTimbau-sts-base` (768-dim) |
| **Max tokens** | 512 (`MAX_TOKENS`) |
| **Batch size** | 32 (`BATCH_SIZE`) |
| **Device** | CUDA if available, otherwise CPU |

## Text Composition Strategy

This is the most important design decision in the embedding stage. Instead of embedding raw chunk text, Douto composes a structured prefix that captures the legal classification within the BERT token limit.

### Format

```
[categoria] | [institutos] | [tipo_conteudo] | [titulo] | [corpo_truncado]
```

### Why This Matters

Legal-BERTimbau has a hard 512-token limit (~2,000 characters in Portuguese). A raw chunk body would fill the entire token budget with text, losing the semantic signal from its classification. By prefixing with structured metadata, the embedding captures both *what* the text is about (instituto, categoria) and *what kind* of content it is (definicao vs. jurisprudencia vs. exemplo) in the vector space.

This means a search for "requisitos da exceptio non adimpleti contractus" will match chunks classified as `instituto: ["exceptio_non_adimpleti_contractus"]` + `tipo_conteudo: ["requisitos"]` even if the body text uses different phrasing.

### The Actual Code

```python
def compose_embedding_text(fm: dict, body: str) -> str:
    parts = []

    # 1. Categoria geral
    cat = fm.get("categoria", "")
    if cat:
        parts.append(cat)

    # 2. Institutos juridicos (most important for search)
    institutos = fm.get("instituto", [])
    if isinstance(institutos, list) and institutos:
        parts.append(", ".join(i.replace("_", " ") for i in institutos[:5]))

    # 3. Tipo de conteudo (definicao, jurisprudencia, etc.)
    tipos = fm.get("tipo_conteudo", [])
    if isinstance(tipos, list) and tipos:
        parts.append(", ".join(t.replace("_", " ") for t in tipos[:3]))

    # 4. Titulo da secao
    titulo = fm.get("titulo", "")
    if titulo:
        titulo_clean = re.sub(r'\(cont\.\s*\d+\)', '', titulo).strip()
        titulo_clean = re.sub(r'^\d+\.\s*', '', titulo_clean).strip()
        if titulo_clean:
            parts.append(titulo_clean[:150])

    # 5. Corpo ‚Äî truncate to fit model (512 tokens ~ 2000 chars PT)
    body_clean = body.strip()
    body_clean = re.sub(r'^>.*\n', '', body_clean, flags=re.MULTILINE).strip()
    body_clean = re.sub(r'^#+\s+.*\n', '', body_clean, count=1).strip()

    prefix = " | ".join(parts)

    # BERTimbau: ~512 tokens, ~4 chars/token PT -> ~2000 chars
    max_body = 1800 - len(prefix)
    if max_body < 200:
        max_body = 200

    text = f"{prefix} | {body_clean[:max_body]}"
    return text
```

### Example Output

```
contratos | contrato bilateral, exceptio non adimpleti contractus |
definicao, requisitos | Contratos bilaterais e unilaterais |
A exceptio non adimpleti contractus e a defesa que pode ser oposta
pelo contratante demandado quando o outro nao cumpriu...
```

:::tip
The underscores in instituto names are replaced with spaces before embedding (`i.replace("_", " ")`) so the BERT tokenizer treats them as natural language rather than single tokens.
:::

## Corpus Entry Structure

For each chunk, `build_corpus_entry()` creates a metadata document used for search result display and metadata filtering:

```python
def build_corpus_entry(fm: dict, body: str, filepath: Path) -> dict:
    return {
        "id": filepath.stem,           # e.g., "026-contratos-bilaterais"
        "livro": fm.get("livro_titulo", ""),
        "autor": fm.get("autor", ""),
        "titulo": fm.get("titulo", ""),
        "chunk_numero": fm.get("chunk_numero", 0),
        "chunk_total": fm.get("chunk_total", 0),
        "categoria": fm.get("categoria", ""),
        "instituto": fm.get("instituto", []),
        "sub_instituto": fm.get("sub_instituto", []),
        "tipo_conteudo": fm.get("tipo_conteudo", []),
        "fase": fm.get("fase", []),
        "ramo": fm.get("ramo", ""),
        "fontes_normativas": fm.get("fontes_normativas", []),
        "confiabilidade": fm.get("confiabilidade", ""),
        "livro_dir": filepath.parent.name,
        "texto": body[:3000],           # Preview for display
    }
```

## Output Files

The script produces three files, all JSON, designed for compatibility with the existing Juca/Valter infrastructure.

| File | Contents | Used By |
|------|----------|---------|
| `embeddings_doutrina.json` | `doc_ids[]` + `embeddings[][]` (768-dim float32, normalized) + model metadata | Semantic search (`search_doutrina_v2.py`) |
| `search_corpus_doutrina.json` | Full metadata per chunk (15 fields from `build_corpus_entry`) | Result display and metadata filtering |
| `bm25_index_doutrina.json` | `doc_ids[]` + `documents[]` (composed text, same as embedding input) | BM25 keyword search |

### Embeddings JSON structure

```json
{
  "model": "rufimelo/Legal-BERTimbau-sts-base",
  "dimension": 768,
  "num_docs": 9365,
  "min_val": -0.123,
  "max_val": 0.456,
  "created_at": "2026-02-28T14:30:00",
  "doc_ids": ["book-dir/001-chapter-slug", "..."],
  "embeddings": [[0.012, -0.034, ...], ...]
}
```

:::note
Embeddings are stored as flat JSON arrays, not as binary formats like FAISS or HNSW indexes. This means the entire embedding matrix must be loaded into memory for search. For the current corpus (~31,500 chunks, 768 dims), this requires approximately 500 MB of RAM. This will not scale past ~100 books without migrating to a vector database (tracked as mitigation **M12** -- Qdrant migration).
:::

## Chunk Collection and Filtering

Before embedding, `collect_chunks()` filters the corpus:

1. **Skip `_` prefixed files** (index files like `_INDEX.md`, `_RAW_FULL.md`)
2. **Skip `status_enriquecimento: "lixo"`** (noise chunks)
3. **Skip chunks with body < 200 characters**
4. **Skip unenriched chunks** (no `instituto` and no `tipo_conteudo` -- not yet classified)

This ensures only substantive, classified chunks enter the embedding index.

## Configuration

### Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `VAULT_PATH` | No | `/mnt/c/Users/sensd/vault` | Base directory with enriched chunks |
| `OUTPUT_PATH` | No | `/home/sensd/.openclaw/workspace/juca/data` | Where to write output JSON files |

### CLI Arguments

```bash
# Generate embeddings with default settings
python3 pipeline/embed_doutrina.py

# Specify output directory
python3 pipeline/embed_doutrina.py --output /path/to/output

# Preview without generating embeddings
python3 pipeline/embed_doutrina.py --dry-run

# Limit chunks for testing
python3 pipeline/embed_doutrina.py --limit 100

# Adjust batch size (default: 32)
python3 pipeline/embed_doutrina.py --batch-size 16
```

The `--dry-run` mode is especially useful: it shows chunk counts per book and sample composed texts without loading the BERT model or generating embeddings.

## Performance

The embedding process has two phases:

1. **Model loading** -- 5-15 seconds depending on hardware and caching
2. **Encoding** -- speed depends on device:

| Device | Approximate Speed |
|--------|-------------------|
| CUDA (GPU) | ~500-1000 chunks/s |
| CPU | ~30-50 chunks/s |

For the full corpus (~9,000+ chunks per area), CPU encoding takes approximately 3-5 minutes.

Embeddings are normalized (`normalize_embeddings=True`) so that cosine similarity can be computed as a simple dot product, avoiding the need for normalization at search time.

## Known Limitations

- **Flat JSON storage** -- no HNSW/FAISS/Qdrant indexing. Search is brute-force dot product over the entire matrix. Works for the current scale but will not scale past ~100 books. Tracked as mitigation **M12**.
- **512-token truncation** -- chunks longer than ~2,000 characters (after prefix) lose tail content. The prefix takes 100-400 characters, leaving 1,400-1,700 characters for the body. Long legal analyses may have important content truncated.
- **Metadata pollution** -- if enrichment metadata is wrong (e.g., `instituto` misclassified), the embedding captures incorrect semantics. Since the prefix is weighted first in the token sequence, bad metadata actively harms search quality. This is why the metadata quality gate (**M06**) is critical.
- **Legal-BERTimbau trained on PT-PT** -- the model was trained on Portuguese from Portugal, not Brazilian Portuguese. While the languages are mutually intelligible, there may be subtle vocabulary mismatches for Brazilian legal terminology.
- **No incremental update** -- adding a new book requires regenerating the entire embedding file. There is no append-only or incremental index update mechanism.
- **BM25 index reuses composed text** -- the BM25 index uses the same metadata-prefixed text as embeddings. This means BM25 keyword search matches against metadata terms (instituto names, categories) in addition to body text, which may inflate relevance of metadata-matching but content-irrelevant results.

---
# docs/features/pipeline/enrichment.md
---


# Chunk Enrichment (F03)

`pipeline/enrich_chunks.py` -- Classifies each chunk using the MiniMax M2.5 LLM to add structured legal metadata. This is the step that transforms raw text into searchable, filterable knowledge by tagging each chunk with its legal concept (`instituto`), content type, branch of law, procedural phase, and normative sources.

## Overview

| Property | Value |
|----------|-------|
| **Script** | `pipeline/enrich_chunks.py` (403 lines) |
| **Input** | Chunked markdown files with `status_enriquecimento: "pendente"` |
| **Output** | Same files with enriched frontmatter (13+ metadata fields) |
| **LLM** | MiniMax M2.5 via Anthropic SDK with custom `base_url` |
| **Concurrency** | 5 threads, 0.5s delay between requests |
| **Idempotent** | Yes -- skips chunks with `status_enriquecimento: "completo"` or `"lixo"` |

:::danger
**CRITICAL: `enrich_prompt.md` is MISSING from the repository.** Line 27 references a prompt file that does not exist in the repo:
```python
PROMPT_PATH = Path(__file__).parent / "enrich_prompt.md"
```
This means the prompt that generated all metadata for ~31,500 chunks is not version-controlled. If lost, all enrichment metadata becomes unreproducible. This is tracked as **F42** (P1 priority, v0.3) and flagged as an existential risk in the ROADMAP.
:::

## How It Works

```mermaid
flowchart TD
    LOAD["Load enrich_prompt.md template"]
    SCAN["Scan for unenriched chunks"]
    NOISE["Check: is_noise_chunk?"]
    SKIP["Mark as lixo, skip"]
    SEND["Send to MiniMax M2.5"]
    PARSE["extract_json() from response"]
    MERGE["merge_classification() into frontmatter"]
    WRITE["Write enriched file"]

    LOAD --> SCAN
    SCAN --> NOISE
    NOISE -->|"Yes (preface, dedication, <200 chars)"| SKIP
    NOISE -->|"No"| SEND
    SEND --> PARSE
    PARSE -->|"Valid JSON"| MERGE
    PARSE -->|"Invalid/empty"| ERR["Log error, counter++"]
    MERGE --> WRITE
```

### Step 1: Load prompt template

The enrichment prompt is loaded from `pipeline/enrich_prompt.md`. Placeholder variables are substituted per chunk:
- `{livro_titulo}` -- book title
- `{autor}` -- author name
- `{capitulo}` -- chapter title
- `{chunk_numero}` / `{chunk_total}` -- chunk position
- `{chunk_content}` -- first 8,000 characters of the chunk body

### Step 2: Noise detection

Before sending to the LLM, each chunk is checked against noise patterns. Chunks matching any of these are marked as `"lixo"` (trash) without API calls:

- Title contains: "prefacio", "agradecimento", "dedicatoria", "nota do editor", "sobre o autor", etc.
- Body text is shorter than 200 characters

### Step 3: LLM classification

The chunk text + prompt is sent to MiniMax M2.5 via the Anthropic SDK with a custom base URL:

```python
client = anthropic.Anthropic(
    api_key=api_key,
    base_url="https://api.minimax.io/anthropic"
)

message = client.messages.create(
    model="MiniMax-M2.5",
    max_tokens=2000,
    system="Voce e um classificador juridico especializado. "
           "Responda APENAS com JSON valido, sem markdown, sem backticks, sem explicacoes.",
    messages=[{"role": "user", "content": prompt}]
)
```

### Step 4: JSON extraction

The LLM response is parsed using `extract_json()`, which has a brace-matching fallback for cases where the LLM wraps the JSON in markdown code fences or adds surrounding text:

```python
def extract_json(text: str) -> dict:
    text = text.strip()
    text = re.sub(r'^```json\s*', '', text)
    text = re.sub(r'\s*```$', '', text)
    text = text.strip()

    # Try direct parse first
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # Extract first complete JSON object by brace matching
    start = text.find('{')
    if start == -1:
        raise json.JSONDecodeError("No JSON object found", text, 0)

    depth = 0
    in_string = False
    escape = False
    for i in range(start, len(text)):
        c = text[i]
        if escape:
            escape = False
            continue
        if c == '\\' and in_string:
            escape = True
            continue
        if c == '"' and not escape:
            in_string = not in_string
            continue
        if in_string:
            continue
        if c == '{':
            depth += 1
        elif c == '}':
            depth -= 1
            if depth == 0:
                return json.loads(text[start:i+1])

    raise json.JSONDecodeError("Incomplete JSON object", text, len(text))
```

:::tip
The brace-matching approach is robust against common LLM output issues: markdown fencing, preamble text, and trailing explanations. It correctly handles nested objects and escaped characters within strings.
:::

### Step 5: Merge classification

The parsed JSON fields are merged into the existing frontmatter via `merge_classification()`, and status metadata is appended:

```python
enriched["status_enriquecimento"] = "completo"
enriched["data_enriquecimento"] = datetime.now().isoformat()
enriched["modelo_enriquecimento"] = "MiniMax-M2.5"
```

## Metadata Schema

The `merge_classification()` function maps 13 fields from the LLM response into the chunk frontmatter. These fields are the foundation for all downstream search and filtering.

| # | Field | Type | Description | Example Values |
|---|-------|------|-------------|----------------|
| 1 | `categoria` | `str` | High-level category | `"doutrina"`, `"legislacao_comentada"` |
| 2 | `tipo_contratual` | `str` | Contract type (if applicable) | `"compra_e_venda"`, `"locacao"` |
| 3 | `objeto_especifico` | `str` | Specific subject matter | `"clausula_penal"`, `"exceptio"` |
| 4 | `instituto` | `list[str]` | Legal concepts/institutes | `["exceptio_non_adimpleti_contractus", "contrato_bilateral"]` |
| 5 | `sub_instituto` | `list[str]` | Sub-concepts | `["inadimplemento_relativo"]` |
| 6 | `fase` | `list[str]` | Contract/procedural phase | `["formacao", "execucao", "extincao"]` |
| 7 | `ramo` | `str` | Branch of law | `"direito_civil"`, `"processo_civil"` |
| 8 | `fontes_normativas` | `list[str]` | Statutory references | `["CC art. 476", "CC art. 477"]` |
| 9 | `tipo_conteudo` | `list[str]` | Content type classification | `["definicao", "requisitos", "jurisprudencia_comentada"]` |
| 10 | `utilidade` | `str` | Practical utility rating | `"alta"`, `"media"`, `"baixa"` |
| 11 | `confiabilidade` | `str` | Source reliability | `"alta"`, `"media"` |
| 12 | `jurisdicao_estrangeira` | `bool` or `str` | Foreign jurisdiction reference | `false`, `"common_law"` |
| 13 | `justificativa` | `str` | LLM reasoning for classification | Free text explaining the tagging logic |

In addition to these 13 LLM-provided fields, `merge_classification()` adds 3 system fields:

| Field | Type | Value |
|-------|------|-------|
| `status_enriquecimento` | `str` | `"completo"` |
| `data_enriquecimento` | `str` | ISO 8601 timestamp |
| `modelo_enriquecimento` | `str` | `"MiniMax-M2.5"` |

## Configuration

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `MINIMAX_API_KEY` | Yes (unless `--dry-run`) | MiniMax API authentication |
| `VAULT_PATH` | No | Base directory (default: `/mnt/c/Users/sensd/vault`) |

### CLI Arguments

```bash
# Enrich all unenriched chunks across all books
python3 pipeline/enrich_chunks.py all

# Enrich a specific book
python3 pipeline/enrich_chunks.py contratos-orlando-gomes

# Re-enrich already completed chunks
python3 pipeline/enrich_chunks.py all --force

# Limit to first N chunks (for testing)
python3 pipeline/enrich_chunks.py all --limit 10

# Preview without API calls
python3 pipeline/enrich_chunks.py all --dry-run

# Use a specific API key
python3 pipeline/enrich_chunks.py all --api-key "sk-..."

# Adjust thread count (default: 5)
python3 pipeline/enrich_chunks.py all --workers 3
```

### Concurrency Settings

| Setting | Value | Description |
|---------|-------|-------------|
| `WORKERS` | 5 | Number of concurrent threads |
| `DELAY_BETWEEN_REQUESTS` | 0.5s | Delay after each API call (per thread) |

The estimated processing time is displayed at startup:
```
Estimativa: ~{(total * 0.5) / workers / 60:.0f} min
```

For 1,000 chunks with 5 workers: approximately 1.7 minutes.

## Logging

Results are appended to `Logs/enrichment_log.jsonl`:

```json
{
  "timestamp": "2026-02-28T14:30:00",
  "file": "/path/to/chunk.md",
  "success": true,
  "model": "MiniMax-M2.5",
  "tags_count": 3,
  "tipo_conteudo": ["definicao", "requisitos"]
}
```

## Known Issues

:::danger
**Missing enrichment prompt (F42):** The prompt file `pipeline/enrich_prompt.md` that generated metadata for the entire corpus is not in the repository. Recovery options:
1. Locate it in the original vault or MiniMax API history
2. Reconstruct from enrichment output patterns
3. Write a new prompt and re-enrich (expensive but ensures reproducibility)
:::

- **No schema validation on LLM output.** If the LLM returns unexpected field values (e.g., `ramo: "unknown_branch"`), the data is accepted without validation. Invalid metadata propagates to embeddings and search. Tracked as mitigation **M10** in the ROADMAP.
- **Broad `except Exception` catches** in `classify_chunk()` and `process_one()` swallow all errors, including API failures, rate limits, and network issues. The error counter increments but the specific cause is not logged in `classify_chunk()`.
- **No accuracy measurement.** The quality of LLM classification has never been validated against human judgment. Mitigation **M06** proposes sampling 200 chunks for manual review. If accuracy is below 85%, all enrichment should be redone.
- **MiniMax via Anthropic SDK** uses an undocumented compatibility layer. If MiniMax changes their API, the integration could break silently. Decision **D06** in the ROADMAP considers migrating to Claude or local models.
- **Thread safety** relies on `threading.Lock()` for counters and log writes. The `anthropic.Anthropic` client is shared across threads without explicit documentation that it is thread-safe.
- **Chunk content is truncated to 8,000 characters** before sending to the LLM. Longer chunks may have important content in their tail that the classifier never sees.

---
# docs/features/pipeline/hybrid-search.md
---


# Hybrid Search (F05, F06, F07)

`pipeline/search_doutrina_v2.py` -- The query interface for the doctrinal corpus. Combines semantic search (cosine similarity on Legal-BERTimbau embeddings) with BM25 keyword search, post-retrieval metadata filtering, and multi-area support. Available as both a CLI tool and an interactive REPL.

## Overview

| Property | Value |
|----------|-------|
| **Script** | `pipeline/search_doutrina_v2.py` (374 lines) |
| **Input** | User query (natural language or legal term) |
| **Indexes** | `embeddings_doutrina.json`, `search_corpus_doutrina.json`, `bm25_index_doutrina.json` |
| **Model** | `rufimelo/Legal-BERTimbau-sts-base` (shared with embedding generation) |
| **Default mode** | Hybrid (0.7 semantic + 0.3 BM25) |
| **Areas** | `contratos`, `processo_civil`, or `all` |

## Search Modes

### Semantic Search

Encodes the user query with Legal-BERTimbau (same model used for indexing), then computes cosine similarity against all chunk embeddings via numpy dot product:

```python
def semantic_search(query: str, areas: list[str], top_k: int = 10):
    model = load_model()
    query_vec = model.encode(
        [query],
        normalize_embeddings=True,
        convert_to_numpy=True
    )

    for area in areas:
        data = load_area(area)
        emb = data["embeddings"]
        scores = np.dot(emb["vectors"], query_vec.T).flatten()
        top_idx = np.argsort(scores)[::-1][:top_k]
```

**Best for:** conceptual queries, synonym matching, questions phrased differently from the source text. Example: searching "defense for non-performance" finds chunks about "exceptio non adimpleti contractus."

### BM25 Search

A from-scratch BM25 implementation with standard parameters:

| Parameter | Value | Meaning |
|-----------|-------|---------|
| `k1` | 1.5 | Term frequency saturation |
| `b` | 0.75 | Document length normalization |

```python
def bm25_search(query: str, areas: list[str], top_k: int = 10):
    query_terms = set(re.findall(r'\w+', query.lower()))
    k1, b = 1.5, 0.75

    for area in areas:
        # ...
        for i, doc in enumerate(docs):
            doc_terms = re.findall(r'\w+', doc.lower())
            dl = len(doc_terms)
            tf = Counter(doc_terms)
            score = 0.0
            for t in query_terms:
                if tf[t] == 0:
                    continue
                idf = math.log((N - df[t] + 0.5) / (df[t] + 0.5) + 1)
                tf_norm = (tf[t] * (k1 + 1)) / (tf[t] + k1 * (1 - b + b * dl / avg_dl))
                score += idf * tf_norm
```

**Best for:** exact term matching, Latin legal phrases, article numbers. Example: searching "Art. 476" finds the exact statutory reference.

### Hybrid Search (Default)

Runs both semantic and BM25 searches, normalizes their scores independently using min-max normalization, then combines them with configurable weights:

```python
def hybrid_search(query, areas, top_k=10, semantic_weight=0.7):
    sem_results = semantic_search(query, areas, top_k=top_k * 2)
    bm25_results = bm25_search(query, areas, top_k=top_k * 2)

    def normalize(results):
        if not results:
            return {}
        max_s = max(s for _, s, _ in results)
        min_s = min(s for _, s, _ in results)
        rng = max_s - min_s if max_s != min_s else 1.0
        return {(doc_id, area): (s - min_s) / rng for doc_id, s, area in results}

    sem_norm = normalize(sem_results)
    bm25_norm = normalize(bm25_results)

    all_keys = set(sem_norm.keys()) | set(bm25_norm.keys())
    combined = {}
    for key in all_keys:
        combined[key] = (
            semantic_weight * sem_norm.get(key, 0.0) +
            (1 - semantic_weight) * bm25_norm.get(key, 0.0)
        )
```

**Default weights:** 0.7 semantic + 0.3 BM25.

**Why hybrid?** Semantic search excels at conceptual matching but can miss exact terms. BM25 excels at term matching but misses synonyms and paraphrases. The combination covers both failure modes. The 0.7/0.3 weighting favors semantic understanding while preserving exact-match capability for legal precision.

```mermaid
flowchart LR
    Q["Query"]
    SEM["Semantic Search"]
    BM25["BM25 Search"]
    NORM_S["Min-max normalize"]
    NORM_B["Min-max normalize"]
    COMBINE["0.7 * sem + 0.3 * bm25"]
    FILTER["Metadata filters"]
    RESULTS["Ranked results"]

    Q --> SEM
    Q --> BM25
    SEM --> NORM_S
    BM25 --> NORM_B
    NORM_S --> COMBINE
    NORM_B --> COMBINE
    COMBINE --> FILTER
    FILTER --> RESULTS
```

## Multi-Area Search (F06)

The search system supports independent indexes for different legal areas. Each area has its own set of three JSON files:

| Area | Embeddings | Corpus | BM25 |
|------|-----------|--------|------|
| `contratos` | `embeddings_doutrina.json` | `search_corpus_doutrina.json` | `bm25_index_doutrina.json` |
| `processo_civil` | `embeddings_processo_civil.json` | `search_corpus_processo_civil.json` | `bm25_index_processo_civil.json` |

When `--area all` is used, results from both areas are merged and re-ranked together. The area of origin is displayed in results.

:::note
Adding a new area requires: (1) processing and enriching the books, (2) generating the three JSON files with appropriate names, and (3) adding an entry to the `AREA_FILES` dictionary in the script.
:::

## Metadata Filtering

Post-retrieval filters narrow results by structured metadata. All filters use **case-insensitive substring matching**:

| Filter | Frontmatter Field | CLI Flag | Example |
|--------|-------------------|----------|---------|
| Instituto | `instituto[]` | `--instituto` | `--instituto "boa-fe"` |
| Content type | `tipo_conteudo[]` | `--tipo` | `--tipo "definicao"` |
| Branch of law | `ramo` | `--ramo` | `--ramo "civil"` |
| Book title | `livro` | `--livro` | `--livro "orlando"` |
| Procedural phase | `fase[]` | `--fase` | `--fase "execucao"` |

```python
def filter_by_metadata(results, areas, instituto=None, tipo=None,
                       ramo=None, livro=None, fase=None):
    for doc_id, score, area in results:
        meta = data["corpus"].get(doc_id, {})
        if instituto and not any(
            instituto.lower() in i.lower()
            for i in meta.get("instituto", [])
        ):
            continue
        # ... similar for tipo, ramo, livro, fase
```

:::caution
Filters are applied **after** scoring and ranking, not during retrieval. This means filtered results may have fewer than `top_k` entries. It also means the scoring does not benefit from metadata constraints -- a filter-aware retrieval system would produce better results.
:::

## Interactive Mode (F07)

Launch with `--interativo` or `-i` for a REPL with runtime-configurable search parameters:

```bash
python3 pipeline/search_doutrina_v2.py --interativo
```

### Commands

| Command | Description | Example |
|---------|-------------|---------|
| `/area` | Switch search area | `/area contratos`, `/area processo_civil`, `/area all` |
| `/filtro` | Set metadata filters | `/filtro instituto=boa-fe tipo=definicao` |
| `/verbose` | Toggle text preview in results | `/verbose` |
| `/top N` | Change result count | `/top 10` |
| `/bm25` | Switch to BM25 mode | `/bm25` |
| `/sem` | Switch to semantic mode | `/sem` |
| `/hybrid` | Switch to hybrid mode (default) | `/hybrid` |
| `/quit` | Exit | `/quit` |

### Example Session

```
BUSCA DOUTRINA JURIDICA v2 -- Multi-Area
============================================================
Carregando...
  Carregando contratos embeddings (487 MB)... OK (9365 docs)
  Carregando processo_civil embeddings (1.2 GB)... OK (22182 docs)
Pronto!

[contratos+processo_civil] > exceptio non adimpleti contractus

  (5 resultados, 0.34s, modo=hybrid, area=all)

  1. [0.947] Contratos bilaterais > Exceptio non adimpleti contractus
     Contratos (Orlando Gomes) (chunk 26/42) [contratos]
     exceptio_non_adimpleti_contractus | definicao, requisitos

  2. [0.891] Da Exceptio non adimpleti contractus
     Curso de Direito Civil (Fabio Ulhoa) (chunk 31/55) [contratos]
     exceptio_non_adimpleti_contractus | doutrina_comparada

[contratos+processo_civil] > /filtro tipo=requisitos
  Filtros: {'tipo': 'requisitos'}

[contratos+processo_civil] > tutela antecipada

  (3 resultados, 0.28s, modo=hybrid, area=all)
  ...
```

## CLI Usage

```bash
# Basic search across all areas
python3 pipeline/search_doutrina_v2.py "exceptio non adimpleti contractus" --area all

# Search in a specific area
python3 pipeline/search_doutrina_v2.py "tutela antecipada requisitos" --area processo_civil

# Search with metadata filter
python3 pipeline/search_doutrina_v2.py "boa-fe objetiva" --instituto "boa-fe" --area contratos

# BM25 only (exact term matching)
python3 pipeline/search_doutrina_v2.py "Art. 476" --modo bm25

# Verbose output with text preview
python3 pipeline/search_doutrina_v2.py "clausula penal" --verbose --top 10

# Filter by book
python3 pipeline/search_doutrina_v2.py "formacao do contrato" --livro "orlando"

# Interactive mode
python3 pipeline/search_doutrina_v2.py --interativo
python3 pipeline/search_doutrina_v2.py -i --area contratos
```

## Known Limitations

- **BM25 recalculates per query.** Document frequencies (`df`) and average document length (`avg_dl`) are recomputed for every query. For ~31,500 documents, this adds noticeable latency. Pre-computing these values at index time would eliminate this cost. Tracked as mitigation **M13**.
- **Full JSON load on startup.** All embedding matrices, corpus metadata, and BM25 indexes must be loaded into memory before the first query. For `all` areas, this can take 10-20 seconds and consume 1+ GB of RAM.
- **No caching between queries.** In CLI (non-interactive) mode, the model and indexes are loaded fresh for each invocation. Interactive mode avoids this by keeping them in memory.
- **Brute-force search.** Cosine similarity is computed against every embedding in the index (O(n) per query). An HNSW or FAISS index would reduce this to O(log n) at the cost of approximate results. Not a problem at current scale (~31K), but will not scale past ~100K chunks.
- **No relevance feedback or learning-to-rank.** The system cannot learn from user behavior to improve ranking over time.
- **Post-retrieval metadata filtering** reduces result count below `top_k` when filters are restrictive. A pre-retrieval approach (filtering before scoring) would guarantee `top_k` results but requires a different architecture.
- **Tokenization is simplistic.** Both BM25 and the query parser use `re.findall(r'\w+', text.lower())` for tokenization. This does not handle Portuguese stop words, stemming, or compound legal terms (e.g., "boa-fe" is split into "boa" and "fe").

---
# docs/features/pipeline/intelligent-chunking.md
---


# Intelligent Chunking v3 (F02)

`pipeline/rechunk_v3.py` -- The most sophisticated and critical component of the pipeline. Splits legal markdown (output of `process_books.py`) into semantically coherent chunks using domain-specific heuristics tuned for Brazilian and international legal textbooks.

## Overview

| Property | Value |
|----------|-------|
| **Script** | `pipeline/rechunk_v3.py` (890 lines) |
| **Input** | Markdown files in `_staging/processed/{book}/` |
| **Output** | Re-chunked markdown files (same directory, overwritten) |
| **Min chunk** | 1,500 characters (`MIN_CHUNK_CHARS`) |
| **Max chunk** | 15,000 characters (`MAX_CHUNK_CHARS`) |
| **Test coverage** | 0% -- flagged as **F26** (P1, v0.3) |

:::caution
This script has **0% test coverage** for 890 lines of complex regex-based logic. It is the single highest-risk component in the pipeline. Adding tests is tracked as **F26** (P1 priority, milestone v0.3).
:::

## The 5-Pass Algorithm

The rechunker processes each book through five sequential passes. Each pass has a specific responsibility, and the output of one feeds into the next.

```mermaid
flowchart TD
    INPUT["Markdown from process_books.py"]
    P1["Pass 1: Section Split"]
    P2["Pass 2: Classify"]
    P3["Pass 3: Merge Small"]
    P4["Pass 4: Split Oversized"]
    P5["Pass 5: Cleanup"]
    OUTPUT["Semantic chunks"]

    INPUT --> P1
    P1 -->|"Split by 14 section patterns"| P2
    P2 -->|"Tag: noise, bibliography, summary, content"| P3
    P3 -->|"Merge chunks < 1500 chars with neighbors"| P4
    P4 -->|"Split chunks > 15000 chars at sentence boundaries"| P5
    P5 -->|"Remove empty, normalize whitespace"| OUTPUT
```

### Pass 1: Section Split

Detects section boundaries using 14 regex patterns (see below) and splits the document at each boundary. Each detected section header becomes the title of a new chunk.

### Pass 2: Classify

Classifies each chunk by content type using `classify_block_content()`:

| Classification | Detection Logic | Handling |
|---------------|-----------------|----------|
| `example` | Patterns: "por exemplo", "imagine que", "e.g.", "for instance" | Kept with preceding principle |
| `table` | More than 5 pipe characters and 2+ newlines | Can be standalone chunk |
| `characteristics` | 3+ contract characteristic terms (bilateral, oneroso, consensual...) | Kept as indivisible block |
| `law_article` | Starts with `Art. {number}` | Kept with subsequent commentary |
| `bibliography` | >50% of lines start with ALL CAPS author names, 5+ lines | Extracted as separate chunk |
| `regular` | Default | Normal processing |

### Pass 3: Merge Small

Chunks below `MIN_CHUNK_CHARS` (1,500) are merged with their neighbors. The merge strategy preserves semantic coherence:
- Examples merge with the preceding principle they illustrate
- Footnotes merge with their referencing paragraph
- Law articles merge with their commentary

### Pass 4: Split Oversized

Chunks exceeding `MAX_CHUNK_CHARS` (15,000) are split at sentence boundaries, ensuring no chunk exceeds the maximum while keeping sentences intact.

### Pass 5: Cleanup

Final pass removes empty chunks, normalizes whitespace, and validates that all chunks meet the minimum content threshold (200 characters of real text).

## 14 Section Patterns (`SECTION_PATTERNS`)

The section detector uses these regex patterns in priority order. The first match wins.

| # | Pattern | Type | Example Match |
|---|---------|------|---------------|
| 1 | `^#{1,3}\s+(.+)$` | `md_header` | `# Chapter 1` |
| 2 | `^\*\*Chapter\s+\d+[\.:]?\*?\*?\s*(.*?)` | `chapter_en` | `**Chapter 5:** Title` |
| 3 | `^\*?\*?Cap[i√≠]tulo\s+\*?\*?\w+\*?\*?\.?\s*(.*?)` | `capitulo_pt` | `Capitulo V - Dos Contratos` |
| 4 | `^CHAPTER\s+\d+\.?\s*(.*)$` | `chapter_caps` | `CHAPTER 5 BILATERAL CONTRACTS` |
| 5 | `^CAP[√çI]TULO\s+\w+\.?\s*(.*)$` | `capitulo_caps` | `CAPITULO V` |
| 6 | `^\*?\*?T[√çI]TULO\s+\w+\*?\*?\.?\s*(.*)$` | `titulo` | `**TITULO VI** Dos Contratos` |
| 7 | `^\*?\*?PARTE\s+\w+\*?\*?\.?\s*(.*)$` | `parte` | `PARTE ESPECIAL` |
| 8 | `^\*?\*?Part\s+\w+\*?\*?\.?\s*(.*)$` | `part_en` | `**Part One** General Theory` |
| 9 | `^(?:#{1,3}\s+)?\*?\*?Art\.?\s+\d+[\.\)]?\*?\*?\s*(.*)$` | `artigo` | `Art. 481.` or `### Art. 481` |
| 10 | `^(?:#{1,3}\s+)?_?\*?\*?Se[√ßc][√£a]o\s+\w+_?\*?\*?\.?\s*(.*)$` | `secao` | `Secao I - Disposicoes Gerais` |
| 11 | `^\*?\*?Section\s+\w+\*?\*?\.?\s*(.*)$` | `section_en` | `**Section 3** Formation` |
| 12 | `^(\d{1,3})\.\s+([A-Z][A-Z\s,]{8,80})$` | `numbered_caps` | `1. CONTRATOS BILATERAIS` |
| 13 | `^\*\*(\d{1,3})\.?\*?\*?\s+(.{5,80})$` | `numbered_bold` | `**1.** Conceito e Natureza` |
| 14 | `^([A-Z\s,]{15,80})$` | `allcaps_title` | `CONTRATOS BILATERAIS E UNILATERAIS` |

:::note
Pattern 14 (`allcaps_title`) matches any line of 15-80 ALL CAPS characters. This is intentionally broad to catch legal section headers that lack formatting, but it may produce false positives on bibliographic entries or publisher information.
:::

## Noise and Metadata Detection

### NOISE_TITLES

Content with these title keywords is filtered as non-substantive:

```python
NOISE_TITLES = {
    # Portuguese
    'pref√°cio', 'prefacio', 'agradecimentos', 'agradecimento',
    'dedicat√≥ria', 'dedicatoria', 'palavras do coordenador',
    'nota do editor', 'notas do editor', 'nota √† edi√ß√£o',
    'sobre o autor', 'sobre os autores', 'dados catalogr√°ficos',
    'ficha catalogr√°fica', 'expediente',
    'editora forense', 'editora saraiva', 'editora atlas',
    'editora renovar', 'editora revista dos tribunais',
    'no_content_here',
    # English
    'preface', 'foreword', 'acknowledgements', 'acknowledgments',
    'dedication', 'about the author', 'about the authors',
    "editor's note", "publisher's note",
}
```

### BIBLIOGRAPHY_TITLES

Sections matching these are extracted as separate bibliography chunks:

```python
BIBLIOGRAPHY_TITLES = {
    'bibliografia', 'refer√™ncias bibliogr√°ficas', 'refer√™ncias',
    'bibliography', 'references', 'works cited', 'further reading',
    'leituras complementares', 'obras consultadas',
}
```

### SUMMARY_TITLES

Tables of contents are tagged as metadata (not discarded, but marked):

```python
SUMMARY_TITLES = {
    'sum√°rio', 'sumario', '√≠ndice', 'indice',
    'table of contents', 'contents', 'summary',
    '√≠ndice remissivo', 'indice remissivo',
    'table of cases', 'table of legislation', 'table of statutes',
}
```

## Domain-Specific Features

### Running Header Detection

PDF extraction often produces repeated lines (book title, author name, chapter heading) at the top of every page. The rechunker detects these by frequency analysis across the document and filters them out before chunking.

### Footnote Aggregation

Legal textbooks heavily use footnotes. The rechunker groups footnotes with their referencing paragraph using two detection functions:

```python
def is_footnote_line(line: str) -> bool:
    """Detect footnote lines at bottom of text."""
    stripped = line.strip()
    # "1 Author, Book, p. 123" or "¬π Author..." or "[1] Author..."
    if re.match(r'^\d{1,3}\s+[A-Z√Å√â√ç√ì√ö√Ä√Ç√ä√î√É√ï√á]', stripped):
        return True
    if re.match(r'^[¬π¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ‚Å∞]+\s+', stripped):
        return True
    if re.match(r'^\[\d{1,3}\]\s+', stripped):
        return True
    return False
```

### Law Article + Commentary Preservation

When a chunk contains a law article transcription (e.g., `Art. 476 do Codigo Civil...`), the rechunker ensures the author's commentary that follows is never separated from the article itself. This is critical because the commentary is only meaningful in the context of the article being discussed.

### Indivisible Blocks

Contract characteristic lists (bilateral, oneroso, consensual, comutativo...) are detected when 3+ characteristic terms appear together and are kept as a single indivisible block, because splitting them would destroy the comparative analysis.

## Configuration

### CLI Arguments

```bash
# Rechunk all books in staging
python3 pipeline/rechunk_v3.py

# Rechunk a specific book
python3 pipeline/rechunk_v3.py contratos-orlando-gomes

# Set minimum chunk size (characters)
python3 pipeline/rechunk_v3.py --min-chars 1500

# Preview changes without writing
python3 pipeline/rechunk_v3.py --dry-run

# Force rechunk even if already processed
python3 pipeline/rechunk_v3.py --force
```

### Constants

| Constant | Value | Description |
|----------|-------|-------------|
| `MIN_CHUNK_CHARS` | 1,500 | Minimum characters for a valid chunk |
| `MAX_CHUNK_CHARS` | 15,000 | Maximum characters before forced split |

## Known Limitations

:::danger
**Hardcoded VAULT_PATH (line 29):** The script uses a hardcoded WSL path:
```python
VAULT_PATH = Path("/mnt/c/Users/sensd/vault")
```
This is different from the path in `process_books.py` (which uses a Linux path). Tracked as **F22** (P0 priority) for v0.2.
:::

- **0% test coverage** for 890 lines of regex-heavy logic. A single false positive in section detection can cascade through the entire document. Tracked as **F26**.
- **Custom YAML parser** uses regex instead of PyYAML. Special characters in titles (colons, quotes) can corrupt frontmatter. The same regex parser is duplicated across `enrich_chunks.py` and `embed_doutrina.py`. Tracked as **F23**.
- **Assumes hierarchical structure** -- books without clear H1/H2/section patterns (e.g., dictionaries, legal compilations, multi-author essays) produce poor chunking results.
- **Running header detection is heuristic** -- repeated lines that are NOT headers (e.g., a legal maxim appearing multiple times) can be falsely filtered.
- **`allcaps_title` pattern is broad** -- any line of 15-80 uppercase characters is treated as a section boundary, which can produce false positives on bibliographic entries, publisher names, or emphasized text.
- **No language-specific tokenization** -- the sentence splitter for oversized chunks does not use a Portuguese-specific tokenizer, which may split at abbreviations (e.g., "Art.", "Dr.").

---
# docs/features/pipeline/pdf-extraction.md
---


# PDF Extraction (F01)

`pipeline/process_books.py` -- The entry point of the Douto pipeline. Converts legal textbook PDFs into structured markdown files with chapter splitting and YAML frontmatter, preparing them for downstream chunking and enrichment.

## Overview

| Property | Value |
|----------|-------|
| **Script** | `pipeline/process_books.py` (414 lines) |
| **Input** | PDF files in `Knowledge/_staging/input/` |
| **Output** | Markdown files in `Knowledge/_staging/processed/{slug}/` |
| **Engine** | [LlamaParse](https://docs.llamaindex.ai/en/stable/llama_cloud/llama_parse/) (LlamaIndex) |
| **Dependencies** | `llama_parse`, `asyncio` |
| **Idempotent** | Yes -- skips already-processed PDFs via marker files |

```mermaid
flowchart TD
    PDF["PDF in _staging/input/"]
    LP["LlamaParse API"]
    MD["Full markdown"]
    SPLIT["Split by H1/H2"]
    CHUNKS["Chapter .md files"]
    INDEX["_INDEX.md"]
    RAW["_RAW_FULL.md (backup)"]

    PDF -->|"Send to API (async)"| LP
    LP -->|"Receive markdown"| MD
    MD --> SPLIT
    SPLIT --> CHUNKS
    SPLIT --> INDEX
    MD --> RAW
```

## How It Works

### 1. Scan for unprocessed PDFs

The script scans `_staging/input/` for `.pdf` files that do not have a `.processed_{filename}` marker file. This marker is created after successful processing, making the operation idempotent.

### 2. Send to LlamaParse

Each PDF is sent to the LlamaParse cloud API with configurable extraction tier:

| Tier | Use Case | Cost |
|------|----------|------|
| `fast` | Quick extraction without formatting, cheapest | Low |
| `cost_effective` | Best cost-benefit for clean digital text (default) | Medium |
| `agentic` | Scanned PDFs or complex layouts | High |

The parser is configured for Portuguese (`language="pt"`) with 2 workers:

```python
parser = LlamaParse(
    result_type="markdown",
    num_workers=2,
    verbose=True,
    language="pt",
)
documents = await parser.aload_data(str(pdf_path))
```

### 3. Split into chapters

The `split_into_chapters()` function divides the markdown output by H1 (`#`) and H2 (`##`) headers. Each chapter becomes an independent chunk:

```python
def split_into_chapters(markdown_text: str, filename: str) -> list[dict]:
    lines = markdown_text.split('\n')
    chunks = []
    current_chunk = {
        "title": "Introdu√ß√£o / Pr√©-textual",
        "level": 0,
        "content": [],
        "page_hint": ""
    }
    parent_title = filename

    for line in lines:
        h1_match = re.match(r'^# (.+)$', line)
        h2_match = re.match(r'^## (.+)$', line)

        if h1_match or h2_match:
            # Save previous chunk if it has substantial content
            if current_chunk["content"]:
                content_text = '\n'.join(current_chunk["content"]).strip()
                if len(content_text) > 100:  # ignore very small chunks
                    chunks.append({
                        "title": current_chunk["title"],
                        "parent": parent_title,
                        "content": content_text
                    })
            # Start new chunk...
```

Key design decisions:
- **H1 headers** set the parent title context. Subsequent H2 chunks inherit it as `parent_title > section_title`.
- **Minimum content threshold**: chunks with fewer than 100 characters are discarded.
- **Fallback**: if no headers are detected, the entire document becomes a single chunk.

### 4. Generate slug

File and directory names use a Portuguese-aware slugification function:

```python
def slugify(text: str) -> str:
    text = text.lower().strip()
    text = re.sub(r'[√†√°√¢√£√§√•]', 'a', text)
    text = re.sub(r'[√®√©√™√´]', 'e', text)
    text = re.sub(r'[√¨√≠√Æ√Ø]', 'i', text)
    text = re.sub(r'[√≤√≥√¥√µ√∂]', 'o', text)
    text = re.sub(r'[√π√∫√ª√º]', 'u', text)
    text = re.sub(r'[√ß]', 'c', text)
    text = re.sub(r'[^a-z0-9\s-]', '', text)
    text = re.sub(r'[\s_]+', '-', text)
    text = re.sub(r'-+', '-', text)
    return text.strip('-')[:80]
```

### 5. Write output files

For each book, the script writes:

- **`_RAW_FULL.md`** -- Complete LlamaParse output as backup
- **`_INDEX.md`** -- Book index note with wikilinks to all chapters
- **`001-{slug}.md` ... `NNN-{slug}.md`** -- Individual chapter files with YAML frontmatter

## Output Format

Each chapter file includes YAML frontmatter designed for downstream enrichment:

```yaml
knowledge_id: ""
tipo: "livro_chunk"
titulo: "Contratos Bilaterais e Unilaterais"
livro_titulo: "Contratos"
livro_arquivo_original: "contratos-orlando-gomes.pdf"
chunk_numero: 5
chunk_total: 42
fonte_primaria: ""
autor: ""
editora: ""
edicao: ""
ano: ""
paginas: ""
confianca: "UNVERIFIED"
confidencialidade: "Publico"
area_direito: []
teses_extraidas: []
casos_vinculados: []
tags: []
data_criacao: "2026-02-28T14:30:00"
data_ultima_modificacao: ""
status_enriquecimento: "pendente"
```

:::note
Fields left empty (e.g., `autor`, `editora`, `area_direito`) are filled by the enrichment stage (F03). The `status_enriquecimento: "pendente"` flag signals that the chunk has not yet been classified.
:::

## Configuration

### Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `LLAMA_CLOUD_API_KEY` | Yes | LlamaParse API authentication (via env or `.env`) |

### CLI Arguments

```bash
# Process all unprocessed PDFs in staging
python3 pipeline/process_books.py

# Process a specific PDF file
python3 pipeline/process_books.py contratos-orlando-gomes.pdf

# Use a different LlamaParse tier
python3 pipeline/process_books.py --tier fast
python3 pipeline/process_books.py --tier agentic

# Default tier is cost_effective
python3 pipeline/process_books.py --tier cost_effective
```

:::caution
There is no `--dry-run` flag in `process_books.py` (unlike other pipeline scripts). Running the script will send PDFs to the LlamaParse API and write files. Use the marker file mechanism to prevent re-processing.
:::

### Directory Structure

```
vault/Knowledge/_staging/
  input/              # Place PDFs here
    livro.pdf
    .processed_livro.pdf   # Marker (auto-created)
  processed/
    contratos-orlando-gomes/
      _RAW_FULL.md
      _INDEX.md
      001-introducao.md
      002-contratos-bilaterais.md
      ...
  failed/             # PDFs that failed processing
```

## Logging

All events are appended to `_staging/processing_log.jsonl`:

```json
{"op": "process_success", "file": "contratos.pdf", "book_title": "Contratos", "chunks": 42, "chars": 185000, "tier": "cost_effective", "output_dir": "/path/to/output", "ts": "2026-02-28T14:30:00"}
{"op": "skip", "file": "contratos.pdf", "reason": "already_processed", "ts": "2026-02-28T14:35:00"}
{"op": "process_failed", "file": "scanned-book.pdf", "error": "LlamaParse retornou vazio", "tier": "fast", "ts": "2026-02-28T14:40:00"}
```

## Known Limitations

:::danger
**Hardcoded VAULT_PATH (line 27):** The script uses a hardcoded Linux path:
```python
VAULT_PATH = Path("/home/sensd/.openclaw/workspace/vault")
```
This will fail on any other machine. Tracked as **F22** (P0 priority) for v0.2.
:::

- **No `--dry-run` mode** -- unlike `rechunk_v3.py` and `enrich_chunks.py`, this script cannot preview operations without making API calls.
- **LlamaParse quality varies** -- scanned PDFs or those with complex layouts (tables, multi-column) may produce poor markdown. Use `--tier agentic` for difficult PDFs.
- **Chapter splitting assumes H1/H2 structure** -- books without markdown headers (e.g., flat extracted text) become a single chunk. Non-hierarchical legal texts (dictionaries, compilations) produce poor results.
- **No OCR quality validation** -- there is no post-extraction check for garbled text or extraction artifacts.
- **Title detection is heuristic** -- the book title is derived from the filename (`pdf_path.stem.replace('-', ' ').title()`), which may not match the actual title.
- **`slugify()` is duplicated** -- the same function exists in `rechunk_v3.py`. Tracked as **F23** for extraction into `pipeline/utils.py`.

---
# docs/getting-started/installation.md
---


# Installation

This guide covers the complete setup for running every stage of the Douto pipeline, from PDF extraction through search. If you only need to search the existing corpus, see [Quickstart](quickstart).

## System Requirements

| Requirement | Minimum | Recommended |
|------------|---------|-------------|
| Python | 3.10+ | 3.11+ |
| RAM | 4 GB | 8 GB+ |
| Disk | 2 GB (models + corpus) | 10 GB (with all books) |
| GPU | Not required | CUDA-compatible (speeds up embedding generation) |
| OS | Linux, macOS, WSL2 | Linux or macOS |

## Step 1: Clone the Repository

```bash
git clone https://github.com/sensdiego/douto.git
cd douto
```

## Step 2: Create a Virtual Environment

```bash
python3 -m venv .venv
source .venv/bin/activate  # Linux/macOS
# .venv\Scripts\activate   # Windows
pip install -r pipeline/requirements.txt
```

:::caution
All dependencies in `requirements.txt` are unpinned (`sentence-transformers` instead of `sentence-transformers==2.6.1`). For reproducible builds, consider running `pip freeze > requirements.lock` after installation and committing it. This is tracked as [F24 in the roadmap](../roadmap/milestones#v02--stable-pipeline).
:::

Current dependencies:

| Package | Purpose | Size |
|---------|---------|------|
| `sentence-transformers` | Embedding generation (Legal-BERTimbau) | ~200 MB |
| `torch` | ML backend for sentence-transformers | ~800 MB |
| `numpy` | Vector operations (cosine similarity, etc.) | ~30 MB |
| `anthropic` | SDK for MiniMax M2.5 API (via custom base_url) | ~5 MB |
| `llama-parse` | PDF extraction via LlamaIndex | ~10 MB |

## Step 3: Configure Environment Variables

```bash
# Required for all pipeline stages
export VAULT_PATH="/path/to/your/vault"

# Required for PDF extraction (process_books.py)
export LLAMA_CLOUD_API_KEY="your-llamaparse-api-key"

# Required for chunk enrichment (enrich_chunks.py)
export MINIMAX_API_KEY="your-minimax-api-key"

# Optional: customize output paths
export OUTPUT_PATH="/path/to/output"    # default: ~/.openclaw/workspace/juca/data
export DATA_PATH="/path/to/search/data" # default: same as OUTPUT_PATH
```

The `VAULT_PATH` directory should be an Obsidian vault with the following structure:

```
$VAULT_PATH/
‚îî‚îÄ‚îÄ Knowledge/
    ‚îî‚îÄ‚îÄ _staging/
        ‚îú‚îÄ‚îÄ input/      # Place PDFs here
        ‚îú‚îÄ‚îÄ processed/  # Output from process_books.py and rechunk_v3.py
        ‚îî‚îÄ‚îÄ failed/     # PDFs that failed extraction
```

:::danger
Two pipeline scripts (`process_books.py` and `rechunk_v3.py`) currently have **hardcoded paths** instead of reading `VAULT_PATH` from the environment. Until [F22](../roadmap/milestones#v02--stable-pipeline) is implemented, you may need to edit these paths directly in the scripts:

- `process_books.py` line 27: `VAULT_PATH = Path("/home/sensd/.openclaw/workspace/vault")`
- `rechunk_v3.py` line 29: `VAULT_PATH = Path("/mnt/c/Users/sensd/vault")`
:::

For a complete environment variable reference, see [Environment Variables](../configuration/environment).

## Step 4: Download the Embedding Model

The `sentence-transformers` library auto-downloads `rufimelo/Legal-BERTimbau-sts-base` (~500 MB) on first run. To pre-download:

```bash
python3 -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('rufimelo/Legal-BERTimbau-sts-base')"
```

:::tip
To cache models in a custom location, set `SENTENCE_TRANSFORMERS_HOME` or `HF_HOME` before downloading:
```bash
export SENTENCE_TRANSFORMERS_HOME="/path/to/model/cache"
```
:::

## Step 5: Verify Installation

```bash
# Check Python version
python3 --version  # Should be 3.10+

# Check core dependencies
python3 -c "from sentence_transformers import SentenceTransformer; print('sentence-transformers OK')"
python3 -c "import torch; print(f'torch OK, CUDA: {torch.cuda.is_available()}')"
python3 -c "import numpy; print(f'numpy OK, version: {numpy.__version__}')"
python3 -c "import anthropic; print('anthropic OK')"

# Check search CLI
python3 pipeline/search_doutrina_v2.py --help
```

## Running the Full Pipeline

Each stage depends on the output of the previous one. Run them in order:

### Stage 1: PDF Extraction

```bash
# Place PDFs in $VAULT_PATH/Knowledge/_staging/input/
python3 pipeline/process_books.py --dry-run   # Preview what will be processed
python3 pipeline/process_books.py             # Run extraction
python3 pipeline/process_books.py --tier fast # Use cheaper LlamaParse tier
```

Requires: `LLAMA_CLOUD_API_KEY`. See [PDF Extraction](../features/pipeline/pdf-extraction).

### Stage 2: Intelligent Chunking

```bash
python3 pipeline/rechunk_v3.py --dry-run       # Preview
python3 pipeline/rechunk_v3.py                  # Process all books
python3 pipeline/rechunk_v3.py contratos-gomes  # Process one book
python3 pipeline/rechunk_v3.py --min-chars 1500 # Custom minimum chunk size
```

No API keys required. See [Intelligent Chunking](../features/pipeline/intelligent-chunking).

### Stage 3: Chunk Enrichment

```bash
python3 pipeline/enrich_chunks.py --dry-run   # Preview
python3 pipeline/enrich_chunks.py all         # Enrich all chunks
python3 pipeline/enrich_chunks.py contratos   # Enrich one area
```

Requires: `MINIMAX_API_KEY`. See [Enrichment](../features/pipeline/enrichment).

### Stage 4: Embedding Generation

```bash
python3 pipeline/embed_doutrina.py --dry-run  # Preview
python3 pipeline/embed_doutrina.py            # Generate embeddings
```

No API keys required (model is downloaded from HuggingFace). See [Embeddings](../features/pipeline/embeddings).

### Stage 5: Search

```bash
python3 pipeline/search_doutrina_v2.py --interativo  # Interactive mode
python3 pipeline/search_doutrina_v2.py "query" --area all
```

See [Hybrid Search](../features/pipeline/hybrid-search).

## Troubleshooting

### `FileNotFoundError` on hardcoded paths

Two scripts have hardcoded paths. Edit them directly or wait for [F22](../roadmap/milestones#v02--stable-pipeline):

```python
# process_books.py line 27 ‚Äî change to:
VAULT_PATH = Path(os.environ.get("VAULT_PATH", "/your/path"))

# rechunk_v3.py line 29 ‚Äî change to:
VAULT_PATH = Path(os.environ.get("VAULT_PATH", "/your/path"))
```

### PyTorch CUDA errors

If your GPU isn't compatible, force CPU mode:

```bash
export CUDA_VISIBLE_DEVICES=""
```

### `enrich_prompt.md` not found

The enrichment prompt file is **missing from the repository**. This is a known critical issue ([RT01 in PREMORTEM.md](https://github.com/sensdiego/douto/blob/main/PREMORTEM.md)). Until it's recovered, enrichment of new chunks will fail.

For more solutions, see [Troubleshooting](../reference/troubleshooting).

---
# docs/getting-started/introduction.md
---


# Introduction

Douto is the doctrine knowledge agent of the sens.legal platform. It processes legal textbooks into structured, searchable knowledge that lawyers and AI agents can query in real time.

## The Problem

Legal research in Brazil requires consulting multiple authors on the same legal concept. A lawyer researching *exceptio non adimpleti contractus* might need to compare positions from Orlando Gomes, Caio M√°rio, and Pontes de Miranda ‚Äî each in a different book, different chapter, different edition. This manual cross-referencing typically takes 2-4 hours per concept.

Current legal tech platforms (Jusbrasil, Turivius, Vlex, LexisNexis) perform **search** ‚Äî they return documents that match a query. None of them perform **synthesis** ‚Äî aggregating and comparing doctrinal positions across authors.

Douto bridges this gap by transforming raw legal textbooks into structured, classified, searchable knowledge with metadata that enables filtering by legal concept, content type, legal branch, and procedural phase.

## What Douto Does

Douto operates in two complementary modes:

### Batch Processing Pipeline

Five Python scripts executed sequentially transform legal PDFs into searchable data:

```
PDF ‚Üí process_books.py ‚Üí rechunk_v3.py ‚Üí enrich_chunks.py ‚Üí embed_doutrina.py ‚Üí search_doutrina_v2.py
```

Each stage adds structure: raw PDF becomes markdown, markdown becomes intelligent chunks, chunks get classified with legal metadata, metadata-enriched text becomes embeddings, and embeddings enable semantic search.

### Navigable Knowledge Base

An Obsidian-style markdown hierarchy organized by legal domain:

```
INDEX_DOUTO.md (root ‚Äî 8 legal domains)
  ‚îî‚îÄ‚îÄ MOC_CIVIL.md (35 books, ~9,365 chunks)
  ‚îî‚îÄ‚îÄ MOC_PROCESSUAL.md (8 books, ~22,182 chunks)
  ‚îî‚îÄ‚îÄ MOC_EMPRESARIAL.md (7 books)
  ‚îî‚îÄ‚îÄ nodes/ (atomic notes ‚Äî planned)
```

## Who Uses Douto

Douto serves three audiences:

| Audience | How they use Douto | Available today? |
|----------|-------------------|-----------------|
| **Lawyers** | Search doctrine via Juca frontend during case research | Not yet ‚Äî requires v0.4 integration |
| **AI agents** | Query doctrine via MCP/API during briefings and analysis | Not yet ‚Äî MCP planned for v0.4 |
| **Developers** | Extend the pipeline, add books, improve the knowledge base | Yes ‚Äî via CLI |

## Where Douto Fits

In the sens.legal ecosystem, each agent handles a different pillar of legal knowledge:

| Agent | Pillar | Current corpus |
|-------|--------|---------------|
| **Valter** | Case law (jurisprud√™ncia) | 23,400+ STJ decisions |
| **Leci** | Legislation (legisla√ß√£o) | Federal laws |
| **Douto** | Doctrine (doutrina) | ~50 books, ~31,500 chunks |
| **Joseph** | Orchestration | Coordinates all agents |
| **Juca** | Frontend | Presents results to lawyers |

When fully integrated, a lawyer asking Juca about a legal concept will receive a unified view combining case law from Valter, legislation from Leci, and doctrine from Douto.

## Core Concepts

These terms appear throughout the documentation:

| Term | Definition |
|------|-----------|
| **Chunk** | A semantically coherent fragment of a legal book (200-1,000 tokens), produced by `rechunk_v3.py`, with YAML frontmatter metadata |
| **Instituto jur√≠dico** | A legal concept or institute ‚Äî e.g., *exceptio non adimpleti contractus*, *boa-f√© objetiva*. The fundamental unit of classification. |
| **Enrichment** | The process of classifying chunks with structured metadata using an LLM (currently MiniMax M2.5) |
| **Embedding** | A 768-dimensional vector representing a chunk's semantic content, generated by Legal-BERTimbau |
| **MOC** | Map of Content ‚Äî an index file listing all books within a legal domain |
| **Skill graph** | The hierarchical knowledge structure: INDEX ‚Üí MOCs ‚Üí Chunks ‚Üí Atomic Notes |
| **Hybrid search** | Combination of semantic search (cosine similarity) and BM25 (keyword matching) with weighted scoring |

For full definitions, see the [Glossary](../reference/glossary).

## What Douto Does NOT Do

Clear boundaries from [AGENTS.md](https://github.com/sensdiego/douto):

- **Does not manage cases** ‚Äî that's Joseph (orchestrator)
- **Does not search case law** ‚Äî that's Valter (23,400+ STJ decisions)
- **Does not search legislation** ‚Äî that's Leci (federal laws)
- **Does not manage infrastructure** ‚Äî that's Valter (FastAPI backend)
- **Does not serve a web interface** ‚Äî that's Juca (Next.js frontend)

---
# docs/getting-started/quickstart.md
---


# Quickstart

This guide gets you running a doctrine search against Douto's pre-built corpus in under 5 minutes. For full pipeline setup (including PDF extraction and enrichment), see [Installation](installation).

## Prerequisites

- Python 3.10 or later
- pip
- ~4 GB RAM (for loading embeddings into memory)
- Pre-built corpus files (JSON) ‚Äî available from the project maintainer

## 1. Clone and Install

```bash
git clone https://github.com/sensdiego/douto.git
cd douto
pip install -r pipeline/requirements.txt
```

:::caution
Dependencies in `requirements.txt` are currently unpinned. If you encounter version conflicts, see [Installation](installation) for workarounds. Pinning versions is planned for [v0.2](../roadmap/milestones#v02--stable-pipeline).
:::

## 2. Set the Data Path

Point Douto to the directory containing the pre-built corpus files:

```bash
export DATA_PATH="/path/to/your/corpus/data"
```

This directory must contain:

| File | Description |
|------|-------------|
| `embeddings_doutrina.json` | 768-dim embedding vectors for contract law chunks |
| `search_corpus_doutrina.json` | Metadata for each chunk (title, author, instituto, etc.) |
| `bm25_index_doutrina.json` | Tokenized documents for BM25 keyword search |
| `embeddings_processo_civil.json` | Embeddings for civil procedure chunks |
| `search_corpus_processo_civil.json` | Metadata for civil procedure chunks |
| `bm25_index_processo_civil.json` | BM25 index for civil procedure |

## 3. Run a Search

### Single Query

```bash
# Search contract law for "exceptio non adimpleti contractus"
python3 pipeline/search_doutrina_v2.py "exceptio non adimpleti contractus" --area contratos

# Search civil procedure for "tutela antecipada"
python3 pipeline/search_doutrina_v2.py "tutela antecipada requisitos" --area processo_civil

# Search all areas
python3 pipeline/search_doutrina_v2.py "boa-f√© objetiva" --area all --verbose
```

### Interactive Mode

```bash
python3 pipeline/search_doutrina_v2.py --interativo
```

In interactive mode, you get a REPL with these commands:

| Command | Description |
|---------|-------------|
| `/area contratos\|processo_civil\|all` | Switch search area |
| `/filtro instituto=X tipo=Y fase=Z` | Set metadata filters |
| `/verbose` | Toggle text preview of chunks |
| `/top N` | Change number of results (default: 5) |
| `/bm25` | Switch to keyword-only search |
| `/sem` | Switch to semantic-only search |
| `/hybrid` | Switch to hybrid search (default) |
| `/quit` | Exit |

### With Filters

```bash
# Search for a specific legal concept (instituto)
python3 pipeline/search_doutrina_v2.py "contrato bilateral" --instituto "exceptio" --area contratos

# Search for definitions only
python3 pipeline/search_doutrina_v2.py "boa-f√© objetiva" --tipo "definicao" --verbose
```

## 4. Understand the Output

A typical search result looks like this:

```
  1. [0.847] üìó Da Exce√ß√£o do Contrato N√£o Cumprido
     üìñ Contratos ‚Äî Orlando Gomes (chunk 26/89) [contratos]
     üè∑Ô∏è  exceptio non adimpleti contractus, contrato bilateral | defini√ß√£o, requisitos
```

| Element | Meaning |
|---------|---------|
| `1.` | Rank position |
| `[0.847]` | Relevance score (0-1, higher is better) |
| `üìó` / `üìò` | Area: üìó = contratos, üìò = processo_civil |
| First line | Chunk title (section heading from the book) |
| `üìñ` | Book title, author, chunk position within the book |
| `üè∑Ô∏è` | Instituto tags and content type tags from enrichment |

With `--verbose`, the actual chunk text (first 300 characters) is also shown.

## Next Steps

- [Installation](installation) ‚Äî set up the full pipeline (PDF extraction, enrichment, embedding generation)
- [Hybrid Search](../features/pipeline/hybrid-search) ‚Äî deep dive into search modes and configuration
- [Architecture Overview](../architecture/overview) ‚Äî understand the complete data flow

---
# docs/index.md
---


# Douto

Douto is the legal doctrine knowledge agent for the [sens.legal](https://sens.legal) ecosystem. It transforms legal textbooks into searchable, structured, AI-ready knowledge through a five-stage Python pipeline and maintains a navigable skill graph organized by legal domain.

## Key Capabilities

Douto's pipeline processes legal textbooks from PDF to searchable embeddings:

- **PDF Extraction** ‚Äî converts legal PDFs to structured markdown via LlamaParse
- **Intelligent Chunking** ‚Äî splits documents using legal-domain heuristics (footnote grouping, law article preservation, running header detection)
- **LLM Enrichment** ‚Äî classifies each chunk with structured metadata: instituto jur√≠dico, tipo de conte√∫do, ramo do direito, fontes normativas
- **Semantic Embeddings** ‚Äî generates 768-dimensional vectors using Legal-BERTimbau with metadata-enhanced text composition
- **Hybrid Search** ‚Äî combines semantic search (cosine similarity) with BM25 keyword search and metadata filtering

## Current Status

| Metric | Value |
|--------|-------|
| Books processed | ~50 |
| Chunks in corpus | ~31,500 |
| Legal domains covered | 3 active (Civil, Processual, Empresarial) + 5 planned |
| Embedding dimensions | 768 (Legal-BERTimbau) |
| Search modes | Semantic, BM25, Hybrid |
| Test coverage | 0% |
| Pipeline scripts | 5 |
| Version | v0.1.0 (pre-release) |

## Quick Links

| Section | Description |
|---------|-------------|
| [Introduction](getting-started/introduction) | What Douto is, why it exists, and who uses it |
| [Quickstart](getting-started/quickstart) | Run a search in under 5 minutes |
| [Architecture](architecture/overview) | How the pipeline and knowledge base work |
| [Features](features/) | Complete feature inventory with status |
| [Roadmap](roadmap/) | Where Douto is going ‚Äî milestones v0.2 through v1.0 |
| [Glossary](reference/glossary) | Legal and technical terminology |

## Part of sens.legal

Douto is one of five components in the sens.legal unified legal research platform:

```mermaid
graph LR
    subgraph "sens.legal"
        JU["Juca<br/>Frontend Hub<br/>Next.js"]
        VA["Valter<br/>Case Law + Backend<br/>FastAPI + Neo4j"]
        LE["Leci<br/>Legislation<br/>Next.js + PG"]
        DO["Douto<br/>Legal Doctrine<br/>Python Pipeline"]
    end

    USER["Lawyer"] --> JU
    JU --> VA
    JU --> LE
    JU --> DO
    VA <-.->|"embeddings,<br/>knowledge graph"| DO
```

| Agent | Role | Stack |
|-------|------|-------|
| **Valter** | Case law backend ‚Äî 23,400+ STJ decisions, 28 MCP tools | FastAPI, PostgreSQL, Qdrant, Neo4j, Redis |
| **Juca** | Frontend hub ‚Äî user interface for lawyers | Next.js 16, block system, briefing progressivo |
| **Leci** | Legislation ‚Äî federal law database | Next.js 15, PostgreSQL, Drizzle |
| **Joseph** | Orchestrator ‚Äî coordinates agents | ‚Äî |
| **Douto** | Legal doctrine ‚Äî this project | Python 3, LlamaParse, Legal-BERTimbau |

---
# docs/reference/faq.md
---


# Frequently Asked Questions

Answers to common questions about Douto, organized by audience.


## For Developers

### How do I add a new book to the corpus?

Run the full pipeline sequentially. Place the PDF in the staging input directory, then execute each step:

```bash
# 1. Place PDF in input directory
cp livro.pdf $VAULT_PATH/Knowledge/_staging/input/

# 2. Extract PDF to markdown
python3 pipeline/process_books.py livro.pdf

# 3. Chunk the markdown
python3 pipeline/rechunk_v3.py slug-of-book

# 4. Enrich chunks with metadata
python3 pipeline/enrich_chunks.py slug-of-book

# 5. Generate embeddings
python3 pipeline/embed_doutrina.py

# 6. Verify with a search
python3 pipeline/search_doutrina_v2.py "query from the book" --area contratos
```

:::caution
Step 4 requires `enrich_prompt.md`, which is currently **missing from the repository**. You cannot enrich new chunks until it is recovered (mitigation M01). Steps 1-3 and 5-6 work without it if chunks are manually enriched.
:::

### Why doesn't the pipeline run on my machine?

The most common cause is **hardcoded paths**. Two of the five scripts (`process_books.py` and `rechunk_v3.py`) have absolute paths from the creator's machine baked into the source code:

- `process_books.py` line 27: `/home/sensd/.openclaw/workspace/vault`
- `rechunk_v3.py` line 29: `/mnt/c/Users/sensd/vault`

**Workaround:** Edit the `VAULT_PATH` line in each script to point to your local vault path.

**Permanent fix:** F22 (v0.2) will standardize all scripts to use `os.environ.get("VAULT_PATH")`.

See [Environment Variables](/docs/configuration/environment/) for the full variable reference.

### Why is there no database?

Douto stores embeddings and search indices as flat JSON files. This was the simplest approach for a single-machine prototype. The trade-offs:

| Flat JSON (current) | Vector DB (planned) |
|---------------------|---------------------|
| No infrastructure needed | Requires Qdrant/FAISS setup |
| Simple to debug (human-readable) | Binary/opaque storage |
| Full load into memory per query | Indexed, sub-second queries |
| ~2 GB for 31,500 chunks | Compact, scalable storage |
| Does not scale past ~100 books | Scales to millions of vectors |

Migration to a vector DB (likely Qdrant, since Valter already uses it) is planned for v0.4 (mitigation M12).

### Why MiniMax M2.5 instead of Claude for enrichment?

Cost. Enriching 31,500 chunks with a classification prompt requires significant token throughput. MiniMax M2.5 is substantially cheaper than Claude for this batch workload. The trade-off is quality -- MiniMax is a generic model, not fine-tuned for Brazilian law.

This is an open decision (D06). Options under evaluation:

| Option | Cost | Quality | Dependency |
|--------|------|---------|------------|
| MiniMax M2.5 (current) | Low | Unknown (unvalidated) | Fragile SDK hack |
| Claude | Higher | Likely better | Ecosystem-consistent |
| Local model | Zero | Unknown | Setup complexity |

### Can I use a different embedding model?

Technically yes, but it requires **re-embedding the entire corpus** (~31,500 chunks). The model is hardcoded in `embed_doutrina.py` (line 24) and `search_doutrina_v2.py` (line 24) as `rufimelo/Legal-BERTimbau-sts-base`.

Important considerations:
- All existing embeddings become incompatible (different vector space)
- Search quality may improve or degrade -- there is no benchmark comparison yet (planned in F40)
- The current model was trained on Portuguese (PT-PT) legal text, which may not be optimal for Brazilian legal terminology

### Where is the enrichment prompt?

:::danger[Critical missing file]
`enrich_prompt.md` is referenced in `enrich_chunks.py` at line 27 (`Path(__file__).parent / "enrich_prompt.md"`) but **does not exist in the repository**. It was lost during migration from the original Obsidian vault.

This means:
- Enrichment cannot be run on new chunks
- The prompt that generated metadata for 31,500 existing chunks is not version-controlled
- If chunks need re-enrichment, the exact same results cannot be reproduced

Recovery is priority M01 (P0) in the roadmap mitigation plan. Options: locate in vault history, reconstruct from MiniMax API logs, or reverse-engineer from existing enriched chunks.
:::

### How do I contribute tests?

This is the highest-impact contribution you can make. Douto has 0% test coverage.

1. Create a `tests/` directory structure (see [Testing](/docs/development/testing/))
2. Add `pytest` to the development dependencies
3. Start with `rechunk_v3.py` functions: `detect_section()`, `classify_title()`, `smart_split()`
4. Use real markdown snippets from legal books as fixtures
5. Mock all external API calls (MiniMax, LlamaParse, HuggingFace)

See the [Testing](/docs/development/testing/) page for the full planned strategy and example tests.


## For Users (Lawyers)

:::note
Douto does not have a user-facing interface yet. These questions anticipate the future integrated experience via Juca/Valter.
:::

### What legal domains does Douto cover?

Currently, three domains have populated content:

| Domain | Books | Chunks | Coverage |
|--------|-------|--------|----------|
| Direito Civil | 35 | ~9,365 | Contracts, obligations, civil liability, property |
| Direito Processual Civil | 8 | ~22,182 | CPC commentary, general theory, procedures |
| Direito Empresarial | 7 | -- | Venture capital, smart contracts, commercial litigation |

**Gaps:** Direito do Consumidor has a placeholder MOC. Tributario, Constitucional, Compliance, and Sucessoes have no content at all. If you search for a topic in an uncovered domain, you will get empty results.

### How accurate are the search results?

**Honest answer: we do not know.** There is no evaluation set, no accuracy benchmark, and no human validation of search quality.

What we do know:
- The hybrid search combines semantic similarity (meaning) with keyword matching (exact terms)
- Results are ranked by a combined score (70% semantic, 30% keyword by default)
- Metadata filters (by instituto, ramo, tipo) depend on the enrichment quality, which has not been validated

Quality measurement is planned for v0.2.5 (validation of 200 chunks) and v0.5 (formal eval set with 30+ queries).

### Can I trust the citations?

**With caution.** Citations include the book title, author, and chapter, but **not page numbers**. There are known risks:

- **Chunking errors** -- the chunk boundary may not align with the chapter boundary in the original book, leading to misattribution (e.g., citing Chapter 5 when the content is from Chapter 4)
- **Quotation nesting** -- legal authors frequently quote other authors at length. A chunk may be attributed to the book's author when the content is actually a quotation from another scholar
- **No edition tracking** -- if a newer edition of a book is processed, old chunks remain in the index. You may receive citations from an outdated edition

**Recommendation:** Always verify doctrinal citations against the original source before using them in legal documents.

### Will this replace legal research?

No. Douto is a search and retrieval tool, not a replacement for legal analysis. It helps you find relevant doctrinal passages faster, but:

- The corpus is limited (~50 books, not comprehensive)
- Metadata may contain errors
- No system can replace a lawyer's judgment about relevance and applicability
- The tool does not understand the nuances of your specific case


## For Stakeholders

### What is the competitive advantage?

Douto's differentiator is **structured metadata on Brazilian legal doctrine**. Each of the ~31,500 chunks is classified with its legal institute, content type, procedural phase, branch of law, and statutory references. This enables filtered semantic search that generic legal search engines cannot do.

No competitor currently offers this level of structured access to Brazilian legal textbooks.

### What is the IP situation?

:::caution[Uncomfortable truth]
The intellectual property status of the corpus requires audit. Evidence in the codebase (`rechunk_v3.py` line 716) suggests that some books may have been obtained from unauthorized sources (Z-Library reference). This creates three risks:

1. **Legal** -- Use of copyrighted works without authorization potentially violates Lei 9.610/98 (Brazilian Copyright Law)
2. **Reputational** -- Due diligence by investors or institutional partners would flag this
3. **Operational** -- If the corpus must be replaced with licensed versions, the entire pipeline needs reprocessing (and the enrichment prompt is currently lost)

An IP audit of the corpus is planned as mitigation action M16 (conditional on the project seeking investment or scale).
:::

### What is the timeline to production?

Based on the current roadmap:

| Milestone | Target | What it enables |
|-----------|--------|-----------------|
| v0.2 | ~March 2026 | Pipeline runs on any machine |
| v0.3 | ~May 2026 | Tests, docs, lint -- project is contributable |
| v0.4 | ~August 2026 | MCP server -- Valter can query doctrine |

**Caveats:**
- The roadmap is maintained by a solo developer managing 5 repositories
- 7 architectural decisions are unresolved, 2 of which block v0.4
- No external users have tested the system
- Timelines are estimates, not commitments

### How much does it cost to run?

| Component | Cost type | Estimate |
|-----------|-----------|----------|
| LlamaParse | Per-PDF, one-time | ~$0.01-0.10 per book (cost_effective tier) |
| MiniMax M2.5 | Per-chunk enrichment | Low (exact pricing varies) |
| Legal-BERTimbau | Free (open source model) | $0 |
| Compute | CPU/GPU for embeddings | Local machine, no cloud cost |
| Storage | JSON files | ~2 GB for current corpus |

<!-- NEEDS_INPUT: Exact MiniMax M2.5 pricing per token is not documented in the codebase. The creator may have this information. -->

### What happens if the solo developer is unavailable?

This is identified as risk RE01 (highest probability in the PREMORTEM). Currently:

- The pipeline runs only on the developer's machine
- The enrichment prompt is not in the repository
- Dependencies are unpinned
- There are no tests and no CI/CD
- Documentation is in progress (these docs)

The v0.2 and v0.3 milestones specifically address this bus-factor risk by making the project portable and contributable. Until those milestones are completed, another developer would face significant onboarding friction.

---
# docs/reference/glossary.md
---


# Glossary

Terms and concepts you will encounter in Douto documentation, organized by domain.


## Legal Domain Terms

### Instituto juridico
A legal concept or institute -- the fundamental unit of legal doctrine classification. Examples: *exceptio non adimpleti contractus* (defense of non-performance), *boa-fe objetiva* (objective good faith), *tutela antecipada* (preliminary injunction). In Douto, each chunk is classified by the instituto(s) it discusses. This is the primary metadata field for filtered search and the planned unit for atomic notes.

### Doutrina
Legal doctrine -- scholarly analysis and interpretation of the law by legal academics and practitioners. Unlike legislation (the law itself) or jurisprudence (court decisions), doutrina represents the academic understanding and theoretical framework of legal concepts. Douto processes doutrina exclusively; case law is handled by Valter/Juca and legislation by Leci.

### Ramo do direito
Branch of law -- a broad classification of legal domains. Douto organizes its knowledge base by ramo. The currently recognized branches are:

| Ramo | Portuguese | MOC Status |
|------|-----------|------------|
| Civil law | Direito Civil | Active (35 books) |
| Civil procedure | Direito Processual Civil | Active (8 books) |
| Business law | Direito Empresarial | Active (7 books) |
| Consumer law | Direito do Consumidor | Placeholder |
| Tax law | Direito Tributario | Not created |
| Constitutional law | Direito Constitucional | Not created |
| Compliance & governance | Compliance & Governanca | Not created |
| Succession law | Sucessoes & Planejamento Patrimonial | Not created |

### Fontes normativas
Statutory sources -- references to specific laws, articles, and legal provisions cited in doctrine. Examples: "CC art. 476" (Civil Code, article 476), "CPC art. 300" (Civil Procedure Code, article 300). Extracted during enrichment as a metadata field to enable cross-referencing with the Leci legislation service.

### Tipo de conteudo
Content type -- classification of what a chunk actually contains. Values used in enrichment:

| Value | Meaning |
|-------|---------|
| `definicao` | Definition of a legal concept |
| `requisitos` | Requirements or elements of a legal institute |
| `exemplo` | Practical example or case illustration |
| `jurisprudencia_comentada` | Commented court decision |
| `critica_doutrinaria` | Doctrinal critique or scholarly debate |

### Fase processual
Procedural phase -- the stage of a legal process or contractual lifecycle. Values: `formacao` (formation), `execucao` (execution/performance), `extincao` (extinction/termination). Used in enrichment metadata to enable phase-specific filtering.

### Jurisdicao estrangeira
Foreign jurisdiction -- when doctrine references legal systems from countries other than Brazil. Relevant because the corpus includes some international comparative law books.


## Technical Terms

### Chunk
A semantically coherent fragment of a legal book, produced by `rechunk_v3.py`. Chunks are the atomic unit of the pipeline -- they are enriched, embedded, and searched individually. A chunk has YAML frontmatter with metadata and a markdown body. Size range: 1,500-15,000 characters of actual text.

### Embedding
A 768-dimensional vector representation of a chunk's semantic content, generated by the Legal-BERTimbau model. Embeddings capture meaning rather than exact words, enabling semantic search (finding conceptually similar content even when different terminology is used). Stored normalized for cosine similarity computation.

### Enrichment
The process of classifying chunks with structured metadata using an LLM (currently MiniMax M2.5). Each chunk is analyzed and tagged with `instituto`, `tipo_conteudo`, `ramo`, `fase`, `fontes_normativas`, and other fields. This metadata enables filtered search and is the foundation for planned synthesis features.

### Hybrid search
A search approach that combines two ranking methods:
- **Semantic search** -- cosine similarity on embeddings (captures meaning)
- **BM25** -- probabilistic keyword ranking (captures exact terms)

The scores are combined with a configurable weight (default: 0.7 semantic, 0.3 BM25). This produces better results than either method alone, especially for legal queries that mix conceptual intent with specific technical terms.

### MOC (Map of Content)
An index file listing all books within a legal domain, with metadata and processing status. MOCs are the second level of the skill graph hierarchy (INDEX -> MOCs -> Books -> Chunks). Each MOC corresponds to a ramo do direito. File naming convention: `MOC_{DOMAIN}.md`.

### Skill graph
The hierarchical knowledge structure maintained by Douto:

```
INDEX_DOUTO.md          # Root: 8 legal domains
  -> MOC_CIVIL.md       # Domain index: 35 books
    -> Book directories  # Per-book chunk collections
      -> chunk_001.md   # Individual enriched chunks
        -> (future) atomic notes  # One per instituto juridico
```

Navigable via Obsidian's graph view and wikilinks.

### Atomic note
A single-concept knowledge note planned for the `knowledge/nodes/` directory -- one note per *instituto juridico*, synthesizing information from all chunks that discuss that concept across all books.

> **Planned Feature** -- Atomic notes are on the roadmap (F36, v0.5) but not yet implemented. Decision D03 (auto-generated vs. manually curated) is pending.

### Frontmatter
YAML metadata block at the top of markdown files, delimited by `---` markers. Contains structured data about the chunk (title, author, legal domain, enrichment status, etc.). Parsed by a custom regex-based parser in the pipeline scripts.

```yaml
knowledge_id: "contratos-orlando-gomes-cap05-001"
tipo: chunk
titulo: "Exceptio non adimpleti contractus"
livro_titulo: "Contratos"
autor: "Orlando Gomes"
area_direito: civil
status_enriquecimento: completo
```

### Running header
Repeated text that appears at the top of PDF pages (typically the book title, chapter name, or author name). These are artifacts of PDF layout, not meaningful content. `rechunk_v3.py` detects them by frequency analysis and filters them out to prevent false chunks.

### Doctrine Brief
A synthesized summary of multiple authors' positions on a single *instituto juridico*. Structured to include consensus views, divergent positions, historical evolution, and practical implications.

> **Planned Feature** -- The Doctrine Brief format is proposed as part of the Synthesis Engine (F43, v0.3.5) but not yet implemented.


## Ecosystem Terms

### sens.legal
The unified legal research platform comprising Douto, Valter, Juca, Leci, and Joseph. Also referred to by the product name **Jude.md**. Goal: provide Brazilian lawyers with integrated access to case law, legislation, and doctrine through a single interface.

### Valter
Backend service for the sens.legal ecosystem. Built with FastAPI, PostgreSQL, Qdrant (vector DB), Neo4j (knowledge graph), and Redis. Handles STJ case law (23,400+ decisions) and 28 MCP tools. Primary consumer of Douto's doctrine embeddings. Repository: separate.

### Juca
Frontend hub for sens.legal. Built with Next.js. Provides the user interface for lawyers, including the progressive briefing system (4 phases: diagnostic, precedents, risks, delivery). Accesses doctrine data through Valter.

### Leci
Legislation service for sens.legal. Built with Next.js, PostgreSQL, and Drizzle ORM. Manages federal legislation database. Future cross-reference target for Douto (F35 -- linking doctrinal commentary to specific statutory provisions).

### Joseph
Orchestrator agent for sens.legal. Coordinates work across Valter, Juca, Leci, and Douto. Manages cases and workflow.

### Jude.md
Product name for the sens.legal unified platform. Juca (jurisprudencia) + Leci (legislacao) + Douto (doutrina) + Valter (backend) = Jude.md. Epic issue: SEN-368.

### MCP (Model Context Protocol)
An open protocol for exposing tools to AI models (developed by Anthropic). Douto plans to expose doctrine search as MCP tools (v0.4, F30), enabling Claude Desktop, Claude Code, and other MCP-compatible clients to query doctrine directly.


## Acronyms

| Acronym | Full Form | Context |
|---------|-----------|---------|
| **BM25** | Best Matching 25 | Probabilistic keyword ranking algorithm used in hybrid search |
| **BERT** | Bidirectional Encoder Representations from Transformers | Architecture behind Legal-BERTimbau |
| **STJ** | Superior Tribunal de Justica | Brazil's Superior Court of Justice -- primary source for Valter's case law |
| **CPC** | Codigo de Processo Civil | Brazilian Civil Procedure Code (Lei 13.105/2015) |
| **CC** | Codigo Civil | Brazilian Civil Code (Lei 10.406/2002) |
| **CDC** | Codigo de Defesa do Consumidor | Brazilian Consumer Protection Code (Lei 8.078/1990) |
| **ETL** | Extract, Transform, Load | Data processing pattern -- Douto's pipeline is an ETL system |
| **ADR** | Architecture Decision Record | Document recording an architectural decision and its rationale |
| **MOC** | Map of Content | Index file listing resources within a topic |
| **nDCG** | Normalized Discounted Cumulative Gain | Search quality metric measuring ranking effectiveness |
| **HNSW** | Hierarchical Navigable Small World | Approximate nearest neighbor algorithm used by vector databases (e.g., Qdrant) |
| **FAISS** | Facebook AI Similarity Search | Vector similarity search library by Meta |
| **LGPD** | Lei Geral de Protecao de Dados | Brazilian General Data Protection Law |
| **MCP** | Model Context Protocol | Protocol for AI tool exposure (Anthropic) |
| **SSE** | Server-Sent Events | Unidirectional server-to-client streaming protocol |
| **WSL** | Windows Subsystem for Linux | Linux compatibility layer on Windows -- one of the hardcoded path environments |

---
# docs/reference/troubleshooting.md
---


# Troubleshooting

Common issues, their causes, and how to fix them. Problems are organized by pipeline stage.


## Pipeline Won't Start

### FileNotFoundError: hardcoded path doesn't exist

**Symptoms:**

```
FileNotFoundError: [Errno 2] No such file or directory: '/home/sensd/.openclaw/workspace/vault/Knowledge/_staging/processed'
```

or

```
FileNotFoundError: [Errno 2] No such file or directory: '/mnt/c/Users/sensd/vault/Knowledge/_staging/processed'
```

**Cause:** Two pipeline scripts (`process_books.py` and `rechunk_v3.py`) have absolute paths from the creator's machine hardcoded in the source.

| Script | Hardcoded path | Environment |
|--------|---------------|-------------|
| `process_books.py` line 27 | `/home/sensd/.openclaw/workspace/vault` | Linux native |
| `rechunk_v3.py` line 29 | `/mnt/c/Users/sensd/vault` | WSL (Windows mount) |

**Fix (immediate):** Edit the `VAULT_PATH` line in the affected script to point to your vault:

```python
# In process_books.py, change line 27:
VAULT_PATH = Path("/your/actual/vault/path")

# In rechunk_v3.py, change line 29:
VAULT_PATH = Path("/your/actual/vault/path")
```

**Fix (for enrich_chunks.py and embed_doutrina.py):** These scripts read the environment variable:

```bash
export VAULT_PATH="/your/actual/vault/path"
```

**Permanent fix:** F22 (v0.2) will standardize all scripts to use `os.environ.get()`.


### ModuleNotFoundError: No module named 'sentence_transformers'

**Cause:** Python dependencies are not installed, or you are running outside the virtual environment.

**Fix:**

```bash
# Activate virtual environment first
source .venv/bin/activate

# Install dependencies
pip install -r pipeline/requirements.txt
```

If you get permission errors, use:

```bash
pip install --user -r pipeline/requirements.txt
```

:::note
The `requirements.txt` does not pin versions. If a dependency fails to install, it may be due to version conflicts with other packages. Try installing individually: `pip install sentence-transformers torch numpy anthropic llama-parse`.
:::


### Python version error: unsupported syntax

**Symptoms:**

```
TypeError: 'type' object is not subscriptable
```

on lines like `tuple[dict, str]`.

**Cause:** Python version is below 3.10. The codebase uses modern type hint syntax that requires Python 3.10+.

**Fix:** Upgrade Python to 3.10 or later. Verify with:

```bash
python3 --version
# Must be 3.10.x or higher
```


## PDF Extraction Issues (process_books.py)

### LlamaParse API key not found

**Symptoms:**

```
Error: LLAMA_CLOUD_API_KEY not set
```

or a generic authentication error from the LlamaIndex SDK.

**Cause:** The `LLAMA_CLOUD_API_KEY` environment variable is not set. This key is loaded implicitly by the LlamaIndex SDK, not explicitly in the code.

**Fix:**

```bash
export LLAMA_CLOUD_API_KEY="llx-your-key-here"
```

Get a key at [cloud.llamaindex.ai](https://cloud.llamaindex.ai/).


### PDF extraction produces garbled text

**Cause:** The PDF may be scanned (image-based) or have a complex multi-column layout that the `cost_effective` tier cannot handle well.

**Fix:** Try the `agentic` tier, which uses more sophisticated extraction:

```bash
python3 pipeline/process_books.py --tier agentic problematic-book.pdf
```

:::tip
If the book has been digitized from a physical copy (OCR), expect some text artifacts regardless of tier. Characters like `fi` may be corrupted to separate characters, `art.` may become `art,`, etc. These artifacts propagate through the entire pipeline and contaminate embeddings.
:::


## Chunking Issues (rechunk_v3.py)

### Chunks are too small or too large

**Cause:** The hardcoded `MIN_CHUNK_CHARS` (1500) or `MAX_CHUNK_CHARS` (15000) may not suit your specific book.

**Fix:** Override the minimum via CLI:

```bash
python3 pipeline/rechunk_v3.py --min-chars 1000 your-book-slug
```

`MAX_CHUNK_CHARS` is not configurable via CLI. Edit line 33 of `rechunk_v3.py` to change it.


### Running headers appearing as content

**Symptoms:** Chunks that consist mostly of the book title or author name repeated.

**Cause:** The running header detection heuristic may have failed for this specific book. Headers that appear on every page of a PDF are normally detected by frequency and filtered out, but unusual formatting can bypass the detection.

**Fix:** Check the processed markdown for the book (in `$VAULT_PATH/Knowledge/_staging/processed/{slug}/`). If running headers are present in the source markdown, they need to be cleaned before rechunking. Use `--force` to rechunk:

```bash
# After cleaning the source markdown:
python3 pipeline/rechunk_v3.py --force your-book-slug
```


## Enrichment Issues (enrich_chunks.py)

### enrich_prompt.md not found

**Symptoms:**

```
ERRO: Prompt nao encontrado em /path/to/pipeline/enrich_prompt.md
```

The script exits immediately.

**Cause:** The enrichment prompt file is **missing from the repository**. It was lost during migration from the original Obsidian vault. This is a known critical issue (PREMORTEM RT01).

**Impact:** You cannot enrich new chunks until the prompt is recovered.

**Workarounds:**

1. **Reconstruct from existing chunks** -- examine enriched chunks in `$VAULT_PATH/Knowledge/_staging/processed/` to reverse-engineer the prompt format and expected output structure.
2. **Check vault history** -- if you have access to the original Obsidian vault, look for `enrich_prompt.md` in the pipeline or templates directory.
3. **Write a new prompt** -- based on the metadata fields (categoria, instituto, tipo_conteudo, fase, ramo, fontes_normativas) and the enrichment code's JSON parsing logic.

**Tracking:** Mitigation action M01 (P0 priority).


### MiniMax API authentication failed

**Symptoms:**

```
anthropic.AuthenticationError: Authentication failed
```

or

```
Error code: 401
```

**Cause:** The `MINIMAX_API_KEY` environment variable is not set, expired, or invalid.

**Fix:**

```bash
export MINIMAX_API_KEY="your-valid-minimax-key"
```

Verify the key works:

```bash
python3 -c "
import os
from anthropic import Anthropic
client = Anthropic(api_key=os.environ['MINIMAX_API_KEY'], base_url='https://api.minimax.io/anthropic')
print('Auth OK')
"
```


### MiniMax API returns unexpected format

**Symptoms:** Enrichment completes but chunks get `status_enriquecimento: "pendente"` or metadata fields are empty.

**Cause:** The Anthropic SDK compatibility with MiniMax's endpoint may have changed. This integration uses an undocumented compatibility layer (`base_url="https://api.minimax.io/anthropic"`) that is not officially supported by either Anthropic or MiniMax (PREMORTEM RT02).

**Debugging steps:**

1. Check the enrichment log at `$VAULT_PATH/Logs/enrichment_log.jsonl` for error details.
2. Try a single chunk manually to see the raw API response.
3. Check if the `anthropic` package version has changed: `pip show anthropic`.
4. Verify the MiniMax API endpoint is still accessible: `curl -I https://api.minimax.io/anthropic`.

**Workaround:** If the SDK compatibility is broken, you may need to:
- Pin the `anthropic` package to the last working version
- Switch to calling the MiniMax API directly with `requests` instead of the Anthropic SDK
- Consider migrating to Claude API (decision D06)


### Enrichment is slow

**Cause:** Enrichment processes chunks sequentially with 5 threads and a 0.5-second delay between requests. For large books (hundreds of chunks), this takes time.

**Expected throughput:** ~10 chunks per second (5 threads x 2 requests/second per thread, minus latency).

**For 1,000 chunks:** ~2 minutes. For the full corpus of 31,500 chunks: ~1 hour.

There is no way to increase the thread count or reduce the delay without editing the source code (lines 34-35 of `enrich_chunks.py`).

:::caution
Increasing `WORKERS` above 5 or reducing `DELAY_BETWEEN_REQUESTS` below 0.5s may trigger MiniMax API rate limiting.
:::


## Search Issues (search_doutrina_v2.py)

### Search returns no results

**Possible causes:**

1. **DATA_PATH not set or points to wrong directory:**
   ```bash
   echo $DATA_PATH
   ls $DATA_PATH  # Should contain embeddings_*.json, search_corpus_*.json, bm25_index_*.json
   ```

2. **Area has no corresponding files:**
   ```bash
   # Check available files
   ls $DATA_PATH/embeddings_*.json
   ls $DATA_PATH/search_corpus_*.json
   ```
   The search expects files named `embeddings_{area}.json`, `search_corpus_{area}.json`, and `bm25_index_{area}.json`. Currently supported areas: `contratos` (mapped to `doutrina` files) and `processo_civil`.

3. **JSON files are corrupted or empty:**
   ```bash
   python3 -c "import json; json.load(open('$DATA_PATH/embeddings_doutrina.json')); print('JSON OK')"
   ```

4. **Query terms don't match corpus vocabulary:**
   Try broader terms, or switch to semantic-only mode (`/sem` in interactive mode) which is better for conceptual queries.


### Search is very slow (multi-second queries)

**Cause:** This is a known architectural limitation. On every search:

1. Full JSON files (~2 GB total) are loaded into memory (cached after first load)
2. BM25 recalculates document frequencies for every query (O(N * T) complexity)
3. NumPy dot product runs over all 31,500 embedding vectors

**Expected latency:**
- First query: 5-15 seconds (data loading + search)
- Subsequent queries: 1-3 seconds (data cached, but BM25 still recalculates)

**Workarounds:**
- Use `--area contratos` or `--area processo_civil` to search a single area (smaller dataset)
- Use `/sem` mode (semantic only) to skip BM25 calculation
- Keep the interactive session open to benefit from caching

**Permanent fix:** Migration to Qdrant vector DB (mitigation M12, planned for v0.4) and pre-computed BM25 index (M13).


### Results seem irrelevant

**Possible causes:**

1. **Enrichment metadata quality is unknown** -- the metadata used for filtered search has never been validated (PREMORTEM PF01). If `instituto` or `ramo` classifications are wrong, filtered results will be wrong.

2. **Semantic vs. keyword mismatch** -- try different search modes:
   - `/sem` (semantic only) -- better for conceptual queries like "what is good faith in contracts"
   - `/bm25` (keyword only) -- better for exact terms like "art. 476 CC"
   - `/hybrid` (default) -- balances both

3. **Embedding pollution** -- the embedding is generated from composed text that includes metadata. If metadata is wrong, the embedding captures wrong context.

4. **Check verbose output:**
   ```
   /verbose
   ```
   This shows the full chunk text in results, letting you judge if the content matches your intent.


## Embedding Generation Issues (embed_doutrina.py)

### Out of memory during embedding generation

**Symptoms:**

```
RuntimeError: CUDA out of memory
```

or the process is killed by the OS.

**Cause:** PyTorch + Legal-BERTimbau + a large batch of chunks can exceed available GPU or system memory.

**Fix:**

1. **Reduce batch size** -- edit `BATCH_SIZE` in `embed_doutrina.py` line 25 (default: 32). Try 8 or 16.

2. **Use CPU instead of GPU:**
   ```bash
   export CUDA_VISIBLE_DEVICES=""  # Forces CPU mode
   python3 pipeline/embed_doutrina.py
   ```

3. **Process fewer chunks:**
   ```bash
   python3 pipeline/embed_doutrina.py --limit 100  # If supported
   ```

:::note
CPU mode is slower but uses less memory and works on any machine. For the full corpus of 31,500 chunks, expect ~10-30 minutes on CPU vs. ~2-5 minutes on GPU.
:::


### Model download fails

**Symptoms:**

```
OSError: Can't load tokenizer for 'rufimelo/Legal-BERTimbau-sts-base'
```

or

```
ConnectionError: HTTPSConnectionPool(host='huggingface.co', port=443)
```

**Cause:** Network issues or HuggingFace Hub is temporarily unavailable.

**Fix:**

1. **Retry** -- HuggingFace Hub outages are usually brief.

2. **Pre-download the model** on a machine with internet:
   ```bash
   python3 -c "from sentence_transformers import SentenceTransformer; SentenceTransformer('rufimelo/Legal-BERTimbau-sts-base')"
   ```

3. **Copy the cache** to the target machine:
   ```bash
   # Default cache location
   cp -r ~/.cache/huggingface/hub/models--rufimelo--Legal-BERTimbau-sts-base /target/path/
   export HF_HOME="/target/path/.cache/huggingface"
   ```


### PyTorch / CUDA compatibility errors

**Symptoms:**

```
RuntimeError: CUDA error: no kernel image is available for execution on the device
```

or various CUDA version mismatch errors.

**Cause:** The installed PyTorch version doesn't match your GPU driver or CUDA version.

**Fix:** CPU mode works for everything Douto does. Embedding generation is slower but fully functional:

```bash
export CUDA_VISIBLE_DEVICES=""
```

If you need GPU acceleration, install the correct PyTorch version for your CUDA:

```bash
# Check your CUDA version
nvidia-smi

# Install matching PyTorch (example for CUDA 12.1)
pip install torch --index-url https://download.pytorch.org/whl/cu121
```


## Knowledge Base Issues

### Frontmatter parsing errors

**Symptoms:** Metadata fields are missing, truncated, or contain wrong values after processing.

**Cause:** Douto uses a custom regex-based YAML parser (not PyYAML). This parser cannot handle several edge cases:

| Input | Expected | Actual |
|-------|----------|--------|
| `titulo: "Codigo Civil: Comentado"` | `Codigo Civil: Comentado` | `Codigo Civil` (truncated at colon) |
| `autor: O'Brien` | `O'Brien` | May fail to parse |
| `tags: [a, b, c]` | `["a", "b", "c"]` | Parsed by regex, may fail on nested lists |
| Multiline values | Full text | Only first line |

**Workaround:** Avoid special characters in frontmatter values. Specifically:
- Do not use colons (`:`) inside values -- move them to a description field
- Do not use hash symbols (`#`) -- they may be interpreted as comments
- Keep values on a single line
- Wrap values in double quotes if they contain special characters

**Permanent fix:** Migrate to PyYAML (planned as part of F23/M05 in `utils.py`).


### Wikilinks not resolving in Obsidian

**Cause:** The target file may not exist or may have a different name than expected.

**Fix:** Check that the target file exists:
- MOC links should point to `knowledge/mocs/MOC_{DOMAIN}.md`
- All internal links must use wikilink format: `[[target]]`, not markdown links `[text](path)`
- Open the vault in Obsidian to see broken links highlighted


## Getting Help

1. **Check this page first** -- most common issues are documented above.
2. **Check PREMORTEM.md** -- your issue may be a known risk with a documented mitigation plan.
3. **File a GitHub issue** at [github.com/sensdiego/douto/issues](https://github.com/sensdiego/douto/issues) with:
   - The script and command you ran
   - The full error message
   - Your environment (OS, Python version, `pip list` output)
   - The values of relevant environment variables (redact API keys)
4. **For ecosystem questions** (Valter/Juca integration), check the respective repositories.

---
# docs/roadmap/changelog.md
---


# Changelog

Significant changes to Douto, organized chronologically. Follows [Keep a Changelog](https://keepachangelog.com/) conventions.

Format: each entry lists the date, associated commit(s), and categorized changes (Added, Changed, Fixed, Removed).


## v0.1.0 -- Initial Setup (2026-02)

The Douto repository was created and populated with the pipeline scripts (migrated from an Obsidian vault) and the knowledge base structure.

### 2026-02-28 -- North Star Definition

**Commit:** `b7930d3`
**Reference:** SEN-368

#### Added
- `AGENTS.md` updated with Jude.md north star -- Douto is formally a component of the unified legal research platform (Juca + Leci + Douto + Valter = Jude.md)
- Issue epic SEN-368 defined as the convergence target


### 2026-02-28 -- Knowledge Base Population

**Commit:** `c4e2c5b`

#### Added
- Populated MOCs with real corpus data -- 56 books classified across 4 legal domains
- `MOC_CIVIL.md` -- 35 books, ~9,365 chunks (largest domain)
- `MOC_PROCESSUAL.md` -- 8 books, ~22,182 chunks
- `MOC_EMPRESARIAL.md` -- 7 books
- `MOC_CONSUMIDOR.md` -- placeholder structure (not yet populated)


### 2026-02-28 -- Pipeline Migration

**Commit:** `8f9c702`
**Reference:** SEN-358

#### Added
- `pipeline/process_books.py` -- PDF to markdown extraction via LlamaParse (supports tiers: agentic, cost_effective, fast)
- `pipeline/rechunk_v3.py` -- Intelligent legal chunking with 5 processing passes, 16 section patterns, running header detection, footnote grouping
- `pipeline/enrich_chunks.py` -- Chunk enrichment via MiniMax M2.5 (5 concurrent threads, structured legal metadata)
- `pipeline/embed_doutrina.py` -- Embedding generation using Legal-BERTimbau (768-dim, normalized)
- `pipeline/search_doutrina_v2.py` -- Hybrid search (semantic cosine + BM25) with interactive CLI
- `pipeline/requirements.txt` -- Python dependencies (sentence-transformers, torch, numpy, anthropic, llama-parse)

:::note
These 5 scripts were developed inside an Obsidian vault before being migrated to this repository. They predate the repo itself. The migration commit brought them under version control for the first time.
:::


### 2026-02-28 -- Initial Setup

**Commit:** `ce16dbc`

#### Added
- `AGENTS.md` -- Agent identity, responsibilities, boundaries, and git protocol
- `knowledge/INDEX_DOUTO.md` -- Skill graph index mapping 8 legal domains
- `knowledge/mocs/` -- MOC directory structure
- `knowledge/nodes/.gitkeep` -- Placeholder for future atomic notes
- `tools/.gitkeep` -- Placeholder for future auxiliary tools
- `.gitignore` -- Excludes node_modules, embeddings, .env, __pycache__


## Documentation Session (2026-02-28)

In a single documentation session, the following strategic documents were created:

#### Added
- `CLAUDE.md` -- Coding guidelines for AI code agents, aligned with the sens.legal ecosystem conventions (priority order, Python conventions, pipeline rules, knowledge base rules, embedding conventions, git patterns)
- `PROJECT_MAP.md` -- Full project diagnostic: directory tree, stack details, architecture, data flow diagrams, gap analysis, recommendations
- `ROADMAP.md` -- Product roadmap with 42 features across 6 milestones, 7 pending decisions, risk matrix, and mitigation plan
- `PREMORTEM.md` -- Risk analysis: 6 false premises, 14 technical risks, 5 product risks, 4 execution risks, 7 edge cases, and the IP risk disclosure
- `docs/` -- Starlight documentation site with 22+ pages covering getting started, architecture, features, configuration, development, roadmap, and reference


## Pre-History

The pipeline was developed over an undetermined period inside an Obsidian vault before this repository existed. Key facts about the pre-history:

- The 5 Python scripts in `pipeline/` predate the repository
- The corpus (~50 books, ~31,500 chunks) was processed before migration
- Hardcoded paths in the scripts reflect the original development environments (Linux native and WSL)
- The enrichment prompt (`enrich_prompt.md`) was lost during migration and is not in the repository
- Linear issues (SEN-XXX) were used for tracking before the repository had GitHub Issues

<!-- NEEDS_INPUT: Exact dates of pipeline development pre-migration are not available. The creator may have additional context about when each script was originally written. -->

---
# docs/roadmap/index.md
---


# Roadmap

Where Douto is going -- from pipeline stabilization to full sens.legal integration.

:::note[Draft status]
This roadmap is inferred from the codebase, commits, AGENTS.md, and ecosystem context. It has not been formally validated by the project owner. Priorities may change. See Decision D07 in [Milestones](/docs/roadmap/milestones/).
:::

## Product Vision

Douto will be the **doctrine knowledge backend** of the sens.legal ecosystem. When mature, it will:

- Process legal textbooks autonomously (PDF to chunks to embeddings)
- Maintain a navigable skill graph organized by legal domain
- Expose doctrine knowledge via MCP/API for Valter, Juca, and Leci to query in real time
- Support briefings, risk analysis, and legal document drafting with authoritative doctrinal references

The current corpus contains **~50 books** and **~31,500 chunks** across civil law, civil procedure, and business law.

## Current Status

| Category | Count | Details |
|----------|-------|---------|
| Implemented | 18 | F01-F18: full pipeline (PDF to search), skill graph, 4 MOCs, idempotency, logging, dry-run |
| In progress | 3 | F19 (MOC Consumidor), F20 (env var standardization), F21 (atomic notes) |
| Planned | 11 | F22-F32: path fixes, utils.py, tests, MOCs, README, Makefile, linting, MCP |
| Ideas | 10 | F33-F42: Neo4j integration, cross-references, Docker, CI/CD, eval set |
| Pending decisions | 7 | D01-D07: integration protocol, repo vs. module, tracking, model choice |
| Test coverage | 0% | No test framework, no tests |

### What Works Today

- Complete pipeline: `process_books.py` -> `rechunk_v3.py` -> `enrich_chunks.py` -> `embed_doutrina.py` -> `search_doutrina_v2.py`
- Hybrid search (semantic + BM25) with metadata filters
- Interactive CLI search with `/area`, `/filtro`, `/verbose` commands
- 4 populated MOCs: Civil (35 books), Processual (8 books), Empresarial (7 books), Consumidor (placeholder)
- Structured enrichment metadata: instituto, tipo_conteudo, ramo, fase, fontes_normativas

### What Doesn't Work

- Pipeline only runs on the creator's machine (hardcoded paths)
- No automated tests
- No real-time integration with sens.legal ecosystem (JSON files only)
- 4 of 8 MOCs do not exist as files (Tributario, Constitucional, Compliance, Sucessoes)
- `enrich_prompt.md` is missing from the repository (cannot enrich new chunks)
- Unpinned dependency versions

## Milestone Overview

| Milestone | Name | Key Deliverable | Status |
|-----------|------|-----------------|--------|
| **v0.2** | Pipeline Estavel | Runs on any machine | Planned |
| **v0.2.5** | Data Validation | Metadata quality gate (>= 85%) | Proposed (post-PREMORTEM) |
| **v0.3** | Quality & Coverage | Tests, MOCs, docs, linting | Planned |
| **v0.3.5** | Doctrine Synthesis | Synthesis Engine | Proposed |
| **v0.4** | sens.legal Integration | MCP server | Planned |
| **v0.5** | Knowledge Graph & Automation | Atomic notes, eval set, CI/CD | Planned |
| **v0.6** | Legal Ontology | Concept graph | Proposed |
| **v1.0** | Integrated Platform | Full ecosystem integration | Planned |

See [Milestones](/docs/roadmap/milestones/) for detailed breakdown of each.

### Milestone Sequence

```
v0.2 Pipeline Estavel
  |
  v
v0.2.5 Data Validation  <-- Proposed checkpoint
  |  - Validate 200 chunks
  |  - Create eval set
  |  - Schema validation
  |  - Gate: accuracy >= 85%?
  v
v0.3 Quality & Coverage (tests, MOCs, docs)
  |
  v
v0.3.5 Doctrine Synthesis (if quality gate passed)
  |
  v
v0.4 sens.legal Integration (MCP server)
  |
  v
v0.5 Knowledge Graph & Automation
  |
  v
v0.6 Legal Ontology (proposed)
  |
  v
v1.0 Integrated Platform
```

## Pending Decisions

Seven architectural decisions remain unresolved. Two of them (D01 and D02) block the v0.4 milestone entirely.

| # | Question | Impact | Blocks |
|---|----------|--------|--------|
| **D01** | Integration protocol: MCP stdio, MCP HTTP/SSE, REST, or keep JSON files? | Defines long-term architecture | v0.4 |
| **D02** | Independent service or Valter module? | Douto's identity as a service | v0.4 |
| D03 | Atomic notes: auto-generated or manually curated? | Volume vs. quality trade-off | v0.5 |
| D04 | Issue tracking: Linear (SEN-XXX) or GitHub Issues? | Contribution workflow | -- |
| D05 | Doctrine schema in Neo4j? | Knowledge graph integration | v1.0 |
| D06 | Keep MiniMax M2.5 or migrate enrichment model? | Cost, quality, dependency | -- |
| D07 | Are the inferred priorities correct? | Entire roadmap may reorder | All |

See [Architecture Decisions](/docs/architecture/decisions/) for detailed analysis of each option.

## Risk Summary

The top 5 risks from the PREMORTEM analysis, ordered by likelihood:

### 1. Death by Abandonment

**Probability: High** | Solo developer maintains 5 repos (Valter, Juca, Leci, Joseph, Douto). Valter and Juca are customer-facing and likely take priority. Douto may go 6+ months without commits, losing context and momentum.

### 2. Illusion of Quality

**Probability: High** | Enrichment metadata has never been validated against human judgment. If 30-40% of `instituto` and `tipo_conteudo` classifications are wrong, filtered search returns garbage and any synthesis features amplify errors. No eval set exists to measure this.

### 3. Irreproducible Foundation

**Probability: High** | `enrich_prompt.md` is missing. Dependency versions are unpinned. If the corpus needs reprocessing (new model, bug fix, new domain), the result will differ from the original. Two inconsistent datasets with no way to return to the previous state.

### 4. Redundancy with Valter

**Probability: Medium** | If Valter needs doctrine before v0.4, the team may build `valter/stores/doutrina/` with Qdrant (already available). Once Valter has a "good enough" doctrine module, integrating Douto becomes harder to justify than rewriting.

### 5. Zero Scalability

**Probability: Certain** | Embeddings stored as flat JSON (~2 GB for 31,500 chunks). BM25 recalculates document frequencies per query. Load time is seconds. Adding 50 more books doubles everything. Unusable as an MCP tool with this latency.

For the full risk analysis including 14 technical risks, 5 product risks, 4 execution risks, and 7 edge cases, see `PREMORTEM.md` in the repository root.

## Feature Backlog

### Implemented (F01-F18)

| # | Feature | Script/File |
|---|---------|------------|
| F01 | PDF extraction via LlamaParse | `process_books.py` |
| F02 | Intelligent legal chunking v3 | `rechunk_v3.py` |
| F03 | Chunk enrichment via MiniMax M2.5 | `enrich_chunks.py` |
| F04 | Embedding generation (Legal-BERTimbau 768-dim) | `embed_doutrina.py` |
| F05 | Hybrid search (semantic + BM25 + filters) | `search_doutrina_v2.py` |
| F06 | Multi-area search support | `search_doutrina_v2.py` |
| F07 | Interactive CLI search | `search_doutrina_v2.py` |
| F08 | Skill graph INDEX | `INDEX_DOUTO.md` |
| F09 | MOC Direito Civil (35 books) | `MOC_CIVIL.md` |
| F10 | MOC Processual Civil (8 books) | `MOC_PROCESSUAL.md` |
| F11 | MOC Empresarial (7 books) | `MOC_EMPRESARIAL.md` |
| F12 | Pipeline idempotency | All scripts |
| F13 | Structured logging (JSONL) | All scripts |
| F14 | Dry-run in all scripts | All scripts |
| F15 | Standardized YAML frontmatter | All scripts |
| F16 | AGENTS.md | `AGENTS.md` |
| F17 | CLAUDE.md | `CLAUDE.md` |
| F18 | PROJECT_MAP.md | `PROJECT_MAP.md` |

### Planned (F22-F32)

See [Milestones](/docs/roadmap/milestones/) for which features belong to which milestone.

| # | Feature | Priority | Milestone |
|---|---------|----------|-----------|
| F22 | Standardize paths (eliminate hardcodes) | P0 | v0.2 |
| F23 | Extract `pipeline/utils.py` | P1 | v0.2 |
| F24 | Pin dependency versions | P1 | v0.2 |
| F25 | Create missing MOCs (4 MOCs) | P1 | v0.3 |
| F26 | Tests for `rechunk_v3.py` | P1 | v0.3 |
| F27 | Tests for utility functions | P2 | v0.3 |
| F28 | Complete README | P2 | v0.3 |
| F29 | Douto -> Valter integration protocol | P1 | v0.4 |
| F30 | MCP server for doctrine | P1 | v0.4 |
| F31 | Makefile for pipeline orchestration | P2 | v0.3 |
| F32 | Linting with ruff | P2 | v0.3 |

---
# docs/roadmap/milestones.md
---


# Milestones

Detailed breakdown of each milestone -- what is included, what success looks like, and what must come first.

## v0.2 -- Stable Pipeline

**Objective:** Eliminate critical technical debt that prevents the pipeline from running reliably on any machine.

**Features included:**

| # | Feature | Priority | Status |
|---|---------|----------|--------|
| F22 | Standardize all paths via `os.environ.get()` | P0 | Planned |
| F23 | Extract `pipeline/utils.py` (parse_frontmatter, slugify, etc.) | P1 | Planned |
| F24 | Pin dependency versions in `requirements.txt` | P1 | Planned |
| F20 | Complete env var standardization (2 of 5 scripts done) | P1 | In progress |

**Acceptance criteria:**

- [ ] `python3 pipeline/process_books.py --dry-run` runs without error on a clean machine (with env vars set)
- [ ] No hardcoded absolute paths in any script
- [ ] `parse_frontmatter()` and `slugify()` exist in a single shared module (`utils.py`)
- [ ] `pip install -r pipeline/requirements.txt` installs deterministic versions

**Estimate:** 1 work session


## v0.2.5 -- Data Validation (Proposed)

**Objective:** Validate metadata quality before building innovation layer on top. This milestone was proposed after the PREMORTEM analysis identified unvalidated metadata (PF01) as an existential risk.

**Actions included:**

| # | Action | Risk mitigated | Effort |
|---|--------|----------------|--------|
| M06 | Validate 200 random chunks -- verify instituto, tipo_conteudo, ramo against actual content | PF01, RT11 | 2-3 hours |
| M07 | Create evaluation set: 30+ queries with expected results, measure recall@5 and nDCG | RE04 | 2 hours |
| M10 | Schema validation in enrichment -- validate LLM output fields against known value sets | RT11, PF01 | 1 session |

**Quality gate:**

:::caution[Hard gate -- no exceptions]
Metadata accuracy must be **>= 85%** on the 200-chunk sample. If accuracy is below 85%, **all downstream work stops** and the corpus must be re-enriched with a better model or prompt before proceeding to v0.3.
:::

**Rationale:** Building a Synthesis Engine (v0.3.5) or Ontology (v0.6) on top of 60% accurate metadata produces sophisticated-looking but unreliable output. In legal technology, unreliable output is worse than no output.

**Estimate:** 1 work session


## v0.3 -- Quality & Coverage

**Objective:** Tests, linting, complete MOCs, documentation -- the minimum for the project to be contributable by someone other than the creator.

**Features included:**

| # | Feature | Priority | Status |
|---|---------|----------|--------|
| F25 | Create missing MOCs (Tributario, Constitucional, Compliance, Sucessoes) | P1 | Planned |
| F26 | Tests for `rechunk_v3.py` (pytest + fixtures) | P1 | Planned |
| F27 | Tests for utility functions | P2 | Planned |
| F28 | Complete README | P2 | Planned |
| F31 | Makefile (`make pipeline`, `make test`, `make lint`) | P2 | Planned |
| F32 | Linting with ruff | P2 | Planned |
| F42 | Version and commit `enrich_prompt.md` | P1 | Planned |
| F19 | Complete MOC_CONSUMIDOR | P1 | In progress |

**Acceptance criteria:**

- [ ] 8/8 MOCs exist as files (even if some are structural placeholders)
- [ ] `make test` passes with at least 1 test for `rechunk_v3.py`
- [ ] `make lint` passes without errors
- [ ] README has sections: Setup, Usage, Architecture, Corpus, Env Vars
- [ ] `enrich_prompt.md` is present in the repository and version-controlled

**Prerequisites:** v0.2 completed
**Estimate:** 2-3 work sessions


## v0.3.5 -- Doctrine Synthesis (Proposed)

**Objective:** Transform Douto from a "book search engine" into a "doctrine reasoning engine" that synthesizes multiple authors' positions on a single legal concept.

> **Planned Feature** -- The Doctrine Synthesis Engine is proposed but not yet implemented or approved.

**Proposed features:**

| # | Feature | Description |
|---|---------|-------------|
| F43 | Synthesis Engine | Given an instituto juridico, collect all chunks, synthesize a multi-author brief |
| F44 | Synthesis prompt | Versioned prompt template for generating Doctrine Briefs |
| F45 | Doctrine Brief template | Structured output format: consensus, divergence, evolution, practical implications |

**Conditional:** Only proceeds if v0.2.5 quality gate passes (metadata accuracy >= 85%).

**Prerequisites:** v0.3 completed (tests exist), v0.2.5 quality gate passed
**Estimate:** 2-3 work sessions


## v0.4 -- sens.legal Integration

**Objective:** Douto connects to the ecosystem -- Valter can query doctrine programmatically.

**Features included:**

| # | Feature | Priority |
|---|---------|----------|
| F29 | Douto -> Valter integration protocol | P1 |
| F30 | MCP server for doctrine | P1 |

**Acceptance criteria:**

- [ ] Valter can query doctrine via a defined protocol (MCP or REST)
- [ ] At least 3 MCP tools are functional: `search_doutrina`, `get_chunk`, `list_areas`
- [ ] Doctrine search is accessible via Claude Desktop/Code

**Prerequisites:** v0.2 completed, Decisions D01 and D02 resolved
**Blocking decisions:**

| Decision | Question | Options |
|----------|----------|---------|
| D01 | Integration protocol | MCP stdio / MCP HTTP/SSE / REST / JSON files |
| D02 | Douto as independent service or Valter module | Separate repo / `valter/stores/doutrina/` / Proxy through Valter |

**Estimate:** 3-5 work sessions


## v0.5 -- Knowledge Graph & Automation

**Objective:** Atomic notes, quality evaluation, ingestion automation, and CI/CD.

**Features included:**

| # | Feature | Priority |
|---|---------|----------|
| F36 | Automatic atomic notes (1 note per instituto juridico) | P2 |
| F40 | Embedding quality eval set (30+ queries) | P2 |
| F41 | Unified ingestion CLI (`douto ingest livro.pdf`) | P3 |
| F39 | CI/CD: GitHub Actions with ruff + pytest | P3 |
| F21 | Complete knowledge nodes | P2 (in progress) |

**Acceptance criteria:**

- [ ] `nodes/` contains atomic notes generated from enriched chunks
- [ ] Evaluation set with at least 20 queries and expected results exists
- [ ] `make ingest livro.pdf` runs the complete pipeline
- [ ] GitHub Actions runs lint + test on PRs

**Prerequisites:** v0.3 completed (tests exist)
**Pending decisions:** D03 (atomic notes: auto vs. curated), D04 (issue tracking)
**Estimate:** 3-4 work sessions


## v0.6 -- Legal Ontology (Proposed)

**Objective:** Build a concept graph that maps relationships between legal institutes, creating a structured ontology of Brazilian law.

> **Planned Feature** -- The Legal Ontology is a proposed innovation that depends on validated metadata and the synthesis engine.

**Proposed features:**

| # | Feature | Description |
|---|---------|-------------|
| F46 | Instituto relationship extraction | Map how institutos relate (prerequisite, exception, subtype, etc.) |
| F47 | Ontology graph structure | Data model for the concept graph |
| F48 | Ontology navigation API | Query the graph (e.g., "what are the subtypes of responsabilidade civil?") |

**Prerequisites:** v0.3.5 completed (synthesis engine), validated metadata
**Estimate:** 5-8 work sessions


## v1.0 -- Integrated Platform

**Objective:** Douto fully integrated into sens.legal -- doctrine as a first-class source alongside case law and legislation.

**Features included:**

| # | Feature | Priority |
|---|---------|----------|
| F33 | Doctrine in Neo4j knowledge graph (Valter) | P2 |
| F34 | Cross-reference: doctrine <-> case law (STJ) | P2 |
| F35 | Cross-reference: doctrine <-> legislation (Leci) | P3 |
| F37 | Progressive Briefing support (Juca's 4 phases) | P2 |
| F38 | Docker containerization of pipeline | P3 |

**Acceptance criteria:**

- [ ] Doctrine nodes in Neo4j with `CITA_DOUTRINA` and `FUNDAMENTA` relationships
- [ ] Juca briefing includes doctrinal sources automatically
- [ ] Pipeline executable in Docker container

**Prerequisites:** v0.4 completed (MCP functional)
**Pending decisions:** D05 (Neo4j schema)
**Estimate:** Multiple sessions, depends on Valter and Juca evolution


## Gantt Chart

```mermaid
gantt
    title Douto -- Roadmap
    dateFormat YYYY-MM
    axisFormat %b %Y

    section Foundation
    v0.2 Pipeline Estavel           :a1, 2026-03, 1M
    v0.2.5 Data Validation          :a15, after a1, 0.5M
    v0.3 Quality & Coverage         :a2, after a15, 2M

    section Innovation
    v0.3.5 Doctrine Synthesis       :a25, after a2, 1M

    section Integration
    v0.4 sens.legal Integration     :a3, after a25, 2M
    v0.5 Knowledge Graph            :a4, after a3, 2M

    section Platform
    v0.6 Legal Ontology             :a55, after a4, 2M
    v1.0 Integrated Platform        :a5, after a55, 3M
```

## Milestone Comparison: Original vs. Post-PREMORTEM

The PREMORTEM analysis recommended inserting validation checkpoints before building on unverified foundations:

```
ORIGINAL:                       POST-PREMORTEM (current):
v0.2 Pipeline Estavel           v0.2 Pipeline Estavel
  |                               |
  v                               v
v0.3 Quality                    v0.2.5 Data Validation  <-- NEW
  |                               |  Quality gate: >= 85%
  v                               v
v0.4 Integration                v0.3 Quality (tests, MOCs, docs)
  |                               |
  v                               v
v0.5 Knowledge Graph            v0.3.5 Doctrine Synthesis  <-- NEW
  |                               |
  v                               v
v1.0 Platform                   v0.4+ (continues as before)
```

**Impact:** Adds ~1 work session (v0.2.5) but prevents building innovation features on unreliable data. If the quality gate fails (accuracy < 85%), re-enrichment happens before v0.3, not after.

